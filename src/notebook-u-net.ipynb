{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guia de Estudo: Compreendendo a Arquitetura VGG_UNET\n",
    "## Questionário\n",
    "1) Qual a função principal do arquivo VGG_UNET.py?\n",
    "2) Descreva a relação entre os arquivos train.py e data_utils.py no processo de treinamento da rede neural.\n",
    "3) O que se pode esperar encontrar no arquivo inference_sample.py?\n",
    "4) Com base nos nomes dos arquivos, qual a arquitetura de rede neural utilizada neste projeto? Descreva suas características principais.\n",
    "5) Para que serve a arquitetura UNET em aplicações de aprendizado de máquina? Cite exemplos de áreas onde ela se destaca.\n",
    "6) Explique a importância do processo de treinamento em uma rede neural. Como ele se relaciona com os arquivos mencionados?\n",
    "7) O que são dados de treinamento e como o arquivo data_utils.py pode auxiliar nesse processo?\n",
    "8) Deduza a partir dos nomes dos arquivos qual a linguagem de programação utilizada neste projeto. Justifique sua resposta.\n",
    "9) Qual a provável finalidade do arquivo readme.md no contexto deste projeto?\n",
    "10) Imagine que você precisa modificar o conjunto de dados utilizado no treinamento. Qual arquivo você precisaria editar? Por quê?\n",
    "\n",
    "## Gabarito\n",
    "- O arquivo VGG_UNET.py define a arquitetura da rede neural, especificando as camadas, funções de ativação e outros parâmetros do modelo.\n",
    "- O arquivo train.py é responsável por executar o processo de treinamento da rede neural, utilizando os dados preparados pelo arquivo data_utils.py.\n",
    "- O arquivo inference_sample.py provavelmente contém um exemplo de como utilizar a rede neural treinada para realizar inferências em novos dados.\n",
    "- A arquitetura utilizada é a VGG_UNET, uma combinação da rede VGG (rede convolucional profunda) com a arquitetura UNET (eficiente para segmentação de imagens).\n",
    "- A arquitetura UNET é comumente utilizada para tarefas de segmentação de imagens, como em imagens médicas, detecção de objetos e processamento de imagens de satélite.\n",
    "- O treinamento é a etapa onde a rede neural \"aprende\" a partir dos dados fornecidos, ajustando seus parâmetros para realizar a tarefa desejada. O arquivo train.py executa o processo, utilizando a arquitetura definida em VGG_UNET.py e os dados preparados por data_utils.py.\n",
    "- Dados de treinamento são o conjunto de exemplos utilizados para treinar a rede neural. O arquivo data_utils.py auxilia no processamento, organização e carregamento desses dados para o treinamento.\n",
    "- A linguagem de programação utilizada é provavelmente Python, visto que a extensão \".py\" é comumente associada a arquivos de código Python.\n",
    "- O arquivo readme.md serve como um guia introdutório ao projeto, contendo informações sobre os arquivos, como utilizá-los e outras informações relevantes.\n",
    "- Para modificar o conjunto de dados, seria necessário editar o arquivo data_utils.py, pois ele é responsável por carregar e preparar os dados para o treinamento.\n",
    "  \n",
    "## Questões para Dissertação\n",
    "- Discuta as vantagens e desvantagens de utilizar a arquitetura VGG_UNET em comparação com outras arquiteturas de redes neurais para tarefas de segmentação de imagens.\n",
    "- Explique em detalhes como o processo de treinamento da rede neural é realizado no contexto dos arquivos mencionados. Quais parâmetros são ajustados e como o desempenho da rede é avaliado?\n",
    "- Descreva o papel das funções de ativação em uma rede neural e discuta a importância da escolha adequada dessas funções para o sucesso do treinamento.\n",
    "- Com base nos nomes dos arquivos, especule sobre o tipo de dados que este projeto utiliza para treinamento e inferência. Que tipo de pré-processamento de dados pode ser necessário para preparar os dados para a rede neural?\n",
    "- Imagine que você precisa integrar este projeto a um sistema maior. Descreva como você faria a interface entre a rede neural treinada e outros componentes do sistema, considerando os arquivos mencionados.\n",
    "## Glossário\n",
    "- VGG_UNET: Arquitetura de rede neural que combina a rede VGG com a arquitetura UNET, geralmente utilizada para segmentação de imagens.\n",
    "- Rede Neural: Modelo computacional inspirado no cérebro humano, capaz de aprender a partir de dados e realizar tarefas como classificação, regressão e segmentação.\n",
    "- Treinamento: Processo de ajuste dos parâmetros da rede neural para que ela aprenda a realizar a tarefa desejada.\n",
    "- Dados de Treinamento: Conjunto de dados utilizado para treinar a rede neural.\n",
    "- Inferência: Processo de utilizar a rede neural treinada para realizar predições em novos dados.\n",
    "- Segmentação de Imagens: Tarefa de dividir uma imagem em diferentes regiões, cada uma representando um objeto ou classe diferente.\n",
    "- Função de Ativação: Função matemática que introduz não-linearidade na rede neural, permitindo que ela aprenda relações complexas nos dados.\n",
    "- Python: Linguagem de programação popularmente utilizada em projetos de aprendizado de máquina e ciência de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalação de Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (2.5.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchvision in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (0.20.1+cu118)\n",
      "Requirement already satisfied: numpy in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: torch==2.5.1+cu118 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from torchvision) (2.5.1+cu118)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: filelock in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from torch==2.5.1+cu118->torchvision) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from torch==2.5.1+cu118->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from torch==2.5.1+cu118->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from torch==2.5.1+cu118->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from torch==2.5.1+cu118->torchvision) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from torch==2.5.1+cu118->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from sympy==1.13.1->torch==2.5.1+cu118->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from jinja2->torch==2.5.1+cu118->torchvision) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: utils2 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: six in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from utils2) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from utils2) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from utils2) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from requests->utils2) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from requests->utils2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from requests->utils2) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from requests->utils2) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\git\\image-segmentation\\.venv\\lib\\site-packages (from opencv-python) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install torch\n",
    "%pip install torchvision\n",
    "%pip install utils2\n",
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importe os módulos necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas\n",
    "import os\n",
    "import os.path as osp\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaração de Variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponível: True\n",
      "Diretório do Projeto c:\\git\\image-segmentation\\dataset\\mamoeiro.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# CUDA:\n",
    "# Configuração do dispositivo CUDA\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f'CUDA disponível: {cuda_available}')\n",
    "\n",
    "# Caminho do diretório Dataset:\n",
    "directory = os.path.abspath(os.path.join(os.getcwd(), '..')) + r'\\dataset\\mamoeiro'\n",
    "print(f'Diretório do Projeto {directory}.')\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "img_folder_val = directory + r'\\Val'\n",
    "img_folder_train = directory + r'\\Train'\n",
    "img_folder_test = directory + r'\\Test'\n",
    "save_dir = directory + r'\\\\result_unet\\\\'\n",
    "if not os.path.exists(img_folder_val):\n",
    "    os.makedirs(img_folder_val)\n",
    "if not os.path.exists(img_folder_train):\n",
    "    os.makedirs(img_folder_train)\n",
    "if not os.path.exists(img_folder_test):\n",
    "    os.makedirs(img_folder_test)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Local onde o Modelo será salvo\n",
    "model_file_name = save_dir + 'model_unet.pth'\n",
    "\n",
    "# Configurações do treinamento\n",
    "resolution_input = (640, 480)  # Tamanho de entrada\n",
    "patience = 30\n",
    "plot_val = True\n",
    "plot_train = True\n",
    "max_epochs = 100\n",
    "class_weights = [1, 1, 1, 1]\n",
    "nClasses = 4\n",
    "\n",
    "# Mapeamento de classes e cores\n",
    "class_to_color = {'Doenca': (255, 0, 0), 'Folha': (0, 255, 0), 'Solo': (0, 0, 255), 'Saudavel': (0, 255, 255)}\n",
    "class_to_id = {'Doenca': 0, 'Folha': 1, 'Solo': 2, 'Saudavel': 3}\n",
    "id_to_class = {v: k for k, v in class_to_id.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase para Segmentação de Dataset\n",
    "\n",
    "    Este arquivo provavelmente contém funções utilitárias para manipular e preparar os dados para o treinamento do modelo. As utilidades de dados aqui podem incluir:\n",
    "        - Carregamento de dados de diferentes fontes (arquivos, bancos de dados, APIs).\n",
    "        - Limpeza e pré-processamento de dados (por exemplo, lidar com valores ausentes, normalização, conversão de tipos de dados).\n",
    "        - Aumento de dados para aumentar o tamanho do conjunto de dados de treinamento.\n",
    "        - Divisão dos dados em conjuntos de treinamento, validação e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    \"\"\"Segmentation dataset loader.\"\"\"\n",
    "\n",
    "    def __init__(self, json_folder, img_folder, is_train, class_to_id, resolution_input = (640, 480), augmentation = False, transform=None):\n",
    "    #def __init__(self, json_folder, img_folder, is_train, class_to_id, resolution_input = (1280, 960), augmentation = False, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            json_folder (str): Path to folder that contains the annotations.\n",
    "            img_folder (str): Path to all images.\n",
    "            is_train (bool): Is this a training dataset ?\n",
    "            augmentation (bool): Do dataset augmentation (crete artificial variance) ?\n",
    "        \"\"\"\n",
    "\n",
    "        self.gt_file_list = glob.glob(osp.join(json_folder, '*.json'))\n",
    "\n",
    "        self.total_samples = len(self.gt_file_list)\n",
    "        self.img_folder = img_folder\n",
    "        self.is_train = is_train\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "        self.resolution = resolution_input\n",
    "        self.class_to_id = class_to_id\n",
    "        \n",
    "        \n",
    "        # Mean and std are needed because we start from a pre trained net\n",
    "        self.mean = [0.485, 0.456, 0.406]\n",
    "        self.std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        gt_file = self.gt_file_list[idx]\n",
    "        img_number_str = gt_file.split('.')[0].split('/')[-1]\n",
    "\t# Abre Json\n",
    "        gt_json = json.load(open(gt_file, 'r'))\n",
    "\t# Abre imagem\n",
    "        img_np = cv2.imread(osp.join(self.img_folder, img_number_str + '.png'), cv2.IMREAD_IGNORE_ORIENTATION + cv2.IMREAD_COLOR)\n",
    "        original_shape = img_np.shape\n",
    "        img_np = cv2.resize(img_np, (self.resolution[0], self.resolution[1]))[..., ::-1]\n",
    "        img_np = np.ascontiguousarray(img_np)\n",
    "\t# Cria imagem zerada\n",
    "        label_np = np.zeros((img_np.shape[0], img_np.shape[1]))\n",
    "        label_np[...] = -1\n",
    "        \n",
    "\t# Para todos poligonos\n",
    "        for shape in gt_json['shapes']:\n",
    "            # Transforma os pontos do poligono em array\n",
    "            points_np = np.array(shape['points'], dtype = np.float64)\n",
    "\n",
    "\t    # Ajusta os pontos porque eu mudo o resolucao (pode ignorar)\n",
    "            points_np[:, 0] *= self.resolution[0]/original_shape[1]\n",
    "            points_np[:, 1] *= self.resolution[1]/original_shape[0]\n",
    "\t    # As coordenadas dos pontos que formam o poligono tem que ser inteiros\n",
    "            points_np = np.round(points_np).astype(np.int64)\n",
    "\t    # Coloca os pontos no formato certo para o opencv\n",
    "            points_np = points_np.reshape((-1,1,2))\n",
    "\t    # Pinta o poligono usando o opencv com o valor referente ao label\n",
    "            label_np = cv2.fillPoly(label_np, [points_np], self.class_to_id[shape['label']])\n",
    "\n",
    "        # Transforma o GT em inteiro    \n",
    "        label_np = label_np.astype(np.int32)\n",
    "        \n",
    "        if self.is_train and self.augmentation:\n",
    "            if np.random.rand() > 0.5:\n",
    "                img_np = np.fliplr(img_np)\n",
    "                label_np = np.fliplr(label_np)\n",
    "                img_np = np.ascontiguousarray(img_np)\n",
    "                label_np = np.ascontiguousarray(label_np)\n",
    "        \n",
    "        img_pt = img_np.astype(np.float32) / 255.0\n",
    "        for i in range(3):\n",
    "            img_pt[..., i] -= self.mean[i]\n",
    "            img_pt[..., i] /= self.std[i]\n",
    "            \n",
    "        img_pt = img_pt.transpose(2,0,1)\n",
    "            \n",
    "        img_pt = torch.from_numpy(img_pt)\n",
    "        label_pt = torch.from_numpy(label_np).long()\n",
    "        #print(img_number_str, img_pt.shape)\n",
    "\n",
    "        sample = {'image': img_pt, 'gt': label_pt, 'image_original': img_np}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG_UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetVgg(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    BorderNetwork is a NN that aims to detected border and classify occlusion.\n",
    "    The architecture is a VGG without the last pool layer. After that we \n",
    "    have two paths, one for regression and one for classification (occlusion).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(UNetVgg, self).__init__()\n",
    "        \n",
    "        vgg16pre = torchvision.models.vgg16(pretrained=True)\n",
    "        self.vgg0 = torch.nn.Sequential(*list(vgg16pre.features.children())[:4])\n",
    "        self.vgg1 = torch.nn.Sequential(*list(vgg16pre.features.children())[4:9])\n",
    "        self.vgg2 = torch.nn.Sequential(*list(vgg16pre.features.children())[9:16])\n",
    "        self.vgg3 = torch.nn.Sequential(*list(vgg16pre.features.children())[16:23])\n",
    "        self.vgg4 = torch.nn.Sequential(*list(vgg16pre.features.children())[23:30])\n",
    "        \n",
    "        \n",
    "        self.smooth0 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(128, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True),\n",
    "                torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True)\n",
    "                )\n",
    "        self.smooth1 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(256, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True),\n",
    "                torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True)\n",
    "                )\n",
    "        self.smooth2 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(512, 128, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True),\n",
    "                torch.nn.Conv2d(128, 128, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True)\n",
    "                )\n",
    "        self.smooth3 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(1024, 256, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True),\n",
    "                torch.nn.Conv2d(256, 256, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True)\n",
    "                )\n",
    "        \n",
    "        \n",
    "        self.final = torch.nn.Conv2d(64, nClasses, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.tensor): A tensor of size (batch, 3, H, W)\n",
    "        Returns:\n",
    "            reg_out (torch.tensor): A tensor with results of the regression (batch, 4).\n",
    "            cls_out (torch.tensor): A tensor with results of the classification (batch, 2).\n",
    "        \"\"\"\n",
    "        \n",
    "        feat0 = self.vgg0(x)\n",
    "        feat1 = self.vgg1(feat0)\n",
    "        feat2 = self.vgg2(feat1)\n",
    "        feat3 = self.vgg3(feat2)\n",
    "        feat4 = self.vgg4(feat3)\n",
    "        \n",
    "        _,_,H,W = feat3.size()\n",
    "        up3 = torch.nn.functional.interpolate(feat4, size=(H,W), mode='bilinear', align_corners=False)\n",
    "        concat3 = torch.cat([feat3, up3], 1)\n",
    "        end3 = self.smooth3(concat3)\n",
    "        \n",
    "        _,_,H,W = feat2.size()\n",
    "        up2 = torch.nn.functional.interpolate(end3, size=(H,W), mode='bilinear', align_corners=False)\n",
    "        concat2 = torch.cat([feat2, up2], 1)\n",
    "        end2 = self.smooth2(concat2)\n",
    "        \n",
    "        _,_,H,W = feat1.size()\n",
    "        up1 = torch.nn.functional.interpolate(end2, size=(H,W), mode='bilinear', align_corners=False)\n",
    "        concat1 = torch.cat([feat1, up1], 1)\n",
    "        end1 = self.smooth1(concat1)\n",
    "        \n",
    "        _,_,H,W = feat0.size()\n",
    "        up0 = torch.nn.functional.interpolate(end1, size=(H,W), mode='bilinear', align_corners=False)\n",
    "        concat0 = torch.cat([feat0, up0], 1)\n",
    "        end0 = self.smooth0(concat0)\n",
    "        \n",
    "        return self.final(end0)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def eval_net_with_loss(model, inp, gt, class_weights, device):\n",
    "        \"\"\"\n",
    "        Evaluate network including loss.\n",
    "        \n",
    "        Args:\n",
    "            model (torch.nn.Module): The model.\n",
    "            inp (torch.tensor): A tensor (float32) of size (batch, 3, H, W)\n",
    "            gt (torch.tensor): A tensor (long) of size (batch, 1, H, W) with the groud truth (0 to num_classes-1).\n",
    "            class_weights (list of float): A list with len == num_classes.\n",
    "            device (torch.device): device to perform computation\n",
    "            \n",
    "        Returns:\n",
    "            out (torch.tensor): Network output.\n",
    "            loss (torch.tensor): Tensor with the total loss.\n",
    "                \n",
    "        \"\"\"\n",
    "        weights = torch.from_numpy(np.array(class_weights, dtype=np.float32)).to(device)\n",
    "        out = model(inp)\n",
    "        \n",
    "        softmax = torch.nn.functional.log_softmax(out, dim = 1)\n",
    "        loss = torch.nn.functional.nll_loss(softmax, gt, ignore_index=-1, weight=weights)\n",
    "            \n",
    "        return (out, loss)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_params_by_kind(model, n_base = 7):\n",
    "    \n",
    "        base_vgg_bias = []\n",
    "        base_vgg_weight = []\n",
    "        core_weight = []\n",
    "        core_bias = []\n",
    "    \n",
    "        for name, param in model.named_parameters():\n",
    "            if 'vgg' in name and ('weight' in name or 'bias' in name):\n",
    "                vgglayer = int(name.split('.')[-2])\n",
    "                \n",
    "                if vgglayer <= n_base:\n",
    "                    if 'bias' in name:\n",
    "                        print('Adding %s to base vgg bias.' % (name))\n",
    "                        base_vgg_bias.append(param)\n",
    "                    else:\n",
    "                        base_vgg_weight.append(param)\n",
    "                        print('Adding %s to base vgg weight.' % (name))\n",
    "                else:\n",
    "                    if 'bias' in name:\n",
    "                        print('Adding %s to core bias.' % (name))\n",
    "                        core_bias.append(param)\n",
    "                    else:\n",
    "                        print('Adding %s to core weight.' % (name))\n",
    "                        core_weight.append(param)\n",
    "                        \n",
    "            elif ('weight' in name or 'bias' in name):\n",
    "                if 'bias' in name:\n",
    "                    print('Adding %s to core bias.' % (name))\n",
    "                    core_bias.append(param)\n",
    "                else:\n",
    "                    print('Adding %s to core weight.' % (name))\n",
    "                    core_weight.append(param)\n",
    "                    \n",
    "        return (base_vgg_weight, base_vgg_bias, core_weight, core_bias)\n",
    "    \n",
    "# End class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realiza o Treinamento da Rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de amostras no dataset de treinamento: 20\n",
      "Arquivos no dataset de treinamento: ['DJI_0026.JPG', 'DJI_0026.json', 'DJI_0064.JPG', 'DJI_0064.json', 'DJI_0089.JPG', 'DJI_0089.json', 'DJI_0105.JPG', 'DJI_0105.json', 'DJI_0122.JPG', 'DJI_0122.json', 'DJI_0160.JPG', 'DJI_0160.json', 'DJI_0161.JPG', 'DJI_0161.json', 'DJI_0165.JPG', 'DJI_0165.json', 'DJI_0211.JPG', 'DJI_0211.json', 'DJI_0214.JPG', 'DJI_0214.json', 'DJI_0217.JPG', 'DJI_0217.json', 'DJI_0233.JPG', 'DJI_0233.json', 'DJI_0234.JPG', 'DJI_0234.json', 'DJI_0237.JPG', 'DJI_0237.json', 'DJI_0252.JPG', 'DJI_0252.json', 'DJI_0402.JPG', 'DJI_0402.json', 'DJI_0432.JPG', 'DJI_0432.json', 'DJI_0433.JPG', 'DJI_0433.json', 'DJI_0435.JPG', 'DJI_0435.json', 'DJI_0444.JPG', 'DJI_0444.json']\n",
      "Número de amostras no dataset de validação: 10\n",
      "Arquivos no dataset de validação: ['DJI_0052.JPG', 'DJI_0052.JPGGT_.png', 'DJI_0052.JPGIMG_.png', 'DJI_0052.json', 'DJI_0085.JPG', 'DJI_0085.JPGGT_.png', 'DJI_0085.JPGIMG_.png', 'DJI_0085.json', 'DJI_0107.JPG', 'DJI_0107.JPGGT_.png', 'DJI_0107.JPGIMG_.png', 'DJI_0107.json', 'DJI_0124.JPG', 'DJI_0124.JPGGT_.png', 'DJI_0124.JPGIMG_.png', 'DJI_0124.json', 'DJI_0163.JPG', 'DJI_0163.JPGGT_.png', 'DJI_0163.JPGIMG_.png', 'DJI_0163.json', 'DJI_0212.JPG', 'DJI_0212.JPGGT_.png', 'DJI_0212.JPGIMG_.png', 'DJI_0212.json', 'DJI_0215.JPG', 'DJI_0215.JPGGT_.png', 'DJI_0215.JPGIMG_.png', 'DJI_0215.json', 'DJI_0236.JPG', 'DJI_0236.JPGGT_.png', 'DJI_0236.JPGIMG_.png', 'DJI_0236.json', 'DJI_0251.JPG', 'DJI_0251.JPGGT_.png', 'DJI_0251.JPGIMG_.png', 'DJI_0251.json', 'DJI_0434.JPG', 'DJI_0434.JPGGT_.png', 'DJI_0434.JPGIMG_.png', 'DJI_0434.json']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plot_train:\n\u001b[1;32m---> 19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_batched\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimage_np\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_batched\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_original\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_batched\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\git\\image-segmentation\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\git\\image-segmentation\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\git\\image-segmentation\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\git\\image-segmentation\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[6], line 40\u001b[0m, in \u001b[0;36mSegmentationDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     38\u001b[0m \t\u001b[38;5;66;03m# Abre imagem\u001b[39;00m\n\u001b[0;32m     39\u001b[0m         img_np \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_folder, img_number_str \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mIMREAD_IGNORE_ORIENTATION \u001b[38;5;241m+\u001b[39m cv2\u001b[38;5;241m.\u001b[39mIMREAD_COLOR)\n\u001b[1;32m---> 40\u001b[0m         original_shape \u001b[38;5;241m=\u001b[39m \u001b[43mimg_np\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[0;32m     41\u001b[0m         img_np \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(img_np, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolution[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolution[\u001b[38;5;241m1\u001b[39m]))[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     42\u001b[0m         img_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(img_np)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Inicializar listas para armazenar a perda e a precisão\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Inicia o treinamento\n",
    "train_dataset = SegmentationDataset(img_folder_train, img_folder_train, True, class_to_id, resolution_input, True)\n",
    "print(f\"Número de amostras no dataset de treinamento: {len(train_dataset)}\")\n",
    "print(f\"Arquivos no dataset de treinamento: {os.listdir(img_folder_train)}\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, drop_last=True)\n",
    "\n",
    "val_dataset = SegmentationDataset(img_folder_val, img_folder_val, False, class_to_id, resolution_input)\n",
    "print(f\"Número de amostras no dataset de validação: {len(val_dataset)}\")\n",
    "print(f\"Arquivos no dataset de validação: {os.listdir(img_folder_val)}\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=0, drop_last=False)\n",
    "\n",
    "if plot_train:\n",
    "\n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "    \n",
    "            image_np = np.squeeze(sample_batched['image_original'].cpu().numpy())\n",
    "            gt = np.squeeze(sample_batched['gt'].cpu().numpy())\n",
    "                \n",
    "            color_label = np.zeros((resolution_input[1], resolution_input[0], 3))\n",
    "            \n",
    "            for key, val in id_to_class.items():\n",
    "                color_label[gt == key] = class_to_color[val]\n",
    "                \n",
    "            plt.figure()\n",
    "            plt.imshow((image_np/255) * 0.5 + (color_label/255) * 0.5)\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.imshow(color_label.astype(np.uint8))\n",
    "            plt.show()\n",
    "\n",
    "  \n",
    "model = UNetVgg(nClasses).to(device)\n",
    "\n",
    "core_lr = 0.02\n",
    "base_vgg_weight, base_vgg_bias, core_weight, core_bias = UNetVgg.get_params_by_kind(model, 7)\n",
    "\n",
    "optimizer = torch.optim.SGD([{'params': base_vgg_bias, 'lr': 0.000001}, \n",
    "                             {'params': base_vgg_weight, 'lr': 0.000001},\n",
    "                             {'params': core_bias, 'lr': core_lr},\n",
    "                             {'params': core_weight, 'lr': core_lr, 'weight_decay': 0.0005}], momentum=0.9)\n",
    "    \n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 30, gamma=0.2)\n",
    "\n",
    "\n",
    "best_val_acc = -1\n",
    "best_epoch = 0\n",
    "\n",
    "# Start training...\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    print('Epoch %d starting...' % (epoch+1))\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    mean_loss = 0.0\n",
    "    \n",
    "    n_correct = 0\n",
    "    n_false = 0\n",
    "    \n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "    \n",
    "    \n",
    "        image = sample_batched['image'].to(device)\n",
    "        gt = sample_batched['gt'].to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        output, total_loss = model.eval_net_with_loss(model, image, gt, class_weights, device)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        mean_loss += total_loss.cpu().detach().numpy()\n",
    "        \n",
    "        # Measure accuracy\n",
    "        \n",
    "        gt = np.squeeze(sample_batched['gt'].cpu().numpy())\n",
    "        \n",
    "        label_out = torch.nn.functional.softmax(output, dim = 1)\n",
    "        label_out = label_out.cpu().detach().numpy()\n",
    "        label_out = np.squeeze(label_out)\n",
    "        \n",
    "        labels = np.argmax(label_out, axis=0)\n",
    "        valid_mask = gt != -1\n",
    "        curr_correct = np.sum(gt[valid_mask] == labels[valid_mask])\n",
    "        curr_false = np.sum(valid_mask) - curr_correct\n",
    "        n_correct += curr_correct\n",
    "        n_false += curr_false\n",
    "        \n",
    "    mean_loss /= len(train_loader)\n",
    "    train_acc = n_correct / (n_correct + n_false)\n",
    "        \n",
    "    print('Train loss: %f, train acc: %f' % (mean_loss, train_acc))\n",
    "    # Armazenar a perda e a precisão de treinamento\n",
    "    train_losses.append(mean_loss)\n",
    "    train_accuracies.append(train_acc)    \n",
    "    \n",
    "    n_correct = 0\n",
    "    n_false = 0\n",
    "    \n",
    "    \n",
    "    for i_batch, sample_batched in enumerate(val_loader):\n",
    "    \n",
    "    \n",
    "        image = sample_batched['image'].to(device)\n",
    "        image_np = np.squeeze(sample_batched['image_original'].cpu().numpy())\n",
    "        gt = np.squeeze(sample_batched['gt'].cpu().numpy())\n",
    "        \n",
    "    \n",
    "        label_out = model(image)\n",
    "        label_out = torch.nn.functional.softmax(label_out, dim = 1)\n",
    "        label_out = label_out.cpu().detach().numpy()\n",
    "        label_out = np.squeeze(label_out)\n",
    "        \n",
    "        labels = np.argmax(label_out, axis=0)\n",
    "        \n",
    "        if plot_val:\n",
    "            \n",
    "            color_label = np.zeros((resolution_input[1], resolution_input[0], 3))\n",
    "            \n",
    "            for key, val in id_to_class.items():\n",
    "                color_label[labels == key] = class_to_color[val]\n",
    "                \n",
    "            plt.figure()\n",
    "            plt.imshow((image_np/255) * 0.5 + (color_label/255) * 0.5)\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.imshow(color_label.astype(np.uint8))\n",
    "            plt.show()\n",
    "        \n",
    "        valid_mask = gt != -1\n",
    "        curr_correct = np.sum(gt[valid_mask] == labels[valid_mask])\n",
    "        curr_false = np.sum(valid_mask) - curr_correct\n",
    "        n_correct += curr_correct\n",
    "        n_false += curr_false    \n",
    "        \n",
    "    total_acc = n_correct / (n_correct + n_false)\n",
    "    val_accuracies.append(total_acc)\n",
    "    \n",
    "    if best_val_acc < total_acc:\n",
    "        best_val_acc = total_acc\n",
    "        if epoch > 7:\n",
    "            torch.save(model.state_dict(), model_file_name)\n",
    "            print('Nova melhor conta de validação. Salvo... %f', epoch)\n",
    "        best_epoch = epoch\n",
    "\n",
    "    if (epoch - best_epoch) > patience:\n",
    "        print(f\"Terminando o treinamento, melhor conta de validação {best_val_acc:.6f}\")\n",
    "        break\n",
    "    \n",
    "    print('Validação Acc: %f -- Melhor Avaliação Acc: %f -- epoch %d.' % (total_acc, best_val_acc, best_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotar os gráficos de perda e precisão\n",
    "    Inicialização das listas: train_losses, train_accuracies e val_accuracies são listas para armazenar a perda e a precisão de treinamento e validação em cada época.\n",
    "    \n",
    "    Armazenamento dos valores: Durante o loop de treinamento, a perda e a precisão são calculadas e armazenadas nas listas correspondentes.\n",
    "    Plotagem dos gráficos: Após o loop de treinamento, os gráficos de perda e precisão são plotados usando matplotlib.\n",
    "\n",
    "Este código deve ser adicionado ao final do seu loop de treinamento no notebook para visualizar os resultados do treinamento ao longo das épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Perda de Treinamento')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Perda')\n",
    "plt.title('Perda de Treinamento ao longo das Épocas')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Precisão de Treinamento')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Precisão de Validação')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Precisão')\n",
    "plt.title('Precisão de Treinamento e Validação ao longo das Épocas')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferência de dados\n",
    "Processo de usar um modelo treinado para fazer previsões sobre novos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNetVgg(nClasses)\n",
    "model.load_state_dict(torch.load(model_file_name))\n",
    "model.eval()\n",
    "print(\"Modelo carregado e pronto para uso.\")\n",
    "model.to(device)\n",
    "\n",
    "img_list = glob.glob(osp.join(img_folder_val, '*.png'))\n",
    "\n",
    "for img_path in img_list:\n",
    "\n",
    "        img_np = cv2.imread(img_path, cv2.IMREAD_IGNORE_ORIENTATION + cv2.IMREAD_COLOR)\n",
    "        img_np = cv2.resize(img_np, (resolution_input[0], resolution_input[1]))[..., ::-1]\n",
    "        img_np = np.ascontiguousarray(img_np)\n",
    "        \n",
    "        img_pt = np.copy(img_np).astype(np.float32) / 255.0\n",
    "        for i in range(3):\n",
    "            img_pt[..., i] -= mean[i]\n",
    "            img_pt[..., i] /= std[i]\n",
    "            \n",
    "        img_pt = img_pt.transpose(2,0,1)\n",
    "            \n",
    "        img_pt = torch.from_numpy(img_pt[None, ...]).to(device)\n",
    "        \n",
    "        label_out = model(img_pt)\n",
    "        label_out = torch.nn.functional.softmax(label_out, dim = 1)\n",
    "        label_out = label_out.cpu().detach().numpy()\n",
    "        label_out = np.squeeze(label_out)\n",
    "        \n",
    "        labels = np.argmax(label_out, axis=0)\n",
    "        \n",
    "        color_label = np.zeros((resolution_input[1], resolution_input[0], 3))\n",
    "            \n",
    "        for key, val in id_to_class.items():\n",
    "            color_label[labels == key] = class_to_color[val]\n",
    "            \n",
    "        plt.figure()\n",
    "        plt.imshow((img_np/255) * 0.5 + (color_label/255) * 0.5)\n",
    "        plt.savefig(save_dir + \"IMG\" + \".png\")\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.imshow(color_label.astype(np.uint8))\n",
    "        plt.savefig(save_dir + \"GT\" + \".png\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
