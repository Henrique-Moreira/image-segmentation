{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guia de Estudo: Compreendendo a Arquitetura VGG_UNET\n",
    "## Questionário\n",
    "1) Qual a função principal do arquivo VGG_UNET.py?\n",
    "2) Descreva a relação entre os arquivos train.py e data_utils.py no processo de treinamento da rede neural.\n",
    "3) O que se pode esperar encontrar no arquivo inference_sample.py?\n",
    "4) Com base nos nomes dos arquivos, qual a arquitetura de rede neural utilizada neste projeto? Descreva suas características principais.\n",
    "5) Para que serve a arquitetura UNET em aplicações de aprendizado de máquina? Cite exemplos de áreas onde ela se destaca.\n",
    "6) Explique a importância do processo de treinamento em uma rede neural. Como ele se relaciona com os arquivos mencionados?\n",
    "7) O que são dados de treinamento e como o arquivo data_utils.py pode auxiliar nesse processo?\n",
    "8) Deduza a partir dos nomes dos arquivos qual a linguagem de programação utilizada neste projeto. Justifique sua resposta.\n",
    "9) Qual a provável finalidade do arquivo readme.md no contexto deste projeto?\n",
    "10) Imagine que você precisa modificar o conjunto de dados utilizado no treinamento. Qual arquivo você precisaria editar? Por quê?\n",
    "\n",
    "## Gabarito\n",
    "- O arquivo VGG_UNET.py define a arquitetura da rede neural, especificando as camadas, funções de ativação e outros parâmetros do modelo.\n",
    "- O arquivo train.py é responsável por executar o processo de treinamento da rede neural, utilizando os dados preparados pelo arquivo data_utils.py.\n",
    "- O arquivo inference_sample.py provavelmente contém um exemplo de como utilizar a rede neural treinada para realizar inferências em novos dados.\n",
    "- A arquitetura utilizada é a VGG_UNET, uma combinação da rede VGG (rede convolucional profunda) com a arquitetura UNET (eficiente para segmentação de imagens).\n",
    "- A arquitetura UNET é comumente utilizada para tarefas de segmentação de imagens, como em imagens médicas, detecção de objetos e processamento de imagens de satélite.\n",
    "- O treinamento é a etapa onde a rede neural \"aprende\" a partir dos dados fornecidos, ajustando seus parâmetros para realizar a tarefa desejada. O arquivo train.py executa o processo, utilizando a arquitetura definida em VGG_UNET.py e os dados preparados por data_utils.py.\n",
    "- Dados de treinamento são o conjunto de exemplos utilizados para treinar a rede neural. O arquivo data_utils.py auxilia no processamento, organização e carregamento desses dados para o treinamento.\n",
    "- A linguagem de programação utilizada é provavelmente Python, visto que a extensão \".py\" é comumente associada a arquivos de código Python.\n",
    "- O arquivo readme.md serve como um guia introdutório ao projeto, contendo informações sobre os arquivos, como utilizá-los e outras informações relevantes.\n",
    "- Para modificar o conjunto de dados, seria necessário editar o arquivo data_utils.py, pois ele é responsável por carregar e preparar os dados para o treinamento.\n",
    "  \n",
    "## Questões para Dissertação\n",
    "- Discuta as vantagens e desvantagens de utilizar a arquitetura VGG_UNET em comparação com outras arquiteturas de redes neurais para tarefas de segmentação de imagens.\n",
    "- Explique em detalhes como o processo de treinamento da rede neural é realizado no contexto dos arquivos mencionados. Quais parâmetros são ajustados e como o desempenho da rede é avaliado?\n",
    "- Descreva o papel das funções de ativação em uma rede neural e discuta a importância da escolha adequada dessas funções para o sucesso do treinamento.\n",
    "- Com base nos nomes dos arquivos, especule sobre o tipo de dados que este projeto utiliza para treinamento e inferência. Que tipo de pré-processamento de dados pode ser necessário para preparar os dados para a rede neural?\n",
    "- Imagine que você precisa integrar este projeto a um sistema maior. Descreva como você faria a interface entre a rede neural treinada e outros componentes do sistema, considerando os arquivos mencionados.\n",
    "## Glossário\n",
    "- VGG_UNET: Arquitetura de rede neural que combina a rede VGG com a arquitetura UNET, geralmente utilizada para segmentação de imagens.\n",
    "- Rede Neural: Modelo computacional inspirado no cérebro humano, capaz de aprender a partir de dados e realizar tarefas como classificação, regressão e segmentação.\n",
    "- Treinamento: Processo de ajuste dos parâmetros da rede neural para que ela aprenda a realizar a tarefa desejada.\n",
    "- Dados de Treinamento: Conjunto de dados utilizado para treinar a rede neural.\n",
    "- Inferência: Processo de utilizar a rede neural treinada para realizar predições em novos dados.\n",
    "- Segmentação de Imagens: Tarefa de dividir uma imagem em diferentes regiões, cada uma representando um objeto ou classe diferente.\n",
    "- Função de Ativação: Função matemática que introduz não-linearidade na rede neural, permitindo que ela aprenda relações complexas nos dados.\n",
    "- Python: Linguagem de programação popularmente utilizada em projetos de aprendizado de máquina e ciência de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalação de Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\python312\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\henrique moreira\\appdata\\roaming\\python\\python312\\site-packages (3.9.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\henrique moreira\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\henrique moreira\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\henrique moreira\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\henrique moreira\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\python312\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python312\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\henrique moreira\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\henrique moreira\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\henrique moreira\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\python312\\lib\\site-packages (2.4.0+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\henrique moreira\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\henrique moreira\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\henrique moreira\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\henrique moreira\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\henrique moreira\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from torch) (70.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\python312\\lib\\site-packages (0.19.0+cu118)\n",
      "Requirement already satisfied: numpy in c:\\python312\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.4.0+cu118 in c:\\python312\\lib\\site-packages (from torchvision) (2.4.0+cu118)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\henrique moreira\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\henrique moreira\\appdata\\roaming\\python\\python312\\site-packages (from torch==2.4.0+cu118->torchvision) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python312\\lib\\site-packages (from torch==2.4.0+cu118->torchvision) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\henrique moreira\\appdata\\roaming\\python\\python312\\site-packages (from torch==2.4.0+cu118->torchvision) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\henrique moreira\\appdata\\roaming\\python\\python312\\site-packages (from torch==2.4.0+cu118->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\henrique moreira\\appdata\\roaming\\python\\python312\\site-packages (from torch==2.4.0+cu118->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\henrique moreira\\appdata\\roaming\\python\\python312\\site-packages (from torch==2.4.0+cu118->torchvision) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from torch==2.4.0+cu118->torchvision) (70.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from jinja2->torch==2.4.0+cu118->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python312\\lib\\site-packages (from sympy->torch==2.4.0+cu118->torchvision) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: utils2 in c:\\python312\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: six in c:\\python312\\lib\\site-packages (from utils2) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\henrique moreira\\appdata\\roaming\\python\\python312\\site-packages (from utils2) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in c:\\python312\\lib\\site-packages (from utils2) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests->utils2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests->utils2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests->utils2) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests->utils2) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\henrique moreira\\appdata\\roaming\\python\\python312\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\python312\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install torch\n",
    "%pip install torchvision\n",
    "%pip install utils2\n",
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importe os módulos necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas\n",
    "import os\n",
    "import os.path as osp\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaração de Variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório do Projeto e:\\Mestrado\\Materias\\2024-02-Sistemas para Processamento Multimidia\\seminario\\image-segmentation\\dataset\\nematode.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# CUDA:\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Caminho do diretório Dataset:\n",
    "directory = os.path.abspath(os.path.join(os.getcwd(), '..')) + r'\\dataset\\nematode'\n",
    "print(f'Diretório do Projeto {directory}.')\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "img_folder_val = directory + r'\\Val'\n",
    "img_folder_train = directory + r'\\Train'\n",
    "img_folder_test = directory + r'\\Test'\n",
    "save_dir = directory + r'\\\\result_unet\\\\'\n",
    "if not os.path.exists(img_folder_val):\n",
    "    os.makedirs(img_folder_val)\n",
    "if not os.path.exists(img_folder_train):\n",
    "    os.makedirs(img_folder_train)\n",
    "if not os.path.exists(img_folder_test):\n",
    "    os.makedirs(img_folder_test)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Local onde o Modelo será salvo\n",
    "model_file_name = save_dir + 'model_unet.pth'\n",
    "\n",
    "# size mismatch (got input: [1, 3, 480, 640] , target: [1, 960, 1280]\n",
    "resolution_input = (640, 480) # Size Out (1280, 960)\n",
    "\n",
    "patience = 30\n",
    "plot_val = True\n",
    "plot_train = True\n",
    "\n",
    "max_epochs = 100\n",
    "\n",
    "#Width x Height - MUST be divisible by 32\n",
    "class_weights = [1, 1, 1]\n",
    "nClasses = 3\n",
    "\n",
    "# Color in RGB\n",
    "class_to_color = {'Ground': (127, 0, 0) , 'Healthy': (0, 127, 127), 'Pest': (0, 255, 0)}\n",
    "class_to_id = {'Ground': 0, 'Healthy': 1, 'Pest': 2}\n",
    "id_to_class = {v: k for k, v in class_to_id.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase para Segmentação de Dataset\n",
    "\n",
    "    Este arquivo provavelmente contém funções utilitárias para manipular e preparar os dados para o treinamento do modelo. As utilidades de dados aqui podem incluir:\n",
    "        - Carregamento de dados de diferentes fontes (arquivos, bancos de dados, APIs).\n",
    "        - Limpeza e pré-processamento de dados (por exemplo, lidar com valores ausentes, normalização, conversão de tipos de dados).\n",
    "        - Aumento de dados para aumentar o tamanho do conjunto de dados de treinamento.\n",
    "        - Divisão dos dados em conjuntos de treinamento, validação e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    \"\"\"Segmentation dataset loader.\"\"\"\n",
    "\n",
    "    def __init__(self, json_folder, img_folder, is_train, class_to_id, resolution_input = (640, 480), augmentation = False, transform=None):\n",
    "    #def __init__(self, json_folder, img_folder, is_train, class_to_id, resolution_input = (1280, 960), augmentation = False, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            json_folder (str): Path to folder that contains the annotations.\n",
    "            img_folder (str): Path to all images.\n",
    "            is_train (bool): Is this a training dataset ?\n",
    "            augmentation (bool): Do dataset augmentation (crete artificial variance) ?\n",
    "        \"\"\"\n",
    "\n",
    "        self.gt_file_list = glob.glob(osp.join(json_folder, '*.json'))\n",
    "\n",
    "        self.total_samples = len(self.gt_file_list)\n",
    "        self.img_folder = img_folder\n",
    "        self.is_train = is_train\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "        self.resolution = resolution_input\n",
    "        self.class_to_id = class_to_id\n",
    "        \n",
    "        \n",
    "        # Mean and std are needed because we start from a pre trained net\n",
    "        self.mean = [0.485, 0.456, 0.406]\n",
    "        self.std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        gt_file = self.gt_file_list[idx]\n",
    "        img_number_str = gt_file.split('.')[0].split('/')[-1]\n",
    "\t# Abre Json\n",
    "        gt_json = json.load(open(gt_file, 'r'))\n",
    "\t# Abre imagem\n",
    "        img_np = cv2.imread(osp.join(self.img_folder, img_number_str + '.png'), cv2.IMREAD_IGNORE_ORIENTATION + cv2.IMREAD_COLOR)\n",
    "        original_shape = img_np.shape\n",
    "        img_np = cv2.resize(img_np, (self.resolution[0], self.resolution[1]))[..., ::-1]\n",
    "        img_np = np.ascontiguousarray(img_np)\n",
    "\t# Cria imagem zerada\n",
    "        label_np = np.zeros((img_np.shape[0], img_np.shape[1]))\n",
    "        label_np[...] = -1\n",
    "        \n",
    "\t# Para todos poligonos\n",
    "        for shape in gt_json['shapes']:\n",
    "            # Transforma os pontos do poligono em array\n",
    "            points_np = np.array(shape['points'], dtype = np.float64)\n",
    "\n",
    "\t    # Ajusta os pontos porque eu mudo o resolucao (pode ignorar)\n",
    "            points_np[:, 0] *= self.resolution[0]/original_shape[1]\n",
    "            points_np[:, 1] *= self.resolution[1]/original_shape[0]\n",
    "\t    # As coordenadas dos pontos que formam o poligono tem que ser inteiros\n",
    "            points_np = np.round(points_np).astype(np.int64)\n",
    "\t    # Coloca os pontos no formato certo para o opencv\n",
    "            points_np = points_np.reshape((-1,1,2))\n",
    "\t    # Pinta o poligono usando o opencv com o valor referente ao label\n",
    "            label_np = cv2.fillPoly(label_np, [points_np], self.class_to_id[shape['label']])\n",
    "\n",
    "        # Transforma o GT em inteiro    \n",
    "        label_np = label_np.astype(np.int32)\n",
    "        \n",
    "        if self.is_train and self.augmentation:\n",
    "            if np.random.rand() > 0.5:\n",
    "                img_np = np.fliplr(img_np)\n",
    "                label_np = np.fliplr(label_np)\n",
    "                img_np = np.ascontiguousarray(img_np)\n",
    "                label_np = np.ascontiguousarray(label_np)\n",
    "        \n",
    "        img_pt = img_np.astype(np.float32) / 255.0\n",
    "        for i in range(3):\n",
    "            img_pt[..., i] -= self.mean[i]\n",
    "            img_pt[..., i] /= self.std[i]\n",
    "            \n",
    "        img_pt = img_pt.transpose(2,0,1)\n",
    "            \n",
    "        img_pt = torch.from_numpy(img_pt)\n",
    "        label_pt = torch.from_numpy(label_np).long()\n",
    "        #print(img_number_str, img_pt.shape)\n",
    "\n",
    "        sample = {'image': img_pt, 'gt': label_pt, 'image_original': img_np}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG_UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetVgg(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    BorderNetwork is a NN that aims to detected border and classify occlusion.\n",
    "    The architecture is a VGG without the last pool layer. After that we \n",
    "    have two paths, one for regression and one for classification (occlusion).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(UNetVgg, self).__init__()\n",
    "        \n",
    "        vgg16pre = torchvision.models.vgg16(pretrained=True)\n",
    "        self.vgg0 = torch.nn.Sequential(*list(vgg16pre.features.children())[:4])\n",
    "        self.vgg1 = torch.nn.Sequential(*list(vgg16pre.features.children())[4:9])\n",
    "        self.vgg2 = torch.nn.Sequential(*list(vgg16pre.features.children())[9:16])\n",
    "        self.vgg3 = torch.nn.Sequential(*list(vgg16pre.features.children())[16:23])\n",
    "        self.vgg4 = torch.nn.Sequential(*list(vgg16pre.features.children())[23:30])\n",
    "        \n",
    "        \n",
    "        self.smooth0 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(128, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True),\n",
    "                torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True)\n",
    "                )\n",
    "        self.smooth1 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(256, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True),\n",
    "                torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True)\n",
    "                )\n",
    "        self.smooth2 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(512, 128, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True),\n",
    "                torch.nn.Conv2d(128, 128, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True)\n",
    "                )\n",
    "        self.smooth3 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(1024, 256, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True),\n",
    "                torch.nn.Conv2d(256, 256, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True)\n",
    "                )\n",
    "        \n",
    "        \n",
    "        self.final = torch.nn.Conv2d(64, nClasses, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.tensor): A tensor of size (batch, 3, H, W)\n",
    "        Returns:\n",
    "            reg_out (torch.tensor): A tensor with results of the regression (batch, 4).\n",
    "            cls_out (torch.tensor): A tensor with results of the classification (batch, 2).\n",
    "        \"\"\"\n",
    "        \n",
    "        feat0 = self.vgg0(x)\n",
    "        feat1 = self.vgg1(feat0)\n",
    "        feat2 = self.vgg2(feat1)\n",
    "        feat3 = self.vgg3(feat2)\n",
    "        feat4 = self.vgg4(feat3)\n",
    "        \n",
    "        _,_,H,W = feat3.size()\n",
    "        up3 = torch.nn.functional.interpolate(feat4, size=(H,W), mode='bilinear', align_corners=False)\n",
    "        concat3 = torch.cat([feat3, up3], 1)\n",
    "        end3 = self.smooth3(concat3)\n",
    "        \n",
    "        _,_,H,W = feat2.size()\n",
    "        up2 = torch.nn.functional.interpolate(end3, size=(H,W), mode='bilinear', align_corners=False)\n",
    "        concat2 = torch.cat([feat2, up2], 1)\n",
    "        end2 = self.smooth2(concat2)\n",
    "        \n",
    "        _,_,H,W = feat1.size()\n",
    "        up1 = torch.nn.functional.interpolate(end2, size=(H,W), mode='bilinear', align_corners=False)\n",
    "        concat1 = torch.cat([feat1, up1], 1)\n",
    "        end1 = self.smooth1(concat1)\n",
    "        \n",
    "        _,_,H,W = feat0.size()\n",
    "        up0 = torch.nn.functional.interpolate(end1, size=(H,W), mode='bilinear', align_corners=False)\n",
    "        concat0 = torch.cat([feat0, up0], 1)\n",
    "        end0 = self.smooth0(concat0)\n",
    "        \n",
    "        return self.final(end0)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def eval_net_with_loss(model, inp, gt, class_weights, device):\n",
    "        \"\"\"\n",
    "        Evaluate network including loss.\n",
    "        \n",
    "        Args:\n",
    "            model (torch.nn.Module): The model.\n",
    "            inp (torch.tensor): A tensor (float32) of size (batch, 3, H, W)\n",
    "            gt (torch.tensor): A tensor (long) of size (batch, 1, H, W) with the groud truth (0 to num_classes-1).\n",
    "            class_weights (list of float): A list with len == num_classes.\n",
    "            device (torch.device): device to perform computation\n",
    "            \n",
    "        Returns:\n",
    "            out (torch.tensor): Network output.\n",
    "            loss (torch.tensor): Tensor with the total loss.\n",
    "                \n",
    "        \"\"\"\n",
    "        weights = torch.from_numpy(np.array(class_weights, dtype=np.float32)).to(device)\n",
    "        out = model(inp)\n",
    "        \n",
    "        softmax = torch.nn.functional.log_softmax(out, dim = 1)\n",
    "        loss = torch.nn.functional.nll_loss(softmax, gt, ignore_index=-1, weight=weights)\n",
    "            \n",
    "        return (out, loss)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_params_by_kind(model, n_base = 7):\n",
    "    \n",
    "        base_vgg_bias = []\n",
    "        base_vgg_weight = []\n",
    "        core_weight = []\n",
    "        core_bias = []\n",
    "    \n",
    "        for name, param in model.named_parameters():\n",
    "            if 'vgg' in name and ('weight' in name or 'bias' in name):\n",
    "                vgglayer = int(name.split('.')[-2])\n",
    "                \n",
    "                if vgglayer <= n_base:\n",
    "                    if 'bias' in name:\n",
    "                        print('Adding %s to base vgg bias.' % (name))\n",
    "                        base_vgg_bias.append(param)\n",
    "                    else:\n",
    "                        base_vgg_weight.append(param)\n",
    "                        print('Adding %s to base vgg weight.' % (name))\n",
    "                else:\n",
    "                    if 'bias' in name:\n",
    "                        print('Adding %s to core bias.' % (name))\n",
    "                        core_bias.append(param)\n",
    "                    else:\n",
    "                        print('Adding %s to core weight.' % (name))\n",
    "                        core_weight.append(param)\n",
    "                        \n",
    "            elif ('weight' in name or 'bias' in name):\n",
    "                if 'bias' in name:\n",
    "                    print('Adding %s to core bias.' % (name))\n",
    "                    core_bias.append(param)\n",
    "                else:\n",
    "                    print('Adding %s to core weight.' % (name))\n",
    "                    core_weight.append(param)\n",
    "                    \n",
    "        return (base_vgg_weight, base_vgg_bias, core_weight, core_bias)\n",
    "    \n",
    "# End class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realiza o Treinamento da Rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henrique Moreira\\AppData\\Local\\Temp\\ipykernel_2264\\24345260.py:28: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "C:\\Users\\Henrique Moreira\\AppData\\Local\\Temp\\ipykernel_2264\\24345260.py:32: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "C:\\Users\\Henrique Moreira\\AppData\\Local\\Temp\\ipykernel_2264\\24345260.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure()\n",
      "c:\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding vgg0.0.weight to base vgg weight.\n",
      "Adding vgg0.0.bias to base vgg bias.\n",
      "Adding vgg0.2.weight to base vgg weight.\n",
      "Adding vgg0.2.bias to base vgg bias.\n",
      "Adding vgg1.1.weight to base vgg weight.\n",
      "Adding vgg1.1.bias to base vgg bias.\n",
      "Adding vgg1.3.weight to base vgg weight.\n",
      "Adding vgg1.3.bias to base vgg bias.\n",
      "Adding vgg2.1.weight to base vgg weight.\n",
      "Adding vgg2.1.bias to base vgg bias.\n",
      "Adding vgg2.3.weight to base vgg weight.\n",
      "Adding vgg2.3.bias to base vgg bias.\n",
      "Adding vgg2.5.weight to base vgg weight.\n",
      "Adding vgg2.5.bias to base vgg bias.\n",
      "Adding vgg3.1.weight to base vgg weight.\n",
      "Adding vgg3.1.bias to base vgg bias.\n",
      "Adding vgg3.3.weight to base vgg weight.\n",
      "Adding vgg3.3.bias to base vgg bias.\n",
      "Adding vgg3.5.weight to base vgg weight.\n",
      "Adding vgg3.5.bias to base vgg bias.\n",
      "Adding vgg4.1.weight to base vgg weight.\n",
      "Adding vgg4.1.bias to base vgg bias.\n",
      "Adding vgg4.3.weight to base vgg weight.\n",
      "Adding vgg4.3.bias to base vgg bias.\n",
      "Adding vgg4.5.weight to base vgg weight.\n",
      "Adding vgg4.5.bias to base vgg bias.\n",
      "Adding smooth0.0.weight to core weight.\n",
      "Adding smooth0.0.bias to core bias.\n",
      "Adding smooth0.2.weight to core weight.\n",
      "Adding smooth0.2.bias to core bias.\n",
      "Adding smooth1.0.weight to core weight.\n",
      "Adding smooth1.0.bias to core bias.\n",
      "Adding smooth1.2.weight to core weight.\n",
      "Adding smooth1.2.bias to core bias.\n",
      "Adding smooth2.0.weight to core weight.\n",
      "Adding smooth2.0.bias to core bias.\n",
      "Adding smooth2.2.weight to core weight.\n",
      "Adding smooth2.2.bias to core bias.\n",
      "Adding smooth3.0.weight to core weight.\n",
      "Adding smooth3.0.bias to core bias.\n",
      "Adding smooth3.2.weight to core weight.\n",
      "Adding smooth3.2.bias to core bias.\n",
      "Adding final.weight to core weight.\n",
      "Adding final.bias to core bias.\n",
      "Epoch 1 starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.898745, train acc: 0.569166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henrique Moreira\\AppData\\Local\\Temp\\ipykernel_2264\\24345260.py:129: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "C:\\Users\\Henrique Moreira\\AppData\\Local\\Temp\\ipykernel_2264\\24345260.py:133: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validação Acc: 0.711366 -- Melhor Avaliação Acc: 0.711366 -- epoch 0.\n",
      "Epoch 2 starting...\n",
      "Train loss: 0.719971, train acc: 0.780063\n",
      "Validação Acc: 0.680961 -- Melhor Avaliação Acc: 0.711366 -- epoch 0.\n",
      "Epoch 3 starting...\n",
      "Train loss: 0.595612, train acc: 0.780754\n",
      "Validação Acc: 0.709776 -- Melhor Avaliação Acc: 0.711366 -- epoch 0.\n",
      "Epoch 4 starting...\n",
      "Train loss: 0.613455, train acc: 0.768594\n",
      "Validação Acc: 0.744808 -- Melhor Avaliação Acc: 0.744808 -- epoch 3.\n",
      "Epoch 5 starting...\n",
      "Train loss: 0.571205, train acc: 0.655476\n",
      "Validação Acc: 0.709776 -- Melhor Avaliação Acc: 0.744808 -- epoch 3.\n",
      "Epoch 6 starting...\n",
      "Train loss: 0.636805, train acc: 0.778439\n",
      "Validação Acc: 0.709776 -- Melhor Avaliação Acc: 0.744808 -- epoch 3.\n",
      "Epoch 7 starting...\n",
      "Train loss: 0.567895, train acc: 0.780797\n",
      "Validação Acc: 0.721079 -- Melhor Avaliação Acc: 0.744808 -- epoch 3.\n",
      "Epoch 8 starting...\n",
      "Train loss: 0.600120, train acc: 0.781015\n",
      "Validação Acc: 0.709776 -- Melhor Avaliação Acc: 0.744808 -- epoch 3.\n",
      "Epoch 9 starting...\n",
      "Train loss: 0.582203, train acc: 0.760188\n",
      "Validação Acc: 0.726462 -- Melhor Avaliação Acc: 0.744808 -- epoch 3.\n",
      "Epoch 10 starting...\n",
      "Train loss: 0.509189, train acc: 0.690436\n",
      "Validação Acc: 0.678680 -- Melhor Avaliação Acc: 0.744808 -- epoch 3.\n",
      "Epoch 11 starting...\n",
      "Train loss: 0.543604, train acc: 0.700843\n",
      "Validação Acc: 0.709776 -- Melhor Avaliação Acc: 0.744808 -- epoch 3.\n",
      "Epoch 12 starting...\n",
      "Train loss: 0.547150, train acc: 0.787620\n",
      "Validação Acc: 0.742629 -- Melhor Avaliação Acc: 0.744808 -- epoch 3.\n",
      "Epoch 13 starting...\n",
      "Train loss: 0.552440, train acc: 0.679202\n",
      "Nova melhor conta de validação. Salvo... %f 12\n",
      "Validação Acc: 0.761671 -- Melhor Avaliação Acc: 0.761671 -- epoch 12.\n",
      "Epoch 14 starting...\n",
      "Train loss: 0.528396, train acc: 0.814870\n",
      "Validação Acc: 0.752783 -- Melhor Avaliação Acc: 0.761671 -- epoch 12.\n",
      "Epoch 15 starting...\n",
      "Train loss: 0.519924, train acc: 0.811964\n",
      "Validação Acc: 0.710207 -- Melhor Avaliação Acc: 0.761671 -- epoch 12.\n",
      "Epoch 16 starting...\n",
      "Train loss: 0.505022, train acc: 0.718321\n",
      "Validação Acc: 0.751698 -- Melhor Avaliação Acc: 0.761671 -- epoch 12.\n",
      "Epoch 17 starting...\n",
      "Train loss: 0.538206, train acc: 0.795687\n",
      "Validação Acc: 0.709776 -- Melhor Avaliação Acc: 0.761671 -- epoch 12.\n",
      "Epoch 18 starting...\n",
      "Train loss: 0.521572, train acc: 0.800934\n",
      "Validação Acc: 0.709822 -- Melhor Avaliação Acc: 0.761671 -- epoch 12.\n",
      "Epoch 19 starting...\n",
      "Train loss: 0.559270, train acc: 0.773462\n",
      "Validação Acc: 0.670149 -- Melhor Avaliação Acc: 0.761671 -- epoch 12.\n",
      "Epoch 20 starting...\n",
      "Train loss: 0.632537, train acc: 0.778075\n",
      "Validação Acc: 0.709762 -- Melhor Avaliação Acc: 0.761671 -- epoch 12.\n",
      "Epoch 21 starting...\n",
      "Train loss: 0.608470, train acc: 0.689403\n",
      "Validação Acc: 0.709776 -- Melhor Avaliação Acc: 0.761671 -- epoch 12.\n",
      "Epoch 22 starting...\n",
      "Train loss: 0.586583, train acc: 0.778439\n",
      "Validação Acc: 0.709776 -- Melhor Avaliação Acc: 0.761671 -- epoch 12.\n",
      "Epoch 23 starting...\n",
      "Train loss: 0.542110, train acc: 0.771681\n",
      "Validação Acc: 0.732698 -- Melhor Avaliação Acc: 0.761671 -- epoch 12.\n",
      "Epoch 24 starting...\n",
      "Train loss: 0.505492, train acc: 0.693273\n",
      "Validação Acc: 0.717032 -- Melhor Avaliação Acc: 0.761671 -- epoch 12.\n",
      "Epoch 25 starting...\n",
      "Train loss: 0.528996, train acc: 0.736897\n",
      "Validação Acc: 0.733347 -- Melhor Avaliação Acc: 0.761671 -- epoch 12.\n",
      "Epoch 26 starting...\n",
      "Train loss: 0.493487, train acc: 0.741232\n",
      "Validação Acc: 0.731868 -- Melhor Avaliação Acc: 0.761671 -- epoch 12.\n",
      "Epoch 27 starting...\n",
      "Train loss: 0.451122, train acc: 0.727821\n",
      "Validação Acc: 0.702246 -- Melhor Avaliação Acc: 0.761671 -- epoch 12.\n",
      "Epoch 28 starting...\n",
      "Train loss: 0.660948, train acc: 0.738676\n",
      "Validação Acc: 0.709776 -- Melhor Avaliação Acc: 0.761671 -- epoch 12.\n",
      "Epoch 29 starting...\n",
      "Train loss: 0.582994, train acc: 0.778439\n",
      "Validação Acc: 0.709776 -- Melhor Avaliação Acc: 0.761671 -- epoch 12.\n",
      "Epoch 30 starting...\n",
      "Train loss: 0.557960, train acc: 0.778439\n",
      "Validação Acc: 0.709776 -- Melhor Avaliação Acc: 0.761671 -- epoch 12.\n",
      "Epoch 31 starting...\n",
      "Train loss: 0.511348, train acc: 0.778439\n",
      "Validação Acc: 0.709938 -- Melhor Avaliação Acc: 0.761671 -- epoch 12.\n",
      "Epoch 32 starting...\n",
      "Train loss: 0.463598, train acc: 0.802213\n",
      "Validação Acc: 0.749913 -- Melhor Avaliação Acc: 0.761671 -- epoch 12.\n",
      "Epoch 33 starting...\n",
      "Train loss: 0.470937, train acc: 0.708146\n",
      "Nova melhor conta de validação. Salvo... %f 32\n",
      "Validação Acc: 0.780625 -- Melhor Avaliação Acc: 0.780625 -- epoch 32.\n",
      "Epoch 34 starting...\n",
      "Train loss: 0.429262, train acc: 0.745939\n",
      "Nova melhor conta de validação. Salvo... %f 33\n",
      "Validação Acc: 0.824744 -- Melhor Avaliação Acc: 0.824744 -- epoch 33.\n",
      "Epoch 35 starting...\n",
      "Train loss: 0.395591, train acc: 0.763645\n",
      "Nova melhor conta de validação. Salvo... %f 34\n",
      "Validação Acc: 0.830494 -- Melhor Avaliação Acc: 0.830494 -- epoch 34.\n",
      "Epoch 36 starting...\n",
      "Train loss: 0.352531, train acc: 0.820544\n",
      "Validação Acc: 0.830234 -- Melhor Avaliação Acc: 0.830494 -- epoch 34.\n",
      "Epoch 37 starting...\n",
      "Train loss: 0.359861, train acc: 0.826668\n",
      "Validação Acc: 0.760243 -- Melhor Avaliação Acc: 0.830494 -- epoch 34.\n",
      "Epoch 38 starting...\n",
      "Train loss: 0.351297, train acc: 0.828538\n",
      "Validação Acc: 0.778649 -- Melhor Avaliação Acc: 0.830494 -- epoch 34.\n",
      "Epoch 39 starting...\n",
      "Train loss: 0.311617, train acc: 0.878531\n",
      "Nova melhor conta de validação. Salvo... %f 38\n",
      "Validação Acc: 0.869806 -- Melhor Avaliação Acc: 0.869806 -- epoch 38.\n",
      "Epoch 40 starting...\n",
      "Train loss: 0.262104, train acc: 0.882287\n",
      "Nova melhor conta de validação. Salvo... %f 39\n",
      "Validação Acc: 0.879899 -- Melhor Avaliação Acc: 0.879899 -- epoch 39.\n",
      "Epoch 41 starting...\n",
      "Train loss: 0.224746, train acc: 0.911350\n",
      "Validação Acc: 0.829427 -- Melhor Avaliação Acc: 0.879899 -- epoch 39.\n",
      "Epoch 42 starting...\n",
      "Train loss: 0.306993, train acc: 0.827531\n",
      "Nova melhor conta de validação. Salvo... %f 41\n",
      "Validação Acc: 0.930301 -- Melhor Avaliação Acc: 0.930301 -- epoch 41.\n",
      "Epoch 43 starting...\n",
      "Train loss: 0.236868, train acc: 0.895426\n",
      "Nova melhor conta de validação. Salvo... %f 42\n",
      "Validação Acc: 0.952356 -- Melhor Avaliação Acc: 0.952356 -- epoch 42.\n",
      "Epoch 44 starting...\n",
      "Train loss: 0.145403, train acc: 0.940560\n",
      "Nova melhor conta de validação. Salvo... %f 43\n",
      "Validação Acc: 0.953747 -- Melhor Avaliação Acc: 0.953747 -- epoch 43.\n",
      "Epoch 45 starting...\n",
      "Train loss: 0.131112, train acc: 0.951184\n",
      "Nova melhor conta de validação. Salvo... %f 44\n",
      "Validação Acc: 0.964258 -- Melhor Avaliação Acc: 0.964258 -- epoch 44.\n",
      "Epoch 46 starting...\n",
      "Train loss: 0.097259, train acc: 0.954021\n",
      "Nova melhor conta de validação. Salvo... %f 45\n",
      "Validação Acc: 0.968347 -- Melhor Avaliação Acc: 0.968347 -- epoch 45.\n",
      "Epoch 47 starting...\n",
      "Train loss: 0.078674, train acc: 0.973247\n",
      "Nova melhor conta de validação. Salvo... %f 46\n",
      "Validação Acc: 0.968899 -- Melhor Avaliação Acc: 0.968899 -- epoch 46.\n",
      "Epoch 48 starting...\n",
      "Train loss: 0.076163, train acc: 0.975663\n",
      "Nova melhor conta de validação. Salvo... %f 47\n",
      "Validação Acc: 0.970322 -- Melhor Avaliação Acc: 0.970322 -- epoch 47.\n",
      "Epoch 49 starting...\n",
      "Train loss: 0.073029, train acc: 0.976517\n",
      "Validação Acc: 0.970030 -- Melhor Avaliação Acc: 0.970322 -- epoch 47.\n",
      "Epoch 50 starting...\n",
      "Train loss: 0.085989, train acc: 0.967826\n",
      "Validação Acc: 0.968510 -- Melhor Avaliação Acc: 0.970322 -- epoch 47.\n",
      "Epoch 51 starting...\n",
      "Train loss: 0.078723, train acc: 0.970662\n",
      "Validação Acc: 0.964434 -- Melhor Avaliação Acc: 0.970322 -- epoch 47.\n",
      "Epoch 52 starting...\n",
      "Train loss: 0.058067, train acc: 0.981971\n",
      "Nova melhor conta de validação. Salvo... %f 51\n",
      "Validação Acc: 0.974885 -- Melhor Avaliação Acc: 0.974885 -- epoch 51.\n",
      "Epoch 53 starting...\n",
      "Train loss: 0.049665, train acc: 0.982303\n",
      "Nova melhor conta de validação. Salvo... %f 52\n",
      "Validação Acc: 0.976317 -- Melhor Avaliação Acc: 0.976317 -- epoch 52.\n",
      "Epoch 54 starting...\n",
      "Train loss: 0.043343, train acc: 0.985738\n",
      "Validação Acc: 0.972933 -- Melhor Avaliação Acc: 0.976317 -- epoch 52.\n",
      "Epoch 55 starting...\n",
      "Train loss: 0.041924, train acc: 0.987418\n",
      "Validação Acc: 0.967425 -- Melhor Avaliação Acc: 0.976317 -- epoch 52.\n",
      "Epoch 56 starting...\n",
      "Train loss: 0.057263, train acc: 0.975764\n",
      "Validação Acc: 0.952097 -- Melhor Avaliação Acc: 0.976317 -- epoch 52.\n",
      "Epoch 57 starting...\n",
      "Train loss: 0.051959, train acc: 0.981787\n",
      "Validação Acc: 0.971380 -- Melhor Avaliação Acc: 0.976317 -- epoch 52.\n",
      "Epoch 58 starting...\n",
      "Train loss: 0.038185, train acc: 0.988467\n",
      "Nova melhor conta de validação. Salvo... %f 57\n",
      "Validação Acc: 0.978459 -- Melhor Avaliação Acc: 0.978459 -- epoch 57.\n",
      "Epoch 59 starting...\n",
      "Train loss: 0.037961, train acc: 0.988664\n",
      "Nova melhor conta de validação. Salvo... %f 58\n",
      "Validação Acc: 0.978714 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 60 starting...\n",
      "Train loss: 0.032053, train acc: 0.988036\n",
      "Validação Acc: 0.976280 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 61 starting...\n",
      "Train loss: 0.032641, train acc: 0.990722\n",
      "Validação Acc: 0.975752 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 62 starting...\n",
      "Train loss: 0.031912, train acc: 0.989869\n",
      "Validação Acc: 0.977922 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 63 starting...\n",
      "Train loss: 0.029830, train acc: 0.990609\n",
      "Validação Acc: 0.975089 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 64 starting...\n",
      "Train loss: 0.030838, train acc: 0.990362\n",
      "Validação Acc: 0.977662 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 65 starting...\n",
      "Train loss: 0.029965, train acc: 0.990227\n",
      "Validação Acc: 0.977208 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 66 starting...\n",
      "Train loss: 0.027677, train acc: 0.991648\n",
      "Validação Acc: 0.977486 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 67 starting...\n",
      "Train loss: 0.029067, train acc: 0.990794\n",
      "Validação Acc: 0.977504 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 68 starting...\n",
      "Train loss: 0.029540, train acc: 0.991303\n",
      "Validação Acc: 0.977384 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 69 starting...\n",
      "Train loss: 0.027037, train acc: 0.991034\n",
      "Validação Acc: 0.977537 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 70 starting...\n",
      "Train loss: 0.027519, train acc: 0.991133\n",
      "Validação Acc: 0.977305 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 71 starting...\n",
      "Train loss: 0.028229, train acc: 0.990918\n",
      "Validação Acc: 0.977718 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 72 starting...\n",
      "Train loss: 0.027338, train acc: 0.991118\n",
      "Validação Acc: 0.977662 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 73 starting...\n",
      "Train loss: 0.026574, train acc: 0.991996\n",
      "Validação Acc: 0.977337 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 74 starting...\n",
      "Train loss: 0.024205, train acc: 0.992534\n",
      "Validação Acc: 0.977486 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 75 starting...\n",
      "Train loss: 0.026764, train acc: 0.991159\n",
      "Validação Acc: 0.977676 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 76 starting...\n",
      "Train loss: 0.024381, train acc: 0.992798\n",
      "Validação Acc: 0.976586 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 77 starting...\n",
      "Train loss: 0.026279, train acc: 0.991853\n",
      "Validação Acc: 0.977384 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 78 starting...\n",
      "Train loss: 0.024758, train acc: 0.991904\n",
      "Validação Acc: 0.977611 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 79 starting...\n",
      "Train loss: 0.024500, train acc: 0.992228\n",
      "Validação Acc: 0.977384 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 80 starting...\n",
      "Train loss: 0.023406, train acc: 0.992779\n",
      "Validação Acc: 0.977588 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 81 starting...\n",
      "Train loss: 0.024451, train acc: 0.992426\n",
      "Validação Acc: 0.977467 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 82 starting...\n",
      "Train loss: 0.024308, train acc: 0.992217\n",
      "Validação Acc: 0.977421 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 83 starting...\n",
      "Train loss: 0.022952, train acc: 0.992894\n",
      "Validação Acc: 0.977472 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 84 starting...\n",
      "Train loss: 0.020927, train acc: 0.993347\n",
      "Validação Acc: 0.977722 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 85 starting...\n",
      "Train loss: 0.022180, train acc: 0.993078\n",
      "Validação Acc: 0.976864 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 86 starting...\n",
      "Train loss: 0.022352, train acc: 0.993119\n",
      "Validação Acc: 0.977277 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 87 starting...\n",
      "Train loss: 0.021786, train acc: 0.993136\n",
      "Validação Acc: 0.977657 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 88 starting...\n",
      "Train loss: 0.023308, train acc: 0.992845\n",
      "Validação Acc: 0.977198 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 89 starting...\n",
      "Train loss: 0.022273, train acc: 0.992543\n",
      "Validação Acc: 0.977871 -- Melhor Avaliação Acc: 0.978714 -- epoch 58.\n",
      "Epoch 90 starting...\n",
      "Train loss: 0.021592, train acc: 0.993099\n",
      "Terminando o treinamento, melhor conta de validação 0.978714\n"
     ]
    }
   ],
   "source": [
    "# Inicializar listas para armazenar a perda e a precisão\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Inicia o treinamento\n",
    "train_dataset = SegmentationDataset(img_folder_train, img_folder_train, True, class_to_id, resolution_input, True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, drop_last=True)\n",
    "\n",
    "val_dataset = SegmentationDataset(img_folder_val, img_folder_val, False, class_to_id, resolution_input)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=0, drop_last=False)\n",
    "\n",
    "\n",
    "if plot_train:\n",
    "\n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "    \n",
    "            image_np = np.squeeze(sample_batched['image_original'].cpu().numpy())\n",
    "            gt = np.squeeze(sample_batched['gt'].cpu().numpy())\n",
    "                \n",
    "            color_label = np.zeros((resolution_input[1], resolution_input[0], 3))\n",
    "            \n",
    "            for key, val in id_to_class.items():\n",
    "                color_label[gt == key] = class_to_color[val]\n",
    "                \n",
    "            plt.figure()\n",
    "            plt.imshow((image_np/255) * 0.5 + (color_label/255) * 0.5)\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.imshow(color_label.astype(np.uint8))\n",
    "            plt.show()\n",
    "\n",
    "  \n",
    "model = UNetVgg(nClasses).to(device)\n",
    "\n",
    "core_lr = 0.02\n",
    "base_vgg_weight, base_vgg_bias, core_weight, core_bias = UNetVgg.get_params_by_kind(model, 7)\n",
    "\n",
    "optimizer = torch.optim.SGD([{'params': base_vgg_bias, 'lr': 0.000001}, \n",
    "                             {'params': base_vgg_weight, 'lr': 0.000001},\n",
    "                             {'params': core_bias, 'lr': core_lr},\n",
    "                             {'params': core_weight, 'lr': core_lr, 'weight_decay': 0.0005}], momentum=0.9)\n",
    "    \n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 30, gamma=0.2)\n",
    "\n",
    "\n",
    "best_val_acc = -1\n",
    "best_epoch = 0\n",
    "\n",
    "# Start training...\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    print('Epoch %d starting...' % (epoch+1))\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    mean_loss = 0.0\n",
    "    \n",
    "    n_correct = 0\n",
    "    n_false = 0\n",
    "    \n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "    \n",
    "    \n",
    "        image = sample_batched['image'].to(device)\n",
    "        gt = sample_batched['gt'].to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        output, total_loss = model.eval_net_with_loss(model, image, gt, class_weights, device)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        mean_loss += total_loss.cpu().detach().numpy()\n",
    "        \n",
    "        # Measure accuracy\n",
    "        \n",
    "        gt = np.squeeze(sample_batched['gt'].cpu().numpy())\n",
    "        \n",
    "        label_out = torch.nn.functional.softmax(output, dim = 1)\n",
    "        label_out = label_out.cpu().detach().numpy()\n",
    "        label_out = np.squeeze(label_out)\n",
    "        \n",
    "        labels = np.argmax(label_out, axis=0)\n",
    "        valid_mask = gt != -1\n",
    "        curr_correct = np.sum(gt[valid_mask] == labels[valid_mask])\n",
    "        curr_false = np.sum(valid_mask) - curr_correct\n",
    "        n_correct += curr_correct\n",
    "        n_false += curr_false\n",
    "        \n",
    "    mean_loss /= len(train_loader)\n",
    "    train_acc = n_correct / (n_correct + n_false)\n",
    "        \n",
    "    print('Train loss: %f, train acc: %f' % (mean_loss, train_acc))\n",
    "    # Armazenar a perda e a precisão de treinamento\n",
    "    train_losses.append(mean_loss)\n",
    "    train_accuracies.append(train_acc)    \n",
    "    \n",
    "    n_correct = 0\n",
    "    n_false = 0\n",
    "    \n",
    "    \n",
    "    for i_batch, sample_batched in enumerate(val_loader):\n",
    "    \n",
    "    \n",
    "        image = sample_batched['image'].to(device)\n",
    "        image_np = np.squeeze(sample_batched['image_original'].cpu().numpy())\n",
    "        gt = np.squeeze(sample_batched['gt'].cpu().numpy())\n",
    "        \n",
    "    \n",
    "        label_out = model(image)\n",
    "        label_out = torch.nn.functional.softmax(label_out, dim = 1)\n",
    "        label_out = label_out.cpu().detach().numpy()\n",
    "        label_out = np.squeeze(label_out)\n",
    "        \n",
    "        labels = np.argmax(label_out, axis=0)\n",
    "        \n",
    "        if plot_val:\n",
    "            \n",
    "            color_label = np.zeros((resolution_input[1], resolution_input[0], 3))\n",
    "            \n",
    "            for key, val in id_to_class.items():\n",
    "                color_label[labels == key] = class_to_color[val]\n",
    "                \n",
    "            plt.figure()\n",
    "            plt.imshow((image_np/255) * 0.5 + (color_label/255) * 0.5)\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.imshow(color_label.astype(np.uint8))\n",
    "            plt.show()\n",
    "        \n",
    "        valid_mask = gt != -1\n",
    "        curr_correct = np.sum(gt[valid_mask] == labels[valid_mask])\n",
    "        curr_false = np.sum(valid_mask) - curr_correct\n",
    "        n_correct += curr_correct\n",
    "        n_false += curr_false    \n",
    "        \n",
    "    total_acc = n_correct / (n_correct + n_false)\n",
    "    val_accuracies.append(total_acc)\n",
    "    \n",
    "    if best_val_acc < total_acc:\n",
    "        best_val_acc = total_acc\n",
    "        if epoch > 7:\n",
    "            torch.save(model.state_dict(), model_file_name)\n",
    "            print('Nova melhor conta de validação. Salvo... %f', epoch)\n",
    "        best_epoch = epoch\n",
    "\n",
    "    if (epoch - best_epoch) > patience:\n",
    "        print(f\"Terminando o treinamento, melhor conta de validação {best_val_acc:.6f}\")\n",
    "        break\n",
    "    \n",
    "    print('Validação Acc: %f -- Melhor Avaliação Acc: %f -- epoch %d.' % (total_acc, best_val_acc, best_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotar os gráficos de perda e precisão\n",
    "    Inicialização das listas: train_losses, train_accuracies e val_accuracies são listas para armazenar a perda e a precisão de treinamento e validação em cada época.\n",
    "    \n",
    "    Armazenamento dos valores: Durante o loop de treinamento, a perda e a precisão são calculadas e armazenadas nas listas correspondentes.\n",
    "    Plotagem dos gráficos: Após o loop de treinamento, os gráficos de perda e precisão são plotados usando matplotlib.\n",
    "\n",
    "Este código deve ser adicionado ao final do seu loop de treinamento no notebook para visualizar os resultados do treinamento ao longo das épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henrique Moreira\\AppData\\Local\\Temp\\ipykernel_2264\\4265528127.py:19: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Perda de Treinamento')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Perda')\n",
    "plt.title('Perda de Treinamento ao longo das Épocas')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Precisão de Treinamento')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Precisão de Validação')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Precisão')\n",
    "plt.title('Precisão de Treinamento e Validação ao longo das Épocas')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferência de dados\n",
    "Processo de usar um modelo treinado para fazer previsões sobre novos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henrique Moreira\\AppData\\Local\\Temp\\ipykernel_2264\\3878609034.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo carregado e pronto para uso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henrique Moreira\\AppData\\Local\\Temp\\ipykernel_2264\\3878609034.py:48: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "C:\\Users\\Henrique Moreira\\AppData\\Local\\Temp\\ipykernel_2264\\3878609034.py:53: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Color in RGB\n",
    "class_to_color = {'Ground': (127, 0, 0) , 'Healthy': (0, 127, 127), 'Pest': (0, 255, 0)}\n",
    "class_to_id = {'Ground': 0, 'Healthy': 1, 'Pest': 2}\n",
    "id_to_class = {v: k for k, v in class_to_id.items()}\n",
    "nClasses = 3\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "model = UNetVgg(nClasses)\n",
    "model.load_state_dict(torch.load(model_file_name))\n",
    "model.eval()\n",
    "print(\"Modelo carregado e pronto para uso.\")\n",
    "model.to(device)\n",
    "\n",
    "img_list = glob.glob(osp.join(img_folder_val, '*.png'))\n",
    "\n",
    "for img_path in img_list:\n",
    "\n",
    "        img_np = cv2.imread(img_path, cv2.IMREAD_IGNORE_ORIENTATION + cv2.IMREAD_COLOR)\n",
    "        img_np = cv2.resize(img_np, (resolution_input[0], resolution_input[1]))[..., ::-1]\n",
    "        img_np = np.ascontiguousarray(img_np)\n",
    "        \n",
    "        img_pt = np.copy(img_np).astype(np.float32) / 255.0\n",
    "        for i in range(3):\n",
    "            img_pt[..., i] -= mean[i]\n",
    "            img_pt[..., i] /= std[i]\n",
    "            \n",
    "        img_pt = img_pt.transpose(2,0,1)\n",
    "            \n",
    "        img_pt = torch.from_numpy(img_pt[None, ...]).to(device)\n",
    "        \n",
    "        label_out = model(img_pt)\n",
    "        label_out = torch.nn.functional.softmax(label_out, dim = 1)\n",
    "        label_out = label_out.cpu().detach().numpy()\n",
    "        label_out = np.squeeze(label_out)\n",
    "        \n",
    "        labels = np.argmax(label_out, axis=0)\n",
    "        \n",
    "        color_label = np.zeros((resolution_input[1], resolution_input[0], 3))\n",
    "            \n",
    "        for key, val in id_to_class.items():\n",
    "            color_label[labels == key] = class_to_color[val]\n",
    "            \n",
    "        plt.figure()\n",
    "        plt.imshow((img_np/255) * 0.5 + (color_label/255) * 0.5)\n",
    "        plt.savefig(save_dir + \"IMG\" + \".png\")\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.imshow(color_label.astype(np.uint8))\n",
    "        plt.savefig(save_dir + \"GT\" + \".png\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
