{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import os.path as osp\n",
    "import glob\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição da classe UNetVgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição da classe UNetVgg\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "class UNetVgg(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    BorderNetwork is a NN that aims to detected border and classify occlusion.\n",
    "    The architecture is a VGG without the last pool layer. After that we \n",
    "    have two paths, one for regression and one for classification (occlusion).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(UNetVgg, self).__init__()\n",
    "        \n",
    "        #vgg16pre = torchvision.models.vgg16(pretrained=True)\n",
    "        vgg16pre = torchvision.models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "        self.vgg0 = torch.nn.Sequential(*list(vgg16pre.features.children())[:4])\n",
    "        self.vgg1 = torch.nn.Sequential(*list(vgg16pre.features.children())[4:9])\n",
    "        self.vgg2 = torch.nn.Sequential(*list(vgg16pre.features.children())[9:16])\n",
    "        self.vgg3 = torch.nn.Sequential(*list(vgg16pre.features.children())[16:23])\n",
    "        self.vgg4 = torch.nn.Sequential(*list(vgg16pre.features.children())[23:30])\n",
    "        \n",
    "        \n",
    "        self.smooth0 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(128, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True),\n",
    "                torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True)\n",
    "                )\n",
    "        self.smooth1 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(256, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True),\n",
    "                torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True)\n",
    "                )\n",
    "        self.smooth2 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(512, 128, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True),\n",
    "                torch.nn.Conv2d(128, 128, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True)\n",
    "                )\n",
    "        self.smooth3 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(1024, 256, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True),\n",
    "                torch.nn.Conv2d(256, 256, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True)\n",
    "                )\n",
    "        \n",
    "        \n",
    "        self.final = torch.nn.Conv2d(64, nClasses, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.tensor): A tensor of size (batch, 3, H, W)\n",
    "        Returns:\n",
    "            reg_out (torch.tensor): A tensor with results of the regression (batch, 4).\n",
    "            cls_out (torch.tensor): A tensor with results of the classification (batch, 2).\n",
    "        \"\"\"\n",
    "        \n",
    "        feat0 = self.vgg0(x)\n",
    "        feat1 = self.vgg1(feat0)\n",
    "        feat2 = self.vgg2(feat1)\n",
    "        feat3 = self.vgg3(feat2)\n",
    "        feat4 = self.vgg4(feat3)\n",
    "        \n",
    "        _,_,H,W = feat3.size()\n",
    "        up3 = torch.nn.functional.interpolate(feat4, size=(H,W), mode='bilinear', align_corners=False)\n",
    "        concat3 = torch.cat([feat3, up3], 1)\n",
    "        end3 = self.smooth3(concat3)\n",
    "        \n",
    "        _,_,H,W = feat2.size()\n",
    "        up2 = torch.nn.functional.interpolate(end3, size=(H,W), mode='bilinear', align_corners=False)\n",
    "        concat2 = torch.cat([feat2, up2], 1)\n",
    "        end2 = self.smooth2(concat2)\n",
    "        \n",
    "        _,_,H,W = feat1.size()\n",
    "        up1 = torch.nn.functional.interpolate(end2, size=(H,W), mode='bilinear', align_corners=False)\n",
    "        concat1 = torch.cat([feat1, up1], 1)\n",
    "        end1 = self.smooth1(concat1)\n",
    "        \n",
    "        _,_,H,W = feat0.size()\n",
    "        up0 = torch.nn.functional.interpolate(end1, size=(H,W), mode='bilinear', align_corners=False)\n",
    "        concat0 = torch.cat([feat0, up0], 1)\n",
    "        end0 = self.smooth0(concat0)\n",
    "        \n",
    "        return self.final(end0)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def eval_net_with_loss(model, inp, gt, class_weights, device):\n",
    "        \"\"\"\n",
    "        Evaluate network including loss.\n",
    "        \n",
    "        Args:\n",
    "            model (torch.nn.Module): The model.\n",
    "            inp (torch.tensor): A tensor (float32) of size (batch, 3, H, W)\n",
    "            gt (torch.tensor): A tensor (long) of size (batch, 1, H, W) with the groud truth (0 to num_classes-1).\n",
    "            class_weights (list of float): A list with len == num_classes.\n",
    "            device (torch.device): device to perform computation\n",
    "            \n",
    "        Returns:\n",
    "            out (torch.tensor): Network output.\n",
    "            loss (torch.tensor): Tensor with the total loss.\n",
    "                \n",
    "        \"\"\"\n",
    "        weights = torch.from_numpy(np.array(class_weights, dtype=np.float32)).to(device)\n",
    "        out = model(inp)\n",
    "        \n",
    "        softmax = torch.nn.functional.log_softmax(out, dim = 1)\n",
    "        loss = torch.nn.functional.nll_loss(softmax, gt, ignore_index=-1, weight=weights)\n",
    "            \n",
    "        return (out, loss)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_params_by_kind(model, n_base = 7):\n",
    "    \n",
    "        base_vgg_bias = []\n",
    "        base_vgg_weight = []\n",
    "        core_weight = []\n",
    "        core_bias = []\n",
    "    \n",
    "        for name, param in model.named_parameters():\n",
    "            if 'vgg' in name and ('weight' in name or 'bias' in name):\n",
    "                vgglayer = int(name.split('.')[-2])\n",
    "                \n",
    "                if vgglayer <= n_base:\n",
    "                    if 'bias' in name:\n",
    "                        print('Adding %s to base vgg bias.' % (name))\n",
    "                        base_vgg_bias.append(param)\n",
    "                    else:\n",
    "                        base_vgg_weight.append(param)\n",
    "                        print('Adding %s to base vgg weight.' % (name))\n",
    "                else:\n",
    "                    if 'bias' in name:\n",
    "                        print('Adding %s to core bias.' % (name))\n",
    "                        core_bias.append(param)\n",
    "                    else:\n",
    "                        print('Adding %s to core weight.' % (name))\n",
    "                        core_weight.append(param)\n",
    "                        \n",
    "            elif ('weight' in name or 'bias' in name):\n",
    "                if 'bias' in name:\n",
    "                    print('Adding %s to core bias.' % (name))\n",
    "                    core_bias.append(param)\n",
    "                else:\n",
    "                    print('Adding %s to core weight.' % (name))\n",
    "                    core_weight.append(param)\n",
    "                    \n",
    "        return (base_vgg_weight, base_vgg_bias, core_weight, core_bias)\n",
    "    \n",
    "# End class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declarações Variavies e outras configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variável que define se as figuras são exibidas no console ou salvas em um arquivo\n",
    "plt_show = False\n",
    "\n",
    "# Configuração do dispositivo CUDA\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f'CUDA disponível: {cuda_available}')\n",
    "\n",
    "if cuda_available:\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_total = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # Convertendo para GB\n",
    "    vram_available = torch.cuda.memory_reserved(0) / (1024 ** 3)  # Convertendo para GB\n",
    "    print(f'Nome da GPU: {gpu_name}')\n",
    "    print(f'VRAM Total: {vram_total:.2f} GB')\n",
    "\n",
    "# Caminho do diretório Dataset\n",
    "directory = \"C:/git/image-segmentation/Dataset/mamoeiro\"\n",
    "print(f'Diretório do Projeto {directory}.')\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "img_folder_val = directory + r'\\Val'\n",
    "img_folder_train = directory + r'\\Train'\n",
    "img_folder_test = directory + r'\\Test'\n",
    "save_dir = directory + r'\\\\result_UnetVgg\\\\'\n",
    "if not os.path.exists(img_folder_val):\n",
    "    os.makedirs(img_folder_val)\n",
    "if not os.path.exists(img_folder_train):\n",
    "    os.makedirs(img_folder_train)\n",
    "if not os.path.exists(img_folder_test):\n",
    "    os.makedirs(img_folder_test)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "## Imagens Segmentadas\n",
    "img_folder_train_segmentadas = directory + r'\\\\segmentadas\\\\train\\\\'\n",
    "img_folder_val_segmentadas = directory + r'\\\\segmentadas\\\\val\\\\'\n",
    "img_folder_test_segmentadas = directory + r'\\\\segmentadas\\\\test\\\\'\n",
    "if not os.path.exists(img_folder_train_segmentadas):\n",
    "    os.makedirs(img_folder_train_segmentadas)\n",
    "if not os.path.exists(img_folder_val_segmentadas):\n",
    "    os.makedirs(img_folder_val_segmentadas)\n",
    "if not os.path.exists(img_folder_test_segmentadas):\n",
    "    os.makedirs(img_folder_test_segmentadas)\n",
    "    \n",
    "# Local onde o Modelo será salvo\n",
    "model_file_name = save_dir + 'model_segmentadas_unet.pth'\n",
    "\n",
    "# Configurações do treinamento\n",
    "resolution_input = (640, 480)  # Tamanho de entrada\n",
    "patience = 30\n",
    "plot_val = True\n",
    "plot_train = True\n",
    "max_epochs = 100\n",
    "class_weights = [1, 1, 1, 1]\n",
    "nClasses = 4\n",
    "\n",
    "# Mapeamento de classes e cores\n",
    "class_to_color = {'Doenca': (255, 0, 0), 'Folha': (0, 255, 0), 'Solo': (0, 0, 255), 'Saudavel': (0, 255, 255)}\n",
    "class_to_id = {'Doenca': 0, 'Folha': 1, 'Solo': 2, 'Saudavel': 3}\n",
    "id_to_class = {v: k for k, v in class_to_id.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição do Dataset\n",
    "class SegmentationDataset(Dataset):\n",
    "    \"\"\"Segmentation dataset loader.\"\"\"\n",
    "\n",
    "    def __init__(self, json_folder, img_folder, is_train, class_to_id, resolution_input=(640, 480), augmentation=False, transform=None):\n",
    "        self.gt_file_list = glob.glob(osp.join(json_folder, '*.json'))\n",
    "        self.total_samples = len(self.gt_file_list)\n",
    "        self.img_folder = img_folder\n",
    "        self.is_train = is_train\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "        self.resolution = resolution_input\n",
    "        self.class_to_id = class_to_id\n",
    "        self.mean = [0.485, 0.456, 0.406]\n",
    "        self.std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gt_file = self.gt_file_list[idx]\n",
    "        img_number_str = osp.splitext(osp.basename(gt_file))[0]\n",
    "        \n",
    "        # Verificação de existência de arquivos\n",
    "        if not osp.exists(gt_file):\n",
    "            raise FileNotFoundError(f\"Arquivo JSON não encontrado: {gt_file}\")\n",
    "        \n",
    "        # Extrair o nome da imagem considerando que o nome da classe está no meio do nome do arquivo\n",
    "        img_path = osp.join(self.img_folder, img_number_str + '.JPG')\n",
    "        \n",
    "        if not osp.exists(img_path):\n",
    "            raise FileNotFoundError(f\"Imagem não encontrada: {img_path}\")\n",
    "        \n",
    "        gt_json = json.load(open(gt_file, 'r'))\n",
    "        img_np = cv2.imread(img_path, cv2.IMREAD_IGNORE_ORIENTATION + cv2.IMREAD_COLOR)\n",
    "        \n",
    "        if img_np is None:\n",
    "            raise FileNotFoundError(f\"Imagem não encontrada: {img_path}\")\n",
    "        \n",
    "        original_shape = img_np.shape\n",
    "        img_np = cv2.resize(img_np, (self.resolution[0], self.resolution[1]))[..., ::-1]\n",
    "        img_np = np.ascontiguousarray(img_np)\n",
    "        label_np = np.zeros((img_np.shape[0], img_np.shape[1]))\n",
    "        label_np[...] = -1\n",
    "\n",
    "        for shape in gt_json['shapes']:\n",
    "            points_np = np.array(shape['points'], dtype=np.float64)\n",
    "            points_np[:, 0] *= self.resolution[0] / original_shape[1]\n",
    "            points_np[:, 1] *= self.resolution[1] / original_shape[0]\n",
    "            points_np = np.round(points_np).astype(np.int64)\n",
    "            points_np = points_np.reshape((-1, 1, 2))\n",
    "            label = shape['label']\n",
    "            if label not in self.class_to_id:\n",
    "                raise KeyError(f\"Label '{label}' não encontrado em class_to_id\")\n",
    "            label_np = cv2.fillPoly(label_np, [points_np], self.class_to_id[label])\n",
    "\n",
    "        label_np = label_np.astype(np.int32)\n",
    "\n",
    "        if self.is_train and self.augmentation:\n",
    "            if np.random.rand() > 0.5:\n",
    "                img_np = np.fliplr(img_np)\n",
    "                label_np = np.fliplr(label_np)\n",
    "                img_np = np.ascontiguousarray(img_np)\n",
    "                label_np = np.ascontiguousarray(label_np)\n",
    "\n",
    "        img_pt = img_np.astype(np.float32) / 255.0\n",
    "        for i in range(3):\n",
    "            img_pt[..., i] -= self.mean[i]\n",
    "            img_pt[..., i] /= self.std[i]\n",
    "\n",
    "        img_pt = img_pt.transpose(2, 0, 1)\n",
    "        img_pt = torch.from_numpy(img_pt)\n",
    "        label_pt = torch.from_numpy(label_np).long()\n",
    "\n",
    "        sample = {'image': img_pt, 'gt': label_pt, 'image_original': img_np}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar listas para armazenar a perda e a precisão\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Inicia o treinamento\n",
    "train_dataset = SegmentationDataset(img_folder_train, img_folder_train, True, class_to_id, resolution_input, True, None)\n",
    "print(f\"Número de amostras no dataset de treinamento: {len(train_dataset)}\")\n",
    "print(f\"Arquivos no dataset de treinamento: {os.listdir(img_folder_train)}\")\n",
    "\n",
    "val_dataset = SegmentationDataset(img_folder_val, img_folder_val, False, class_to_id, resolution_input, False, None)\n",
    "print(f\"Número de amostras no dataset de validação: {len(val_dataset)}\")\n",
    "print(f\"Arquivos no dataset de validação: {os.listdir(img_folder_val)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=0, drop_last=False)\n",
    "\n",
    "if plot_train:\n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "        image_np = np.squeeze(sample_batched['image_original'].cpu().numpy())\n",
    "        gt = np.squeeze(sample_batched['gt'].cpu().numpy())\n",
    "        \n",
    "        color_label = np.zeros((resolution_input[1], resolution_input[0], 3))\n",
    "        \n",
    "        for key, val in id_to_class.items():\n",
    "            color_label[gt == key] = class_to_color.get(val, [0, 0, 0])  # Provide a default color if key is missing\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.imshow((image_np / 255) * 0.5 + (color_label / 255) * 0.5)\n",
    "        # print(f\"Imagem de Treinamento {i_batch}\")\n",
    "        plt.savefig(img_folder_train_segmentadas + \"IMG_\" + str(i_batch) + \".png\")\n",
    "        if plt_show: plt.show()\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.imshow(color_label.astype(np.uint8))\n",
    "        # print(f\"Imagem de Treinamento {i_batch} - Segmentada\")\n",
    "        plt.savefig(img_folder_train_segmentadas + \"GT_\" + str(i_batch) + \".png\")\n",
    "        if plt_show: plt.show()\n",
    "\n",
    "model = UNetVgg(nClasses).to(device)\n",
    "\n",
    "core_lr = 0.02\n",
    "base_vgg_weight, base_vgg_bias, core_weight, core_bias = UNetVgg.get_params_by_kind(model, 7)\n",
    "\n",
    "optimizer = torch.optim.SGD([{'params': base_vgg_bias, 'lr': 0.000001}, \n",
    "                             {'params': base_vgg_weight, 'lr': 0.000001},\n",
    "                             {'params': core_bias, 'lr': core_lr},\n",
    "                             {'params': core_weight, 'lr': core_lr, 'weight_decay': 0.0005}], momentum=0.9)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 30, gamma=0.2)\n",
    "\n",
    "best_val_acc = -1\n",
    "best_epoch = 0\n",
    "\n",
    "# Start training...\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    print('Epoch %d starting...' % (epoch+1))\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    mean_loss = 0.0\n",
    "    \n",
    "    n_correct = 0\n",
    "    n_false = 0\n",
    "    \n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "    \n",
    "    \n",
    "        image = sample_batched['image'].to(device)\n",
    "        gt = sample_batched['gt'].to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        output, total_loss = model.eval_net_with_loss(model, image, gt, class_weights, device)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        mean_loss += total_loss.cpu().detach().numpy()\n",
    "        \n",
    "        # Measure accuracy\n",
    "        \n",
    "        gt = np.squeeze(sample_batched['gt'].cpu().numpy())\n",
    "        \n",
    "        label_out = torch.nn.functional.softmax(output, dim = 1)\n",
    "        label_out = label_out.cpu().detach().numpy()\n",
    "        label_out = np.squeeze(label_out)\n",
    "        \n",
    "        labels = np.argmax(label_out, axis=0)\n",
    "        valid_mask = gt != -1\n",
    "        curr_correct = np.sum(gt[valid_mask] == labels[valid_mask])\n",
    "        curr_false = np.sum(valid_mask) - curr_correct\n",
    "        n_correct += curr_correct\n",
    "        n_false += curr_false\n",
    "        \n",
    "    mean_loss /= len(train_loader)\n",
    "    train_acc = n_correct / (n_correct + n_false)\n",
    "        \n",
    "    print('Train loss: %f, train acc: %f' % (mean_loss, train_acc))\n",
    "    # Armazenar a perda e a precisão de treinamento\n",
    "    train_losses.append(mean_loss)\n",
    "    train_accuracies.append(train_acc)    \n",
    "    \n",
    "    n_correct = 0\n",
    "    n_false = 0\n",
    "    \n",
    "    \n",
    "    for i_batch, sample_batched in enumerate(val_loader):\n",
    "    \n",
    "    \n",
    "        image = sample_batched['image'].to(device)\n",
    "        image_np = np.squeeze(sample_batched['image_original'].cpu().numpy())\n",
    "        gt = np.squeeze(sample_batched['gt'].cpu().numpy())\n",
    "        \n",
    "    \n",
    "        label_out = model(image)\n",
    "        label_out = torch.nn.functional.softmax(label_out, dim = 1)\n",
    "        label_out = label_out.cpu().detach().numpy()\n",
    "        label_out = np.squeeze(label_out)\n",
    "        \n",
    "        labels = np.argmax(label_out, axis=0)\n",
    "        \n",
    "        if plot_val:\n",
    "            \n",
    "            color_label = np.zeros((resolution_input[1], resolution_input[0], 3))\n",
    "            \n",
    "            for key, val in id_to_class.items():\n",
    "                color_label[labels == key] = class_to_color[val]\n",
    "                \n",
    "            plt.figure()\n",
    "            plt.imshow((image_np/255) * 0.5 + (color_label/255) * 0.5)\n",
    "            # print(f\"Imagem de Validação {i_batch}\")\n",
    "            plt.savefig(img_folder_val_segmentadas + \"IMG_\" + str(i_batch) + \"_epoch_\" + str(epoch) + \".png\")\n",
    "            if plt_show: plt.show()\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.imshow(color_label.astype(np.uint8))\n",
    "            # print(f\"Imagem de Validação {i_batch} - Segmentada\")\n",
    "            plt.savefig(img_folder_val_segmentadas + \"GT_\" + str(i_batch) + \"_epoch_\" + str(epoch) +  \".png\")\n",
    "            if plt_show: plt.show()\n",
    "        \n",
    "        valid_mask = gt != -1\n",
    "        curr_correct = np.sum(gt[valid_mask] == labels[valid_mask])\n",
    "        curr_false = np.sum(valid_mask) - curr_correct\n",
    "        n_correct += curr_correct\n",
    "        n_false += curr_false    \n",
    "        \n",
    "    total_acc = n_correct / (n_correct + n_false)\n",
    "    val_accuracies.append(total_acc)\n",
    "    \n",
    "    if best_val_acc < total_acc:\n",
    "        best_val_acc = total_acc\n",
    "        if epoch > 7:\n",
    "            torch.save(model.state_dict(), model_file_name)\n",
    "            print('Nova melhor conta de validação. Salvo... %f', epoch)\n",
    "        best_epoch = epoch\n",
    "\n",
    "    if (epoch - best_epoch) > patience:\n",
    "        print(f\"Terminando o treinamento, melhor conta de validação {best_val_acc:.6f}\")\n",
    "        break\n",
    "    \n",
    "    print('Validação Acc: %f -- Melhor Avaliação Acc: %f -- epoch %d.' % (total_acc, best_val_acc, best_epoch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotar os gráficos de perda e precisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar os gráficos de perda e precisão\n",
    "\"\"\"\n",
    "    Inicialização das listas: train_losses, train_accuracies e val_accuracies são listas para armazenar a perda e a precisão de treinamento e validação em cada época.\n",
    "    \n",
    "    Armazenamento dos valores: Durante o loop de treinamento, a perda e a precisão são calculadas e armazenadas nas listas correspondentes.\n",
    "    Plotagem dos gráficos: Após o loop de treinamento, os gráficos de perda e precisão são plotados usando matplotlib.\n",
    "\n",
    "    Este código deve ser adicionado ao final do seu loop de treinamento no notebook para visualizar os resultados do treinamento ao longo das épocas.\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Perda de Treinamento')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Perda')\n",
    "plt.title('Perda de Treinamento ao longo das Épocas')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Precisão de Treinamento')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Precisão de Validação')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Precisão')\n",
    "plt.title('Precisão de Treinamento e Validação ao longo das Épocas')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "## Salve o gráfico em um diretorio de resultados \"save_dir\"\n",
    "plt.savefig(save_dir + 'result_model_segmentadas_unet_loss_accuracy.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferência de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configurações do treinamento\n",
    "resolution_input = (640, 480)  # Tamanho de entrada\n",
    "patience = 30\n",
    "plot_val = True\n",
    "plot_train = True\n",
    "max_epochs = 100\n",
    "class_weights = [1, 1, 1, 1]\n",
    "nClasses = 4\n",
    "\n",
    "## Mapeamento de classes e cores\n",
    "class_to_color = {'Doenca': (255, 0, 0), 'Folha': (0, 255, 0), 'Solo': (0, 0, 255), 'Saudavel': (0, 255, 255)}\n",
    "class_to_id = {'Doenca': 0, 'Folha': 1, 'Solo': 2, 'Saudavel': 3}\n",
    "id_to_class = {v: k for k, v in class_to_id.items()}\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "model = UNetVgg(nClasses)\n",
    "#model.load_state_dict(torch.load(model_file_name))\n",
    "model.load_state_dict(torch.load(model_file_name, weights_only=True))\n",
    "model.eval()\n",
    "print(\"Modelo carregado e pronto para uso.\")\n",
    "model.to(device)\n",
    "\n",
    "img_list = glob.glob(osp.join(img_folder_val, '*.JPG'))\n",
    "print(f\"Imagens de teste: {len(img_list)}\")\n",
    "\n",
    "for img_path in img_list:\n",
    "    img_np = cv2.imread(img_path, cv2.IMREAD_IGNORE_ORIENTATION + cv2.IMREAD_COLOR)\n",
    "    img_np = cv2.resize(img_np, (resolution_input[0], resolution_input[1]))[..., ::-1]\n",
    "    img_np = np.ascontiguousarray(img_np)\n",
    "    \n",
    "    img_pt = np.copy(img_np).astype(np.float32) / 255.0\n",
    "    for i in range(3):\n",
    "        img_pt[..., i] -= mean[i]\n",
    "        img_pt[..., i] /= std[i]\n",
    "\n",
    "    img_pt = img_pt.transpose(2, 0, 1)\n",
    "    img_pt = torch.from_numpy(img_pt[None, ...]).to(device)\n",
    "\n",
    "    label_out = model(img_pt)\n",
    "    label_out = torch.nn.functional.softmax(label_out, dim=1)\n",
    "    label_out = label_out.cpu().detach().numpy()\n",
    "    label_out = np.squeeze(label_out)\n",
    "\n",
    "    labels = np.argmax(label_out, axis=0)\n",
    "\n",
    "    color_label = np.zeros((resolution_input[1], resolution_input[0], 3))\n",
    "\n",
    "    for key, val in id_to_class.items():\n",
    "        color_label[labels == key] = class_to_color[val]\n",
    "        \n",
    "    final_image = osp.basename(img_path)\n",
    "    final_image = osp.splitext(final_image)[0]\n",
    "    final_image = osp.join(img_folder_test_segmentadas, final_image)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow((img_np / 255) * 0.5 + (color_label / 255) * 0.5)\n",
    "    plt.savefig(final_image + \"IMG_\" + \".png\")\n",
    "    plt.show()\n",
    "    plt.close('all')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(color_label.astype(np.uint8))\n",
    "    plt.savefig(final_image + \"GT_\" + \".png\")\n",
    "    plt.show()\n",
    "    plt.close('all')        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
