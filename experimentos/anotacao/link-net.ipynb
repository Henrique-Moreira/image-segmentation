{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compreendendo a Arquitetura LINKNET\n",
    "LinkNet é uma arquitetura de rede neural convolucional projetada para tarefas de segmentação semântica de imagens. A LinkNet propoe uma eĄciente e de alta precisão para a segmentação de objetos em imagens, especialmente em cenários onde o número de classes a serem segmentadas é grande.\n",
    "\n",
    "Arquitetura LinkNet, adaptada por (RAMASAMY; SINGH; YUAN, 2023). Torna-a computacionalmente eficiente e facilmente treinável, proporcionando uma abordagem eficaz para segmentação semântica de imagens. Ela é aplicada em várias tarefas que requerem identificação e separação precisa de objetos em imagens, como detecção de objetos, reconhecimento de padrões e análise de cenas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalação de Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install torch\n",
    "%pip install torchvision\n",
    "%pip install utils2\n",
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importe os módulos necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Train\n",
    "matplotlib.use('agg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaração de Variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório do Projeto c:\\git\\image-segmentation\\dataset\\base.\n"
     ]
    }
   ],
   "source": [
    "# CUDA:\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Caminho do diretório Dataset:\n",
    "directory = os.path.abspath(os.path.join(os.getcwd(), '..\\\\..')) + r'\\dataset\\base'\n",
    "print(f'Diretório do Projeto {directory}.')\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "img_folder_val = directory + r'\\Val'\n",
    "img_folder_train = directory + r'\\Train'\n",
    "img_folder_test = directory + r'\\Test'\n",
    "save_dir = directory + r'\\result_linknet'\n",
    "if not os.path.exists(img_folder_val):\n",
    "    os.makedirs(img_folder_val)\n",
    "if not os.path.exists(img_folder_train):\n",
    "    os.makedirs(img_folder_train)\n",
    "if not os.path.exists(img_folder_test):\n",
    "    os.makedirs(img_folder_test)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Local onde o Modelo será salvo\n",
    "model_file_name = save_dir + '\\model_linknet.pth'\n",
    "\n",
    "# Configurações do treinamento\n",
    "resolution_input = (640, 480)  # Tamanho de entrada\n",
    "assert resolution_input[0] % 32 == 0 and resolution_input[1] % 32 == 0, \"A resolução de entrada deve ser divisível por 32.\"\n",
    "patience = 30\n",
    "plot_val = True\n",
    "plot_train = True\n",
    "max_epochs = 300\n",
    "class_weights = [1.0, 2.0, 3.0] \n",
    "class_weights = torch.tensor(class_weights).to(device)  \n",
    "num_classes = 3\n",
    "\n",
    "# Mapeamento de classes e cores\n",
    "class_to_color = {'Doenca': (255, 0, 0), 'Solo': (0, 0, 255), 'Saudavel': (0, 255, 255)}\n",
    "class_to_id = {'Doenca': 0, 'Solo': 1, 'Saudavel': 2}\n",
    "id_to_class = {v: k for k, v in class_to_id.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase para Segmentação de Dataset\n",
    "\n",
    "    Este arquivo provavelmente contém funções utilitárias para manipular e preparar os dados para o treinamento do modelo. As utilidades de dados aqui podem incluir:\n",
    "        - Carregamento de dados de diferentes fontes (arquivos, bancos de dados, APIs).\n",
    "        - Limpeza e pré-processamento de dados (por exemplo, lidar com valores ausentes, normalização, conversão de tipos de dados).\n",
    "        - Aumento de dados para aumentar o tamanho do conjunto de dados de treinamento.\n",
    "        - Divisão dos dados em conjuntos de treinamento, validação e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    \"\"\"Segmentation dataset loader.\"\"\"\n",
    "\n",
    "    def __init__(self, json_folder, img_folder, is_train, class_to_id, resolution_input = (640, 480), augmentation = False, transform=None):\n",
    "    #def __init__(self, json_folder, img_folder, is_train, class_to_id, resolution_input = (1280, 960), augmentation = False, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            json_folder (str): Path to folder that contains the annotations.\n",
    "            img_folder (str): Path to all images.\n",
    "            is_train (bool): Is this a training dataset ?\n",
    "            augmentation (bool): Do dataset augmentation (crete artificial variance) ?\n",
    "        \"\"\"\n",
    "\n",
    "        self.gt_file_list = glob.glob(osp.join(json_folder, '*.json'))\n",
    "\n",
    "        self.total_samples = len(self.gt_file_list)\n",
    "        self.img_folder = img_folder\n",
    "        self.is_train = is_train\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "        self.resolution = resolution_input\n",
    "        self.class_to_id = class_to_id\n",
    "        \n",
    "        \n",
    "        # Mean and std are needed because we start from a pre trained net\n",
    "        self.mean = [0.485, 0.456, 0.406]\n",
    "        self.std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gt_file = self.gt_file_list[idx]\n",
    "        img_number_str = gt_file.split('.')[0].split('/')[-1]\n",
    "        \n",
    "        # Abre Json\n",
    "        gt_json = json.load(open(gt_file, 'r'))\n",
    "        \n",
    "        # Abre imagem\n",
    "        img_np = cv2.imread(osp.join(self.img_folder, img_number_str + '.png'), cv2.IMREAD_IGNORE_ORIENTATION + cv2.IMREAD_COLOR)\n",
    "        original_shape = img_np.shape\n",
    "        \n",
    "        # Redimensiona a imagem\n",
    "        img_np = cv2.resize(img_np, (self.resolution[0], self.resolution[1]))[..., ::-1]\n",
    "        img_np = np.ascontiguousarray(img_np)\n",
    "        \n",
    "        # Cria imagem zerada para os rótulos\n",
    "        label_np = np.zeros((img_np.shape[0], img_np.shape[1]))\n",
    "        label_np[...] = -1\n",
    "        \n",
    "        # Para todos os polígonos\n",
    "        for shape in gt_json['shapes']:\n",
    "            # Transforma os pontos do polígono em array\n",
    "            points_np = np.array(shape['points'], dtype=np.float64)\n",
    "            \n",
    "            # Ajusta os pontos porque eu mudo a resolução\n",
    "            points_np[:, 0] *= self.resolution[0] / original_shape[1]\n",
    "            points_np[:, 1] *= self.resolution[1] / original_shape[0]\n",
    "            \n",
    "            # As coordenadas dos pontos que formam o polígono têm que ser inteiros\n",
    "            points_np = np.round(points_np).astype(np.int64)\n",
    "            \n",
    "            # Coloca os pontos no formato certo para o OpenCV\n",
    "            points_np = points_np.reshape((-1, 1, 2))\n",
    "            \n",
    "            # Pinta o polígono usando o OpenCV com o valor referente ao rótulo\n",
    "            label_np = cv2.fillPoly(label_np, [points_np], self.class_to_id[shape['label']])\n",
    "        \n",
    "        # Transforma o GT em inteiro\n",
    "        label_np = label_np.astype(np.int32)\n",
    "        \n",
    "        # Aumento de dados (opcional)\n",
    "        if self.is_train and self.augmentation:\n",
    "            if np.random.rand() > 0.5:\n",
    "                img_np = np.fliplr(img_np)\n",
    "                label_np = np.fliplr(label_np)\n",
    "                img_np = np.ascontiguousarray(img_np)\n",
    "                label_np = np.ascontiguousarray(label_np)\n",
    "        \n",
    "        # Normalização da imagem\n",
    "        img_pt = img_np.astype(np.float32) / 255.0\n",
    "        for i in range(3):\n",
    "            img_pt[..., i] -= self.mean[i]\n",
    "            img_pt[..., i] /= self.std[i]\n",
    "        \n",
    "        # Transposição para formato de tensor\n",
    "        img_pt = img_pt.transpose(2, 0, 1)\n",
    "        img_pt = torch.from_numpy(img_pt)\n",
    "        label_pt = torch.from_numpy(label_np).long()\n",
    "        \n",
    "        return {'image': img_pt, 'gt': label_pt, 'image_original': img_np}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINKNETVGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinkNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LinkNet, self).__init__()\n",
    "        \n",
    "        # Primeira camada convolucional (entrada com 3 canais, saída com 64 canais)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        \n",
    "        # Segunda camada convolucional (saída com 128 canais)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Camada para ajustar de volta para 64 canais (conforme esperado nas camadas seguintes)\n",
    "        self.adjust_channels = nn.Conv2d(128, 64, kernel_size=1)\n",
    "        \n",
    "        # Camada final de classificação\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Passa pela primeira convolução\n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        # Passa pela segunda convolução (128 canais)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        # Ajustar os canais de volta para 64\n",
    "        x = F.relu(self.adjust_channels(x))\n",
    "        \n",
    "        # Saída final de classificação\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "    # Ajustar a função de perda para ignorar o valor -1 (áreas sem rótulo)\n",
    "    def eval_net_with_loss(self, image, gt, class_weights, device):\n",
    "        # Forward pass: obter a saída do modelo\n",
    "        output = self(image)\n",
    "        \n",
    "        # Redimensionar o alvo para corresponder à saída\n",
    "        gt_resized = F.interpolate(gt.unsqueeze(1).float(), size=output.shape[2:], mode='nearest').squeeze(1).long()\n",
    "        \n",
    "        # Verificações de integridade, se as dimensões de saída e do alvo coincidem\n",
    "        assert output.shape[2:] == gt_resized.shape[1:], \\\n",
    "            f\"Dimension mismatch: Output size {output.shape[2:]} and target size {gt_resized.shape[1:]}\"\n",
    "\n",
    "        # print(f\"Tamanho da entrada: {image.shape}\")\n",
    "        # print(f\"Tamanho da saída: {output.shape}\")\n",
    "        # print(f\"Tamanho do alvo redimensionado: {gt_resized.shape}\")\n",
    "\n",
    "        # Ajustar para que a função de perda ignore regiões com -1\n",
    "        loss_fn = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "        \n",
    "        # Calcular a perda\n",
    "        loss = loss_fn(output, gt_resized)\n",
    "        \n",
    "        return output, loss\n",
    "    \n",
    "    def get_params_by_kind(model):\n",
    "        \"\"\"\n",
    "        Retorna os pesos e vieses das camadas do modelo separadamente.\n",
    "        \"\"\"\n",
    "        base_vgg_weight = []\n",
    "        base_vgg_bias = []\n",
    "        core_weight = []\n",
    "        core_bias = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            # Verifica se o parâmetro faz parte de uma camada convolucional\n",
    "            if 'conv' in name:\n",
    "                if 'weight' in name:\n",
    "                    base_vgg_weight.append(param)\n",
    "                elif 'bias' in name:\n",
    "                    base_vgg_bias.append(param)\n",
    "            else:\n",
    "                if 'weight' in name:\n",
    "                    core_weight.append(param)\n",
    "                elif 'bias' in name:\n",
    "                    core_bias.append(param)\n",
    "        \n",
    "        return base_vgg_weight, base_vgg_bias, core_weight, core_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realiza o Treinamento da Rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de amostras no dataset de treinamento: 20\n",
      "Arquivos no dataset de treinamento: ['DJI_0026.JPG', 'DJI_0026.json', 'DJI_0064.JPG', 'DJI_0064.json', 'DJI_0089.JPG', 'DJI_0089.json', 'DJI_0105.JPG', 'DJI_0105.json', 'DJI_0122.JPG', 'DJI_0122.json', 'DJI_0160.JPG', 'DJI_0160.json', 'DJI_0161.JPG', 'DJI_0161.json', 'DJI_0165.JPG', 'DJI_0165.json', 'DJI_0211.JPG', 'DJI_0211.json', 'DJI_0214.JPG', 'DJI_0214.json', 'DJI_0217.JPG', 'DJI_0217.json', 'DJI_0233.JPG', 'DJI_0233.json', 'DJI_0234.JPG', 'DJI_0234.json', 'DJI_0237.JPG', 'DJI_0237.json', 'DJI_0252.JPG', 'DJI_0252.json', 'DJI_0402.JPG', 'DJI_0402.json', 'DJI_0432.JPG', 'DJI_0432.json', 'DJI_0433.JPG', 'DJI_0433.json', 'DJI_0435.JPG', 'DJI_0435.json', 'DJI_0444.JPG', 'DJI_0444.json']\n",
      "Número de amostras no dataset de validação: 10\n",
      "Arquivos no dataset de validação: ['DJI_0052.JPG', 'DJI_0052.json', 'DJI_0085.JPG', 'DJI_0085.json', 'DJI_0107.JPG', 'DJI_0107.json', 'DJI_0124.JPG', 'DJI_0124.json', 'DJI_0163.JPG', 'DJI_0163.json', 'DJI_0212.JPG', 'DJI_0212.json', 'DJI_0215.JPG', 'DJI_0215.json', 'DJI_0236.JPG', 'DJI_0236.json', 'DJI_0251.JPG', 'DJI_0251.json', 'DJI_0434.JPG', 'DJI_0434.json']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m\n\u001b[0;32m     15\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plot_train:\n\u001b[1;32m---> 20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_batched\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimage_np\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_batched\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_original\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_batched\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\git\\image-segmentation\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\git\\image-segmentation\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\git\\image-segmentation\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\git\\image-segmentation\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[5], line 41\u001b[0m, in \u001b[0;36mSegmentationDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Abre imagem\u001b[39;00m\n\u001b[0;32m     40\u001b[0m img_np \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_folder, img_number_str \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mIMREAD_IGNORE_ORIENTATION \u001b[38;5;241m+\u001b[39m cv2\u001b[38;5;241m.\u001b[39mIMREAD_COLOR)\n\u001b[1;32m---> 41\u001b[0m original_shape \u001b[38;5;241m=\u001b[39m \u001b[43mimg_np\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Redimensiona a imagem\u001b[39;00m\n\u001b[0;32m     44\u001b[0m img_np \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(img_np, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolution[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolution[\u001b[38;5;241m1\u001b[39m]))[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Inicializar listas para armazenar a perda e a precisão\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Inicia o treinamento\n",
    "train_dataset = SegmentationDataset(img_folder_train, img_folder_train, True, class_to_id, resolution_input, True)\n",
    "print(f\"Número de amostras no dataset de treinamento: {len(train_dataset)}\")\n",
    "print(f\"Arquivos no dataset de treinamento: {os.listdir(img_folder_train)}\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, drop_last=True)\n",
    "print(f\"Número de amostras no dataset de validação: {len(val_dataset)}\")\n",
    "print(f\"Arquivos no dataset de validação: {os.listdir(img_folder_val)}\")\n",
    "\n",
    "val_dataset = SegmentationDataset(img_folder_val, img_folder_val, False, class_to_id, resolution_input)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=0, drop_last=False)\n",
    "\n",
    "\n",
    "if plot_train:\n",
    "\n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "    \n",
    "            image_np = np.squeeze(sample_batched['image_original'].cpu().numpy())\n",
    "            gt = np.squeeze(sample_batched['gt'].cpu().numpy())\n",
    "                \n",
    "            color_label = np.zeros((resolution_input[1], resolution_input[0], 3))\n",
    "            \n",
    "            for key, val in id_to_class.items():\n",
    "                color_label[gt == key] = class_to_color[val]\n",
    "                \n",
    "            plt.figure()\n",
    "            plt.imshow((image_np/255) * 0.5 + (color_label/255) * 0.5)\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.imshow(color_label.astype(np.uint8))\n",
    "            plt.show()\n",
    "\n",
    "num_classes = 3  # Exemplo de número de classes\n",
    "model = LinkNet(num_classes).to(device)\n",
    "\n",
    "# Gerar uma entrada aleatória (dummy_input) com tamanho (1, 3, 480, 640)\n",
    "dummy_input = torch.randn(1, 3, 480, 640).to(device)\n",
    "\n",
    "# Passar a entrada pelo modelo\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "# Verificar o tamanho da saída\n",
    "print(f\"Input shape: {dummy_input.shape}, Output shape: {dummy_output.shape}\")\n",
    "\n",
    "\n",
    "# Essa função itera sobre os parâmetros do modelo e os separa em pesos e vieses (bias), distinguindo entre camadas convolucionais (base do VGG) e outras partes do modelo (núcleo/core).\n",
    "# Filtragem por Nome: O nome de cada parâmetro é inspecionado para identificar se ele é um peso ou viés de uma camada convolucional (usando conv no nome).\n",
    "base_vgg_weight, base_vgg_bias, core_weight, core_bias = LinkNet.get_params_by_kind(model)\n",
    "\n",
    "# Learning rate para a parte principal do modelo\n",
    "core_lr = 0.02  \n",
    "\n",
    "# Otimizador SGD com diferentes taxas de aprendizado para vieses e pesos\n",
    "optimizer = torch.optim.SGD([\n",
    "    {'params': base_vgg_bias, 'lr': 0.000001}, \n",
    "    {'params': base_vgg_weight, 'lr': 0.000001},\n",
    "    {'params': core_bias, 'lr': core_lr},\n",
    "    {'params': core_weight, 'lr': core_lr}\n",
    "])\n",
    "\n",
    "# Scheduler para ajustar a taxa de aprendizado ao longo do tempo\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.2)\n",
    "\n",
    "# Treinamento e validação\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "n_correct = 0\n",
    "n_false = 0\n",
    "val_accuracies = []\n",
    "patience = 10\n",
    "\n",
    "# Start training...\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    print('Epoch %d starting...' % (epoch+1))\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    model.train()\n",
    "    mean_loss = 0\n",
    "        \n",
    "    # Dentro do loop de treinamento\n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "        image = sample_batched['image'].to(device)\n",
    "        gt = sample_batched['gt'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, total_loss = model.eval_net_with_loss(image, gt, class_weights, device)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        mean_loss += total_loss.cpu().detach().numpy()\n",
    "\n",
    "        # Redimensionar o alvo para o tamanho da saída\n",
    "        gt_resized = F.interpolate(gt.unsqueeze(1).float(), size=(output.shape[2], output.shape[3]), mode='nearest').squeeze(1).long()\n",
    "\n",
    "        # Medir precisão\n",
    "        gt_resized_np = np.squeeze(gt_resized.cpu().numpy())\n",
    "        label_out = torch.nn.functional.softmax(output, dim=1)\n",
    "        label_out = label_out.cpu().detach().numpy()\n",
    "        label_out = np.squeeze(label_out)\n",
    "        labels = np.argmax(label_out, axis=0)\n",
    "        valid_mask = gt_resized_np != -1\n",
    "        curr_correct = np.sum(gt_resized_np[valid_mask] == labels[valid_mask])\n",
    "        curr_false = np.sum(valid_mask) - curr_correct\n",
    "        n_correct += curr_correct\n",
    "        n_false += curr_false    \n",
    "\n",
    "    total_acc = n_correct / (n_correct + n_false)\n",
    "    val_accuracies.append(total_acc)\n",
    "\n",
    "    if best_val_acc < total_acc:\n",
    "        best_val_acc = total_acc\n",
    "        if epoch > 7:\n",
    "            torch.save(model.state_dict(), model_file_name)\n",
    "            print('Nova melhor conta de validação. Salvo... %f' % epoch)\n",
    "        best_epoch = epoch\n",
    "\n",
    "    if (epoch - best_epoch) > patience:\n",
    "        print(\"Terminando o treinamento, melhor conta de validação %f\" % best_val_acc)\n",
    "        break\n",
    "\n",
    "    print('Validação Acc: %f -- Melhor Avaliação Acc: %f -- epoch %d.' % (total_acc, best_val_acc, best_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotar os gráficos de perda e precisão\n",
    "    Inicialização das listas: train_losses, train_accuracies e val_accuracies são listas para armazenar a perda e a precisão de treinamento e validação em cada época.\n",
    "    \n",
    "    Armazenamento dos valores: Durante o loop de treinamento, a perda e a precisão são calculadas e armazenadas nas listas correspondentes.\n",
    "    Plotagem dos gráficos: Após o loop de treinamento, os gráficos de perda e precisão são plotados usando matplotlib.\n",
    "\n",
    "Este código deve ser adicionado ao final do seu loop de treinamento no notebook para visualizar os resultados do treinamento ao longo das épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supondo que train_losses, train_accuracies e val_accuracies já estejam definidos\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Primeiro gráfico: Perda de Treinamento\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Perda de Treinamento')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Perda')\n",
    "plt.title('Perda de Treinamento ao longo das Épocas')\n",
    "plt.legend()\n",
    "\n",
    "# Segundo gráfico: Precisão de Treinamento e Validação\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Precisão de Treinamento')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Precisão de Validação')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Precisão')\n",
    "plt.title('Precisão de Treinamento e Validação ao longo das Épocas')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferência de dados\n",
    "Processo de usar um modelo treinado para fazer previsões sobre novos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color in RGB\n",
    "class_to_color = {'Ground': (127, 0, 0) , 'Healthy': (0, 127, 127), 'Pest': (0, 255, 0)}\n",
    "class_to_id = {'Ground': 0, 'Healthy': 1, 'Pest': 2}\n",
    "id_to_class = {v: k for k, v in class_to_id.items()}\n",
    "num_classes = 21\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "model = LinkNet(num_classes)\n",
    "model.load_state_dict(torch.load(model_file_name))\n",
    "model.eval()\n",
    "print(\"Modelo carregado e pronto para uso.\")\n",
    "model.to(device)\n",
    "\n",
    "img_list = glob.glob(osp.join(img_folder_val, '*.png'))\n",
    "\n",
    "for img_path in img_list:\n",
    "\n",
    "        img_np = cv2.imread(img_path, cv2.IMREAD_IGNORE_ORIENTATION + cv2.IMREAD_COLOR)\n",
    "        img_np = cv2.resize(img_np, (resolution_input[0], resolution_input[1]))[..., ::-1]\n",
    "        img_np = np.ascontiguousarray(img_np)\n",
    "        \n",
    "        img_pt = np.copy(img_np).astype(np.float32) / 255.0\n",
    "        for i in range(3):\n",
    "            img_pt[..., i] -= mean[i]\n",
    "            img_pt[..., i] /= std[i]\n",
    "            \n",
    "        img_pt = img_pt.transpose(2,0,1)\n",
    "            \n",
    "        img_pt = torch.from_numpy(img_pt[None, ...]).to(device)\n",
    "        \n",
    "        label_out = model(img_pt)\n",
    "        label_out = torch.nn.functional.softmax(label_out, dim = 1)\n",
    "        label_out = label_out.cpu().detach().numpy()\n",
    "        label_out = np.squeeze(label_out)\n",
    "        \n",
    "        labels = np.argmax(label_out, axis=0)\n",
    "        \n",
    "        color_label = np.zeros((resolution_input[1], resolution_input[0], 3))\n",
    "            \n",
    "        for key, val in id_to_class.items():\n",
    "            color_label[labels == key] = class_to_color[val]\n",
    "            \n",
    "        plt.figure()\n",
    "        plt.imshow((img_np/255) * 0.5 + (color_label/255) * 0.5)\n",
    "        plt.savefig(save_dir + \"IMG\" + \".png\")\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.imshow(color_label.astype(np.uint8))\n",
    "        plt.savefig(save_dir + \"GT\" + \".png\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
