{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import os.path as osp\n",
    "import glob\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição da classe UNetVgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição da classe UNetVgg\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "class UNetVgg(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    BorderNetwork is a NN that aims to detected border and classify occlusion.\n",
    "    The architecture is a VGG without the last pool layer. After that we \n",
    "    have two paths, one for regression and one for classification (occlusion).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(UNetVgg, self).__init__()\n",
    " \n",
    "        vgg16pre = torchvision.models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "        self.vgg0 = torch.nn.Sequential(*list(vgg16pre.features.children())[:4])\n",
    "        self.vgg1 = torch.nn.Sequential(*list(vgg16pre.features.children())[4:9])\n",
    "        self.vgg2 = torch.nn.Sequential(*list(vgg16pre.features.children())[9:16])\n",
    "        self.vgg3 = torch.nn.Sequential(*list(vgg16pre.features.children())[16:23])\n",
    "        self.vgg4 = torch.nn.Sequential(*list(vgg16pre.features.children())[23:30])\n",
    "        \n",
    "        \n",
    "        self.smooth0 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(128, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True),\n",
    "                torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True)\n",
    "                )\n",
    "        self.smooth1 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(256, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True),\n",
    "                torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True)\n",
    "                )\n",
    "        self.smooth2 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(512, 128, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True),\n",
    "                torch.nn.Conv2d(128, 128, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True)\n",
    "                )\n",
    "        self.smooth3 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(1024, 256, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True),\n",
    "                torch.nn.Conv2d(256, 256, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True)\n",
    "                )\n",
    "        \n",
    "        \n",
    "        self.final = torch.nn.Conv2d(64, nClasses, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.tensor): A tensor of size (batch, 3, H, W)\n",
    "        Returns:\n",
    "            reg_out (torch.tensor): A tensor with results of the regression (batch, 4).\n",
    "            cls_out (torch.tensor): A tensor with results of the classification (batch, 2).\n",
    "        \"\"\"\n",
    "        \n",
    "        feat0 = self.vgg0(x)\n",
    "        feat1 = self.vgg1(feat0)\n",
    "        feat2 = self.vgg2(feat1)\n",
    "        feat3 = self.vgg3(feat2)\n",
    "        feat4 = self.vgg4(feat3)\n",
    "        \n",
    "        _,_,H,W = feat3.size()\n",
    "        up3 = torch.nn.functional.interpolate(feat4, size=(H,W), mode='bilinear', align_corners=False)\n",
    "        concat3 = torch.cat([feat3, up3], 1)\n",
    "        end3 = self.smooth3(concat3)\n",
    "        \n",
    "        _,_,H,W = feat2.size()\n",
    "        up2 = torch.nn.functional.interpolate(end3, size=(H,W), mode='bilinear', align_corners=False)\n",
    "        concat2 = torch.cat([feat2, up2], 1)\n",
    "        end2 = self.smooth2(concat2)\n",
    "        \n",
    "        _,_,H,W = feat1.size()\n",
    "        up1 = torch.nn.functional.interpolate(end2, size=(H,W), mode='bilinear', align_corners=False)\n",
    "        concat1 = torch.cat([feat1, up1], 1)\n",
    "        end1 = self.smooth1(concat1)\n",
    "        \n",
    "        _,_,H,W = feat0.size()\n",
    "        up0 = torch.nn.functional.interpolate(end1, size=(H,W), mode='bilinear', align_corners=False)\n",
    "        concat0 = torch.cat([feat0, up0], 1)\n",
    "        end0 = self.smooth0(concat0)\n",
    "        \n",
    "        return self.final(end0)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def eval_net_with_loss(model, inp, gt, class_weights, device):\n",
    "        \"\"\"\n",
    "        Evaluate network including loss.\n",
    "        \n",
    "        Args:\n",
    "            model (torch.nn.Module): The model.\n",
    "            inp (torch.tensor): A tensor (float32) of size (batch, 3, H, W)\n",
    "            gt (torch.tensor): A tensor (long) of size (batch, 1, H, W) with the groud truth (0 to num_classes-1).\n",
    "            class_weights (list of float): A list with len == num_classes.\n",
    "            device (torch.device): device to perform computation\n",
    "            \n",
    "        Returns:\n",
    "            out (torch.tensor): Network output.\n",
    "            loss (torch.tensor): Tensor with the total loss.\n",
    "                \n",
    "        \"\"\"\n",
    "        weights = torch.from_numpy(np.array(class_weights, dtype=np.float32)).to(device)\n",
    "        out = model(inp)\n",
    "        \n",
    "        softmax = torch.nn.functional.log_softmax(out, dim = 1)\n",
    "        loss = torch.nn.functional.nll_loss(softmax, gt, ignore_index=-1, weight=weights)\n",
    "            \n",
    "        return (out, loss)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_params_by_kind(model, n_base = 7):\n",
    "    \n",
    "        base_vgg_bias = []\n",
    "        base_vgg_weight = []\n",
    "        core_weight = []\n",
    "        core_bias = []\n",
    "    \n",
    "        for name, param in model.named_parameters():\n",
    "            if 'vgg' in name and ('weight' in name or 'bias' in name):\n",
    "                vgglayer = int(name.split('.')[-2])\n",
    "                \n",
    "                if vgglayer <= n_base:\n",
    "                    if 'bias' in name:\n",
    "                        print('Adding %s to base vgg bias.' % (name))\n",
    "                        base_vgg_bias.append(param)\n",
    "                    else:\n",
    "                        base_vgg_weight.append(param)\n",
    "                        print('Adding %s to base vgg weight.' % (name))\n",
    "                else:\n",
    "                    if 'bias' in name:\n",
    "                        print('Adding %s to core bias.' % (name))\n",
    "                        core_bias.append(param)\n",
    "                    else:\n",
    "                        print('Adding %s to core weight.' % (name))\n",
    "                        core_weight.append(param)\n",
    "                        \n",
    "            elif ('weight' in name or 'bias' in name):\n",
    "                if 'bias' in name:\n",
    "                    print('Adding %s to core bias.' % (name))\n",
    "                    core_bias.append(param)\n",
    "                else:\n",
    "                    print('Adding %s to core weight.' % (name))\n",
    "                    core_weight.append(param)\n",
    "                    \n",
    "        return (base_vgg_weight, base_vgg_bias, core_weight, core_bias)\n",
    "    \n",
    "# End class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declarações Variavies e outras configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponível: True\n",
      "Nome da GPU: NVIDIA GeForce RTX 4070\n",
      "VRAM Total: 11.99 GB\n",
      "Diretório do Projeto c:\\git\\image-segmentation\\dataset.\n"
     ]
    }
   ],
   "source": [
    "# Variável que define se as figuras são exibidas no console ou salvas em um arquivo\n",
    "plt_show = False\n",
    "plt_savefig = False\n",
    "\n",
    "# Configuração do dispositivo CUDA\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f'CUDA disponível: {cuda_available}')\n",
    "\n",
    "if cuda_available:\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_total = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # Convertendo para GB\n",
    "    vram_available = torch.cuda.memory_reserved(0) / (1024 ** 3)  # Convertendo para GB\n",
    "    print(f'Nome da GPU: {gpu_name}')\n",
    "    print(f'VRAM Total: {vram_total:.2f} GB')\n",
    "\n",
    "# Caminho do diretório Dataset\n",
    "directory = os.path.abspath(os.path.join(os.getcwd(), '..\\\\..')) + r'\\dataset'\n",
    "print(f'Diretório do Projeto {directory}.')\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "img_folder_val = directory + r'\\\\base\\\\Val'\n",
    "img_folder_train = directory + r'\\\\base\\\\Train'\n",
    "img_folder_test = directory + r'\\\\base\\\\Test'\n",
    "save_dir = directory + r'\\\\result_UnetVgg\\\\'\n",
    "if not os.path.exists(img_folder_val):\n",
    "    os.makedirs(img_folder_val)\n",
    "if not os.path.exists(img_folder_train):\n",
    "    os.makedirs(img_folder_train)\n",
    "if not os.path.exists(img_folder_test):\n",
    "    os.makedirs(img_folder_test)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "## Imagens Segmentadas\n",
    "img_folder_train_segmentadas = directory + r'\\\\segmentadas\\\\train\\\\'\n",
    "img_folder_val_segmentadas = directory + r'\\\\segmentadas\\\\val\\\\'\n",
    "img_folder_test_segmentadas = directory + r'\\\\segmentadas\\\\test\\\\'\n",
    "if not os.path.exists(img_folder_train_segmentadas):\n",
    "    os.makedirs(img_folder_train_segmentadas)\n",
    "if not os.path.exists(img_folder_val_segmentadas):\n",
    "    os.makedirs(img_folder_val_segmentadas)\n",
    "if not os.path.exists(img_folder_test_segmentadas):\n",
    "    os.makedirs(img_folder_test_segmentadas)\n",
    "    \n",
    "# Local onde o Modelo será salvo\n",
    "model_file_name = save_dir + 'model_u-net-VGG16-IMAGENET1K_V1.pth'\n",
    "\n",
    "# Configurações do treinamento\n",
    "resolution_input = (640, 480)  # Tamanho de entrada\n",
    "patience = 30\n",
    "plot_val = True\n",
    "plot_train = True\n",
    "max_epochs = 300\n",
    "class_weights = [1, 1, 1]\n",
    "nClasses = 3\n",
    "\n",
    "# Mapeamento de classes e cores\n",
    "class_to_color = {'Doenca': (255, 0, 0), 'Solo': (0, 0, 255), 'Saudavel': (0, 255, 255)}\n",
    "class_to_id = {'Doenca': 0, 'Solo': 1, 'Saudavel': 2}\n",
    "id_to_class = {v: k for k, v in class_to_id.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição do Dataset\n",
    "class SegmentationDataset(Dataset):\n",
    "    \"\"\"Segmentation dataset loader.\"\"\"\n",
    "\n",
    "    def __init__(self, json_folder, img_folder, is_train, class_to_id, resolution_input=(640, 480), augmentation=False, transform=None):\n",
    "        self.gt_file_list = glob.glob(osp.join(json_folder, '*.json'))\n",
    "        self.total_samples = len(self.gt_file_list)\n",
    "        self.img_folder = img_folder\n",
    "        self.is_train = is_train\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "        self.resolution = resolution_input\n",
    "        self.class_to_id = class_to_id\n",
    "        self.mean = [0.485, 0.456, 0.406]\n",
    "        self.std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gt_file = self.gt_file_list[idx]\n",
    "        img_number_str = osp.splitext(osp.basename(gt_file))[0]\n",
    "        \n",
    "        # Verificação de existência de arquivos\n",
    "        if not osp.exists(gt_file):\n",
    "            raise FileNotFoundError(f\"Arquivo JSON não encontrado: {gt_file}\")\n",
    "        \n",
    "        # Extrair o nome da imagem considerando que o nome da classe está no meio do nome do arquivo\n",
    "        img_path = osp.join(self.img_folder, img_number_str + '.JPG')\n",
    "        \n",
    "        if not osp.exists(img_path):\n",
    "            raise FileNotFoundError(f\"Imagem não encontrada: {img_path}\")\n",
    "        \n",
    "        gt_json = json.load(open(gt_file, 'r'))\n",
    "        img_np = cv2.imread(img_path, cv2.IMREAD_IGNORE_ORIENTATION + cv2.IMREAD_COLOR)\n",
    "        \n",
    "        if img_np is None:\n",
    "            raise FileNotFoundError(f\"Imagem não encontrada: {img_path}\")\n",
    "        \n",
    "        original_shape = img_np.shape\n",
    "        img_np = cv2.resize(img_np, (self.resolution[0], self.resolution[1]))[..., ::-1]\n",
    "        img_np = np.ascontiguousarray(img_np)\n",
    "        label_np = np.zeros((img_np.shape[0], img_np.shape[1]))\n",
    "        label_np[...] = -1\n",
    "\n",
    "        for shape in gt_json['shapes']:\n",
    "            points_np = np.array(shape['points'], dtype=np.float64)\n",
    "            points_np[:, 0] *= self.resolution[0] / original_shape[1]\n",
    "            points_np[:, 1] *= self.resolution[1] / original_shape[0]\n",
    "            points_np = np.round(points_np).astype(np.int64)\n",
    "            points_np = points_np.reshape((-1, 1, 2))\n",
    "            label = shape['label']\n",
    "            if label not in self.class_to_id:\n",
    "                raise KeyError(f\"Label '{label}' não encontrado em class_to_id\")\n",
    "            label_np = cv2.fillPoly(label_np, [points_np], self.class_to_id[label])\n",
    "\n",
    "        label_np = label_np.astype(np.int32)\n",
    "\n",
    "        if self.is_train and self.augmentation:\n",
    "            if np.random.rand() > 0.5:\n",
    "                img_np = np.fliplr(img_np)\n",
    "                label_np = np.fliplr(label_np)\n",
    "                img_np = np.ascontiguousarray(img_np)\n",
    "                label_np = np.ascontiguousarray(label_np)\n",
    "\n",
    "        img_pt = img_np.astype(np.float32) / 255.0\n",
    "        for i in range(3):\n",
    "            img_pt[..., i] -= self.mean[i]\n",
    "            img_pt[..., i] /= self.std[i]\n",
    "\n",
    "        img_pt = img_pt.transpose(2, 0, 1)\n",
    "        img_pt = torch.from_numpy(img_pt)\n",
    "        label_pt = torch.from_numpy(label_np).long()\n",
    "\n",
    "        sample = {'image': img_pt, 'gt': label_pt, 'image_original': img_np}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de amostras no dataset de treinamento: 189\n",
      "Arquivos no dataset de treinamento: ['DJI_0026.JPG', 'DJI_0026.json', 'DJI_0026_Doenca_0.JPG', 'DJI_0026_Doenca_0.json', 'DJI_0026_Saudavel_2.JPG', 'DJI_0026_Saudavel_2.json', 'DJI_0026_Saudavel_3.JPG', 'DJI_0026_Saudavel_3.json', 'DJI_0026_Saudavel_4.JPG', 'DJI_0026_Saudavel_4.json', 'DJI_0026_Solo_1.JPG', 'DJI_0026_Solo_1.json', 'DJI_0064.JPG', 'DJI_0064.json', 'DJI_0064_Doenca_0.JPG', 'DJI_0064_Doenca_0.json', 'DJI_0064_Saudavel_1.JPG', 'DJI_0064_Saudavel_1.json', 'DJI_0064_Saudavel_2.JPG', 'DJI_0064_Saudavel_2.json', 'DJI_0064_Saudavel_3.JPG', 'DJI_0064_Saudavel_3.json', 'DJI_0064_Saudavel_4.JPG', 'DJI_0064_Saudavel_4.json', 'DJI_0064_Saudavel_5.JPG', 'DJI_0064_Saudavel_5.json', 'DJI_0064_Saudavel_6.JPG', 'DJI_0064_Saudavel_6.json', 'DJI_0064_Solo_7.JPG', 'DJI_0064_Solo_7.json', 'DJI_0089.JPG', 'DJI_0089.json', 'DJI_0089_Doenca_0.JPG', 'DJI_0089_Doenca_0.json', 'DJI_0089_Saudavel_1.JPG', 'DJI_0089_Saudavel_1.json', 'DJI_0089_Saudavel_10.JPG', 'DJI_0089_Saudavel_10.json', 'DJI_0089_Saudavel_11.JPG', 'DJI_0089_Saudavel_11.json', 'DJI_0089_Saudavel_2.JPG', 'DJI_0089_Saudavel_2.json', 'DJI_0089_Saudavel_4.JPG', 'DJI_0089_Saudavel_4.json', 'DJI_0089_Saudavel_5.JPG', 'DJI_0089_Saudavel_5.json', 'DJI_0089_Saudavel_6.JPG', 'DJI_0089_Saudavel_6.json', 'DJI_0089_Saudavel_7.JPG', 'DJI_0089_Saudavel_7.json', 'DJI_0089_Saudavel_8.JPG', 'DJI_0089_Saudavel_8.json', 'DJI_0089_Saudavel_9.JPG', 'DJI_0089_Saudavel_9.json', 'DJI_0089_Solo_3.JPG', 'DJI_0089_Solo_3.json', 'DJI_0105.JPG', 'DJI_0105.json', 'DJI_0105_Doenca_0.JPG', 'DJI_0105_Doenca_0.json', 'DJI_0105_Doenca_5.JPG', 'DJI_0105_Doenca_5.json', 'DJI_0105_Saudavel_1.JPG', 'DJI_0105_Saudavel_1.json', 'DJI_0105_Saudavel_2.JPG', 'DJI_0105_Saudavel_2.json', 'DJI_0105_Saudavel_3.JPG', 'DJI_0105_Saudavel_3.json', 'DJI_0105_Saudavel_4.JPG', 'DJI_0105_Saudavel_4.json', 'DJI_0105_Saudavel_6.JPG', 'DJI_0105_Saudavel_6.json', 'DJI_0105_Solo_7.JPG', 'DJI_0105_Solo_7.json', 'DJI_0105_Solo_8.JPG', 'DJI_0105_Solo_8.json', 'DJI_0122.JPG', 'DJI_0122.json', 'DJI_0122_Doenca_0.JPG', 'DJI_0122_Doenca_0.json', 'DJI_0122_Doenca_1.JPG', 'DJI_0122_Doenca_1.json', 'DJI_0122_Saudavel_2.JPG', 'DJI_0122_Saudavel_2.json', 'DJI_0122_Saudavel_3.JPG', 'DJI_0122_Saudavel_3.json', 'DJI_0122_Saudavel_4.JPG', 'DJI_0122_Saudavel_4.json', 'DJI_0122_Saudavel_5.JPG', 'DJI_0122_Saudavel_5.json', 'DJI_0122_Saudavel_6.JPG', 'DJI_0122_Saudavel_6.json', 'DJI_0122_Saudavel_7.JPG', 'DJI_0122_Saudavel_7.json', 'DJI_0122_Saudavel_8.JPG', 'DJI_0122_Saudavel_8.json', 'DJI_0122_Saudavel_9.JPG', 'DJI_0122_Saudavel_9.json', 'DJI_0122_Solo_10.JPG', 'DJI_0122_Solo_10.json', 'DJI_0160.JPG', 'DJI_0160.json', 'DJI_0160_Doenca_0.JPG', 'DJI_0160_Doenca_0.json', 'DJI_0160_Doenca_1.JPG', 'DJI_0160_Doenca_1.json', 'DJI_0160_Doenca_2.JPG', 'DJI_0160_Doenca_2.json', 'DJI_0160_Doenca_7.JPG', 'DJI_0160_Doenca_7.json', 'DJI_0160_Doenca_8.JPG', 'DJI_0160_Doenca_8.json', 'DJI_0160_Doenca_9.JPG', 'DJI_0160_Doenca_9.json', 'DJI_0160_Saudavel_10.JPG', 'DJI_0160_Saudavel_10.json', 'DJI_0160_Saudavel_11.JPG', 'DJI_0160_Saudavel_11.json', 'DJI_0160_Saudavel_12.JPG', 'DJI_0160_Saudavel_12.json', 'DJI_0160_Saudavel_13.JPG', 'DJI_0160_Saudavel_13.json', 'DJI_0160_Saudavel_3.JPG', 'DJI_0160_Saudavel_3.json', 'DJI_0160_Solo_4.JPG', 'DJI_0160_Solo_4.json', 'DJI_0160_Solo_5.JPG', 'DJI_0160_Solo_5.json', 'DJI_0160_Solo_6.JPG', 'DJI_0160_Solo_6.json', 'DJI_0161.JPG', 'DJI_0161.json', 'DJI_0161_Doenca_0.JPG', 'DJI_0161_Doenca_0.json', 'DJI_0161_Doenca_1.JPG', 'DJI_0161_Doenca_1.json', 'DJI_0161_Doenca_10.JPG', 'DJI_0161_Doenca_10.json', 'DJI_0161_Doenca_4.JPG', 'DJI_0161_Doenca_4.json', 'DJI_0161_Doenca_6.JPG', 'DJI_0161_Doenca_6.json', 'DJI_0161_Doenca_7.JPG', 'DJI_0161_Doenca_7.json', 'DJI_0161_Doenca_8.JPG', 'DJI_0161_Doenca_8.json', 'DJI_0161_Doenca_9.JPG', 'DJI_0161_Doenca_9.json', 'DJI_0161_Saudavel_11.JPG', 'DJI_0161_Saudavel_11.json', 'DJI_0161_Saudavel_12.JPG', 'DJI_0161_Saudavel_12.json', 'DJI_0161_Saudavel_13.JPG', 'DJI_0161_Saudavel_13.json', 'DJI_0161_Saudavel_2.JPG', 'DJI_0161_Saudavel_2.json', 'DJI_0161_Saudavel_3.JPG', 'DJI_0161_Saudavel_3.json', 'DJI_0161_Saudavel_5.JPG', 'DJI_0161_Saudavel_5.json', 'DJI_0161_Solo_14.JPG', 'DJI_0161_Solo_14.json', 'DJI_0161_Solo_15.JPG', 'DJI_0161_Solo_15.json', 'DJI_0165.JPG', 'DJI_0165.json', 'DJI_0165_Doenca_2.JPG', 'DJI_0165_Doenca_2.json', 'DJI_0165_Doenca_3.JPG', 'DJI_0165_Doenca_3.json', 'DJI_0165_Saudavel_10.JPG', 'DJI_0165_Saudavel_10.json', 'DJI_0165_Saudavel_11.JPG', 'DJI_0165_Saudavel_11.json', 'DJI_0165_Saudavel_4.JPG', 'DJI_0165_Saudavel_4.json', 'DJI_0165_Saudavel_5.JPG', 'DJI_0165_Saudavel_5.json', 'DJI_0165_Saudavel_6.JPG', 'DJI_0165_Saudavel_6.json', 'DJI_0165_Saudavel_7.JPG', 'DJI_0165_Saudavel_7.json', 'DJI_0165_Saudavel_8.JPG', 'DJI_0165_Saudavel_8.json', 'DJI_0165_Saudavel_9.JPG', 'DJI_0165_Saudavel_9.json', 'DJI_0165_Solo_0.JPG', 'DJI_0165_Solo_0.json', 'DJI_0165_Solo_1.JPG', 'DJI_0165_Solo_1.json', 'DJI_0211.JPG', 'DJI_0211.json', 'DJI_0211_Doenca_1.JPG', 'DJI_0211_Doenca_1.json', 'DJI_0211_Doenca_2.JPG', 'DJI_0211_Doenca_2.json', 'DJI_0211_Saudavel_3.JPG', 'DJI_0211_Saudavel_3.json', 'DJI_0211_Saudavel_4.JPG', 'DJI_0211_Saudavel_4.json', 'DJI_0211_Saudavel_5.JPG', 'DJI_0211_Saudavel_5.json', 'DJI_0211_Saudavel_6.JPG', 'DJI_0211_Saudavel_6.json', 'DJI_0211_Solo_0.JPG', 'DJI_0211_Solo_0.json', 'DJI_0214.JPG', 'DJI_0214.json', 'DJI_0214_Doenca_0.JPG', 'DJI_0214_Doenca_0.json', 'DJI_0214_Doenca_3.JPG', 'DJI_0214_Doenca_3.json', 'DJI_0214_Doenca_4.JPG', 'DJI_0214_Doenca_4.json', 'DJI_0214_Doenca_5.JPG', 'DJI_0214_Doenca_5.json', 'DJI_0214_Saudavel_6.JPG', 'DJI_0214_Saudavel_6.json', 'DJI_0214_Saudavel_7.JPG', 'DJI_0214_Saudavel_7.json', 'DJI_0214_Saudavel_8.JPG', 'DJI_0214_Saudavel_8.json', 'DJI_0214_Saudavel_9.JPG', 'DJI_0214_Saudavel_9.json', 'DJI_0214_Solo_1.JPG', 'DJI_0214_Solo_1.json', 'DJI_0214_Solo_2.JPG', 'DJI_0214_Solo_2.json', 'DJI_0217.JPG', 'DJI_0217.json', 'DJI_0217_Doenca_0.JPG', 'DJI_0217_Doenca_0.json', 'DJI_0217_Doenca_3.JPG', 'DJI_0217_Doenca_3.json', 'DJI_0217_Doenca_4.JPG', 'DJI_0217_Doenca_4.json', 'DJI_0217_Doenca_5.JPG', 'DJI_0217_Doenca_5.json', 'DJI_0217_Doenca_6.JPG', 'DJI_0217_Doenca_6.json', 'DJI_0217_Doenca_7.JPG', 'DJI_0217_Doenca_7.json', 'DJI_0217_Doenca_8.JPG', 'DJI_0217_Doenca_8.json', 'DJI_0217_Saudavel_2.JPG', 'DJI_0217_Saudavel_2.json', 'DJI_0217_Solo_1.JPG', 'DJI_0217_Solo_1.json', 'DJI_0233.JPG', 'DJI_0233.json', 'DJI_0233_Doenca_0.JPG', 'DJI_0233_Doenca_0.json', 'DJI_0233_Doenca_1.JPG', 'DJI_0233_Doenca_1.json', 'DJI_0233_Doenca_2.JPG', 'DJI_0233_Doenca_2.json', 'DJI_0233_Doenca_3.JPG', 'DJI_0233_Doenca_3.json', 'DJI_0233_Doenca_4.JPG', 'DJI_0233_Doenca_4.json', 'DJI_0233_Solo_5.JPG', 'DJI_0233_Solo_5.json', 'DJI_0234.JPG', 'DJI_0234.json', 'DJI_0234_Doenca_0.JPG', 'DJI_0234_Doenca_0.json', 'DJI_0234_Doenca_1.JPG', 'DJI_0234_Doenca_1.json', 'DJI_0234_Doenca_2.JPG', 'DJI_0234_Doenca_2.json', 'DJI_0234_Doenca_3.JPG', 'DJI_0234_Doenca_3.json', 'DJI_0234_Solo_4.JPG', 'DJI_0234_Solo_4.json', 'DJI_0237.JPG', 'DJI_0237.json', 'DJI_0237_Doenca_0.JPG', 'DJI_0237_Doenca_0.json', 'DJI_0237_Saudavel_1.JPG', 'DJI_0237_Saudavel_1.json', 'DJI_0237_Saudavel_2.JPG', 'DJI_0237_Saudavel_2.json', 'DJI_0237_Solo_3.JPG', 'DJI_0237_Solo_3.json', 'DJI_0252.JPG', 'DJI_0252.json', 'DJI_0252_Doenca_0.JPG', 'DJI_0252_Doenca_0.json', 'DJI_0252_Doenca_1.JPG', 'DJI_0252_Doenca_1.json', 'DJI_0252_Doenca_2.JPG', 'DJI_0252_Doenca_2.json', 'DJI_0252_Doenca_3.JPG', 'DJI_0252_Doenca_3.json', 'DJI_0252_Doenca_4.JPG', 'DJI_0252_Doenca_4.json', 'DJI_0252_Solo_5.JPG', 'DJI_0252_Solo_5.json', 'DJI_0402.JPG', 'DJI_0402.json', 'DJI_0402_Doenca_0.JPG', 'DJI_0402_Doenca_0.json', 'DJI_0402_Doenca_1.JPG', 'DJI_0402_Doenca_1.json', 'DJI_0402_Saudavel_2.JPG', 'DJI_0402_Saudavel_2.json', 'DJI_0402_Saudavel_3.JPG', 'DJI_0402_Saudavel_3.json', 'DJI_0402_Solo_4.JPG', 'DJI_0402_Solo_4.json', 'DJI_0402_Solo_5.JPG', 'DJI_0402_Solo_5.json', 'DJI_0402_Solo_6.JPG', 'DJI_0402_Solo_6.json', 'DJI_0432.JPG', 'DJI_0432.json', 'DJI_0432_Doenca_0.JPG', 'DJI_0432_Doenca_0.json', 'DJI_0432_Saudavel_1.JPG', 'DJI_0432_Saudavel_1.json', 'DJI_0432_Saudavel_2.JPG', 'DJI_0432_Saudavel_2.json', 'DJI_0432_Solo_3.JPG', 'DJI_0432_Solo_3.json', 'DJI_0432_Solo_4.JPG', 'DJI_0432_Solo_4.json', 'DJI_0432_Solo_5.JPG', 'DJI_0432_Solo_5.json', 'DJI_0432_Solo_6.JPG', 'DJI_0432_Solo_6.json', 'DJI_0432_Solo_7.JPG', 'DJI_0432_Solo_7.json', 'DJI_0433.JPG', 'DJI_0433.json', 'DJI_0433_Doenca_5.JPG', 'DJI_0433_Doenca_5.json', 'DJI_0433_Doenca_6.JPG', 'DJI_0433_Doenca_6.json', 'DJI_0433_Saudavel_7.JPG', 'DJI_0433_Saudavel_7.json', 'DJI_0433_Saudavel_8.JPG', 'DJI_0433_Saudavel_8.json', 'DJI_0433_Solo_0.JPG', 'DJI_0433_Solo_0.json', 'DJI_0433_Solo_1.JPG', 'DJI_0433_Solo_1.json', 'DJI_0433_Solo_2.JPG', 'DJI_0433_Solo_2.json', 'DJI_0433_Solo_3.JPG', 'DJI_0433_Solo_3.json', 'DJI_0433_Solo_4.JPG', 'DJI_0433_Solo_4.json', 'DJI_0435.JPG', 'DJI_0435.json', 'DJI_0435_Doenca_2.JPG', 'DJI_0435_Doenca_2.json', 'DJI_0435_Doenca_5.JPG', 'DJI_0435_Doenca_5.json', 'DJI_0435_Saudavel_3.JPG', 'DJI_0435_Saudavel_3.json', 'DJI_0435_Solo_0.JPG', 'DJI_0435_Solo_0.json', 'DJI_0435_Solo_1.JPG', 'DJI_0435_Solo_1.json', 'DJI_0435_Solo_4.JPG', 'DJI_0435_Solo_4.json', 'DJI_0444.JPG', 'DJI_0444.json', 'DJI_0444_Doenca_2.JPG', 'DJI_0444_Doenca_2.json', 'DJI_0444_Saudavel_3.JPG', 'DJI_0444_Saudavel_3.json', 'DJI_0444_Saudavel_4.JPG', 'DJI_0444_Saudavel_4.json', 'DJI_0444_Solo_0.JPG', 'DJI_0444_Solo_0.json', 'DJI_0444_Solo_1.JPG', 'DJI_0444_Solo_1.json']\n",
      "Número de amostras no dataset de validação: 89\n",
      "Arquivos no dataset de validação: ['DJI_0052.JPG', 'DJI_0052.json', 'DJI_0052_Doenca_0.JPG', 'DJI_0052_Doenca_0.json', 'DJI_0052_Saudavel_1.JPG', 'DJI_0052_Saudavel_1.json', 'DJI_0052_Saudavel_2.JPG', 'DJI_0052_Saudavel_2.json', 'DJI_0052_Saudavel_3.JPG', 'DJI_0052_Saudavel_3.json', 'DJI_0052_Saudavel_4.JPG', 'DJI_0052_Saudavel_4.json', 'DJI_0052_Saudavel_5.JPG', 'DJI_0052_Saudavel_5.json', 'DJI_0052_Solo_6.JPG', 'DJI_0052_Solo_6.json', 'DJI_0085.JPG', 'DJI_0085.json', 'DJI_0085_Doenca_0.JPG', 'DJI_0085_Doenca_0.json', 'DJI_0085_Doenca_1.JPG', 'DJI_0085_Doenca_1.json', 'DJI_0085_Saudavel_4.JPG', 'DJI_0085_Saudavel_4.json', 'DJI_0085_Saudavel_5.JPG', 'DJI_0085_Saudavel_5.json', 'DJI_0085_Saudavel_6.JPG', 'DJI_0085_Saudavel_6.json', 'DJI_0085_Saudavel_7.JPG', 'DJI_0085_Saudavel_7.json', 'DJI_0085_Saudavel_8.JPG', 'DJI_0085_Saudavel_8.json', 'DJI_0085_Solo_2.JPG', 'DJI_0085_Solo_2.json', 'DJI_0085_Solo_3.JPG', 'DJI_0085_Solo_3.json', 'DJI_0107.JPG', 'DJI_0107.json', 'DJI_0107_Doenca_0.JPG', 'DJI_0107_Doenca_0.json', 'DJI_0107_Doenca_1.JPG', 'DJI_0107_Doenca_1.json', 'DJI_0107_Saudavel_2.JPG', 'DJI_0107_Saudavel_2.json', 'DJI_0107_Saudavel_3.JPG', 'DJI_0107_Saudavel_3.json', 'DJI_0107_Saudavel_4.JPG', 'DJI_0107_Saudavel_4.json', 'DJI_0107_Saudavel_5.JPG', 'DJI_0107_Saudavel_5.json', 'DJI_0107_Saudavel_6.JPG', 'DJI_0107_Saudavel_6.json', 'DJI_0107_Saudavel_7.JPG', 'DJI_0107_Saudavel_7.json', 'DJI_0107_Saudavel_8.JPG', 'DJI_0107_Saudavel_8.json', 'DJI_0107_Saudavel_9.JPG', 'DJI_0107_Saudavel_9.json', 'DJI_0107_Solo_10.JPG', 'DJI_0107_Solo_10.json', 'DJI_0107_Solo_11.JPG', 'DJI_0107_Solo_11.json', 'DJI_0124.JPG', 'DJI_0124.json', 'DJI_0124_Doenca_0.JPG', 'DJI_0124_Doenca_0.json', 'DJI_0124_Doenca_1.JPG', 'DJI_0124_Doenca_1.json', 'DJI_0124_Doenca_2.JPG', 'DJI_0124_Doenca_2.json', 'DJI_0124_Doenca_3.JPG', 'DJI_0124_Doenca_3.json', 'DJI_0124_Saudavel_4.JPG', 'DJI_0124_Saudavel_4.json', 'DJI_0124_Saudavel_5.JPG', 'DJI_0124_Saudavel_5.json', 'DJI_0124_Saudavel_6.JPG', 'DJI_0124_Saudavel_6.json', 'DJI_0124_Saudavel_7.JPG', 'DJI_0124_Saudavel_7.json', 'DJI_0124_Saudavel_8.JPG', 'DJI_0124_Saudavel_8.json', 'DJI_0124_Solo_9.JPG', 'DJI_0124_Solo_9.json', 'DJI_0163.JPG', 'DJI_0163.json', 'DJI_0163_Doenca_0.JPG', 'DJI_0163_Doenca_0.json', 'DJI_0163_Doenca_1.JPG', 'DJI_0163_Doenca_1.json', 'DJI_0163_Saudavel_2.JPG', 'DJI_0163_Saudavel_2.json', 'DJI_0163_Saudavel_3.JPG', 'DJI_0163_Saudavel_3.json', 'DJI_0163_Saudavel_4.JPG', 'DJI_0163_Saudavel_4.json', 'DJI_0163_Saudavel_5.JPG', 'DJI_0163_Saudavel_5.json', 'DJI_0163_Saudavel_6.JPG', 'DJI_0163_Saudavel_6.json', 'DJI_0163_Saudavel_7.JPG', 'DJI_0163_Saudavel_7.json', 'DJI_0163_Saudavel_8.JPG', 'DJI_0163_Saudavel_8.json', 'DJI_0163_Solo_9.JPG', 'DJI_0163_Solo_9.json', 'DJI_0212.JPG', 'DJI_0212.json', 'DJI_0212_Doenca_0.JPG', 'DJI_0212_Doenca_0.json', 'DJI_0212_Doenca_1.JPG', 'DJI_0212_Doenca_1.json', 'DJI_0212_Saudavel_2.JPG', 'DJI_0212_Saudavel_2.json', 'DJI_0212_Saudavel_3.JPG', 'DJI_0212_Saudavel_3.json', 'DJI_0212_Saudavel_4.JPG', 'DJI_0212_Saudavel_4.json', 'DJI_0212_Solo_5.JPG', 'DJI_0212_Solo_5.json', 'DJI_0215.JPG', 'DJI_0215.json', 'DJI_0215_Doenca_0.JPG', 'DJI_0215_Doenca_0.json', 'DJI_0215_Doenca_1.JPG', 'DJI_0215_Doenca_1.json', 'DJI_0215_Doenca_2.JPG', 'DJI_0215_Doenca_2.json', 'DJI_0215_Doenca_3.JPG', 'DJI_0215_Doenca_3.json', 'DJI_0215_Doenca_4.JPG', 'DJI_0215_Doenca_4.json', 'DJI_0215_Doenca_5.JPG', 'DJI_0215_Doenca_5.json', 'DJI_0215_Solo_6.JPG', 'DJI_0215_Solo_6.json', 'DJI_0236.JPG', 'DJI_0236.json', 'DJI_0236_Doenca_0.JPG', 'DJI_0236_Doenca_0.json', 'DJI_0236_Doenca_1.JPG', 'DJI_0236_Doenca_1.json', 'DJI_0236_Saudavel_2.JPG', 'DJI_0236_Saudavel_2.json', 'DJI_0236_Saudavel_3.JPG', 'DJI_0236_Saudavel_3.json', 'DJI_0236_Saudavel_4.JPG', 'DJI_0236_Saudavel_4.json', 'DJI_0236_Solo_5.JPG', 'DJI_0236_Solo_5.json', 'DJI_0251.JPG', 'DJI_0251.json', 'DJI_0251_Doenca_0.JPG', 'DJI_0251_Doenca_0.json', 'DJI_0251_Doenca_1.JPG', 'DJI_0251_Doenca_1.json', 'DJI_0251_Doenca_2.JPG', 'DJI_0251_Doenca_2.json', 'DJI_0251_Doenca_3.JPG', 'DJI_0251_Doenca_3.json', 'DJI_0251_Solo_4.JPG', 'DJI_0251_Solo_4.json', 'DJI_0434.JPG', 'DJI_0434.json', 'DJI_0434_Doenca_2.JPG', 'DJI_0434_Doenca_2.json', 'DJI_0434_Doenca_3.JPG', 'DJI_0434_Doenca_3.json', 'DJI_0434_Saudavel_4.JPG', 'DJI_0434_Saudavel_4.json', 'DJI_0434_Saudavel_5.JPG', 'DJI_0434_Saudavel_5.json', 'DJI_0434_Solo_0.JPG', 'DJI_0434_Solo_0.json', 'DJI_0434_Solo_1.JPG', 'DJI_0434_Solo_1.json', 'DJI_0434_Solo_6.JPG', 'DJI_0434_Solo_6.json']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcjea\\AppData\\Local\\Temp\\ipykernel_9684\\2948098216.py:28: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding vgg0.0.weight to base vgg weight.\n",
      "Adding vgg0.0.bias to base vgg bias.\n",
      "Adding vgg0.2.weight to base vgg weight.\n",
      "Adding vgg0.2.bias to base vgg bias.\n",
      "Adding vgg1.1.weight to base vgg weight.\n",
      "Adding vgg1.1.bias to base vgg bias.\n",
      "Adding vgg1.3.weight to base vgg weight.\n",
      "Adding vgg1.3.bias to base vgg bias.\n",
      "Adding vgg2.1.weight to base vgg weight.\n",
      "Adding vgg2.1.bias to base vgg bias.\n",
      "Adding vgg2.3.weight to base vgg weight.\n",
      "Adding vgg2.3.bias to base vgg bias.\n",
      "Adding vgg2.5.weight to base vgg weight.\n",
      "Adding vgg2.5.bias to base vgg bias.\n",
      "Adding vgg3.1.weight to base vgg weight.\n",
      "Adding vgg3.1.bias to base vgg bias.\n",
      "Adding vgg3.3.weight to base vgg weight.\n",
      "Adding vgg3.3.bias to base vgg bias.\n",
      "Adding vgg3.5.weight to base vgg weight.\n",
      "Adding vgg3.5.bias to base vgg bias.\n",
      "Adding vgg4.1.weight to base vgg weight.\n",
      "Adding vgg4.1.bias to base vgg bias.\n",
      "Adding vgg4.3.weight to base vgg weight.\n",
      "Adding vgg4.3.bias to base vgg bias.\n",
      "Adding vgg4.5.weight to base vgg weight.\n",
      "Adding vgg4.5.bias to base vgg bias.\n",
      "Adding smooth0.0.weight to core weight.\n",
      "Adding smooth0.0.bias to core bias.\n",
      "Adding smooth0.2.weight to core weight.\n",
      "Adding smooth0.2.bias to core bias.\n",
      "Adding smooth1.0.weight to core weight.\n",
      "Adding smooth1.0.bias to core bias.\n",
      "Adding smooth1.2.weight to core weight.\n",
      "Adding smooth1.2.bias to core bias.\n",
      "Adding smooth2.0.weight to core weight.\n",
      "Adding smooth2.0.bias to core bias.\n",
      "Adding smooth2.2.weight to core weight.\n",
      "Adding smooth2.2.bias to core bias.\n",
      "Adding smooth3.0.weight to core weight.\n",
      "Adding smooth3.0.bias to core bias.\n",
      "Adding smooth3.2.weight to core weight.\n",
      "Adding smooth3.2.bias to core bias.\n",
      "Adding final.weight to core weight.\n",
      "Adding final.bias to core bias.\n",
      "Epoch 1 starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\image-segmentation\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.109053, train acc: 0.362459\n",
      "Validação Acc: 0.334031 -- Melhor Avaliação Acc: 0.334031 -- epoch 0.\n",
      "Epoch 2 starting...\n",
      "Train loss: 1.107116, train acc: 0.321676\n",
      "Validação Acc: 0.324762 -- Melhor Avaliação Acc: 0.334031 -- epoch 0.\n",
      "Epoch 3 starting...\n",
      "Train loss: 1.096855, train acc: 0.335259\n",
      "Validação Acc: 0.328341 -- Melhor Avaliação Acc: 0.334031 -- epoch 0.\n",
      "Epoch 4 starting...\n",
      "Train loss: 1.093008, train acc: 0.347690\n",
      "Validação Acc: 0.445724 -- Melhor Avaliação Acc: 0.445724 -- epoch 3.\n",
      "Epoch 5 starting...\n",
      "Train loss: 1.111640, train acc: 0.317671\n",
      "Validação Acc: 0.330133 -- Melhor Avaliação Acc: 0.445724 -- epoch 3.\n",
      "Epoch 6 starting...\n",
      "Train loss: 1.082842, train acc: 0.336832\n",
      "Validação Acc: 0.327184 -- Melhor Avaliação Acc: 0.445724 -- epoch 3.\n",
      "Epoch 7 starting...\n",
      "Train loss: 1.073668, train acc: 0.321982\n",
      "Validação Acc: 0.445182 -- Melhor Avaliação Acc: 0.445724 -- epoch 3.\n",
      "Epoch 8 starting...\n",
      "Train loss: 1.080780, train acc: 0.327436\n",
      "Validação Acc: 0.449138 -- Melhor Avaliação Acc: 0.449138 -- epoch 7.\n",
      "Epoch 9 starting...\n",
      "Train loss: 1.060269, train acc: 0.355155\n",
      "Validação Acc: 0.327297 -- Melhor Avaliação Acc: 0.449138 -- epoch 7.\n",
      "Epoch 10 starting...\n",
      "Train loss: 1.059207, train acc: 0.375618\n",
      "Validação Acc: 0.430921 -- Melhor Avaliação Acc: 0.449138 -- epoch 7.\n",
      "Epoch 11 starting...\n",
      "Train loss: 1.076520, train acc: 0.348625\n",
      "Validação Acc: 0.327075 -- Melhor Avaliação Acc: 0.449138 -- epoch 7.\n",
      "Epoch 12 starting...\n",
      "Train loss: 1.070877, train acc: 0.355926\n",
      "Validação Acc: 0.329026 -- Melhor Avaliação Acc: 0.449138 -- epoch 7.\n",
      "Epoch 13 starting...\n",
      "Train loss: 1.070144, train acc: 0.359351\n",
      "Validação Acc: 0.327066 -- Melhor Avaliação Acc: 0.449138 -- epoch 7.\n",
      "Epoch 14 starting...\n"
     ]
    }
   ],
   "source": [
    "# Inicializar listas para armazenar a perda e a precisão\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Inicia o treinamento\n",
    "train_dataset = SegmentationDataset(img_folder_train, img_folder_train, True, class_to_id, resolution_input, True, None)\n",
    "print(f\"Número de amostras no dataset de treinamento: {len(train_dataset)}\")\n",
    "print(f\"Arquivos no dataset de treinamento: {os.listdir(img_folder_train)}\")\n",
    "\n",
    "val_dataset = SegmentationDataset(img_folder_val, img_folder_val, False, class_to_id, resolution_input, False, None)\n",
    "print(f\"Número de amostras no dataset de validação: {len(val_dataset)}\")\n",
    "print(f\"Arquivos no dataset de validação: {os.listdir(img_folder_val)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=0, drop_last=False)\n",
    "\n",
    "if plot_train:\n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "        image_np = np.squeeze(sample_batched['image_original'].cpu().numpy())\n",
    "        gt = np.squeeze(sample_batched['gt'].cpu().numpy())\n",
    "        \n",
    "        color_label = np.zeros((resolution_input[1], resolution_input[0], 3))\n",
    "        \n",
    "        for key, val in id_to_class.items():\n",
    "            color_label[gt == key] = class_to_color.get(val, [0, 0, 0])  # Provide a default color if key is missing\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.imshow((image_np / 255) * 0.5 + (color_label / 255) * 0.5)\n",
    "        # print(f\"Imagem de Treinamento {i_batch}\")\n",
    "        plt.savefig(img_folder_train_segmentadas + \"IMG_\" + str(i_batch) + \".png\")\n",
    "        if plt_show: plt.show()\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.imshow(color_label.astype(np.uint8))\n",
    "        # print(f\"Imagem de Treinamento {i_batch} - Segmentada\")\n",
    "        plt.savefig(img_folder_train_segmentadas + \"GT_\" + str(i_batch) + \".png\")\n",
    "        if plt_show: plt.show()\n",
    "\n",
    "model = UNetVgg(nClasses).to(device)\n",
    "\n",
    "core_lr = 0.02\n",
    "base_vgg_weight, base_vgg_bias, core_weight, core_bias = UNetVgg.get_params_by_kind(model, 7)\n",
    "\n",
    "optimizer = torch.optim.SGD([{'params': base_vgg_bias, 'lr': 0.000001}, \n",
    "                             {'params': base_vgg_weight, 'lr': 0.000001},\n",
    "                             {'params': core_bias, 'lr': core_lr},\n",
    "                             {'params': core_weight, 'lr': core_lr, 'weight_decay': 0.0005}], momentum=0.9)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 30, gamma=0.2)\n",
    "\n",
    "best_val_acc = -1\n",
    "best_epoch = 0\n",
    "\n",
    "# Start training...\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    print('Epoch %d starting...' % (epoch+1))\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    mean_loss = 0.0\n",
    "    \n",
    "    n_correct = 0\n",
    "    n_false = 0\n",
    "    \n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "    \n",
    "    \n",
    "        image = sample_batched['image'].to(device)\n",
    "        gt = sample_batched['gt'].to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        output, total_loss = model.eval_net_with_loss(model, image, gt, class_weights, device)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        mean_loss += total_loss.cpu().detach().numpy()\n",
    "        \n",
    "        # Measure accuracy\n",
    "        \n",
    "        gt = np.squeeze(sample_batched['gt'].cpu().numpy())\n",
    "        \n",
    "        label_out = torch.nn.functional.softmax(output, dim = 1)\n",
    "        label_out = label_out.cpu().detach().numpy()\n",
    "        label_out = np.squeeze(label_out)\n",
    "        \n",
    "        labels = np.argmax(label_out, axis=0)\n",
    "        valid_mask = gt != -1\n",
    "        curr_correct = np.sum(gt[valid_mask] == labels[valid_mask])\n",
    "        curr_false = np.sum(valid_mask) - curr_correct\n",
    "        n_correct += curr_correct\n",
    "        n_false += curr_false\n",
    "        \n",
    "    mean_loss /= len(train_loader)\n",
    "    train_acc = n_correct / (n_correct + n_false)\n",
    "        \n",
    "    print('Train loss: %f, train acc: %f' % (mean_loss, train_acc))\n",
    "    # Armazenar a perda e a precisão de treinamento\n",
    "    train_losses.append(mean_loss)\n",
    "    train_accuracies.append(train_acc)    \n",
    "    \n",
    "    n_correct = 0\n",
    "    n_false = 0\n",
    "    \n",
    "    \n",
    "    for i_batch, sample_batched in enumerate(val_loader):\n",
    "    \n",
    "    \n",
    "        image = sample_batched['image'].to(device)\n",
    "        image_np = np.squeeze(sample_batched['image_original'].cpu().numpy())\n",
    "        gt = np.squeeze(sample_batched['gt'].cpu().numpy())\n",
    "        \n",
    "    \n",
    "        label_out = model(image)\n",
    "        label_out = torch.nn.functional.softmax(label_out, dim = 1)\n",
    "        label_out = label_out.cpu().detach().numpy()\n",
    "        label_out = np.squeeze(label_out)\n",
    "        \n",
    "        labels = np.argmax(label_out, axis=0)\n",
    "        \n",
    "        if plot_val:\n",
    "            \n",
    "            color_label = np.zeros((resolution_input[1], resolution_input[0], 3))\n",
    "            \n",
    "            for key, val in id_to_class.items():\n",
    "                color_label[labels == key] = class_to_color[val]\n",
    "                \n",
    "            plt.figure()\n",
    "            plt.imshow((image_np/255) * 0.5 + (color_label/255) * 0.5)\n",
    "            if plt_savefig: plt.savefig(img_folder_val_segmentadas + \"IMG_\" + str(i_batch) + \"_epoch_\" + str(epoch) + \".png\")\n",
    "            if plt_show: plt.show()\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.imshow(color_label.astype(np.uint8))\n",
    "            if plt_savefig: plt.savefig(img_folder_val_segmentadas + \"GT_\" + str(i_batch) + \"_epoch_\" + str(epoch) +  \".png\")\n",
    "            if plt_show: plt.show()\n",
    "        \n",
    "        valid_mask = gt != -1\n",
    "        curr_correct = np.sum(gt[valid_mask] == labels[valid_mask])\n",
    "        curr_false = np.sum(valid_mask) - curr_correct\n",
    "        n_correct += curr_correct\n",
    "        n_false += curr_false    \n",
    "        \n",
    "    total_acc = n_correct / (n_correct + n_false)\n",
    "    val_accuracies.append(total_acc)\n",
    "    \n",
    "    if best_val_acc < total_acc:\n",
    "        best_val_acc = total_acc\n",
    "        if epoch > 7:\n",
    "            torch.save(model.state_dict(), model_file_name)\n",
    "            print('Nova melhor conta de validação. Salvo... %f', epoch)\n",
    "        best_epoch = epoch\n",
    "\n",
    "    if (epoch - best_epoch) > patience:\n",
    "        print(f\"Terminando o treinamento, melhor conta de validação {best_val_acc:.6f}\")\n",
    "        break\n",
    "    \n",
    "    print('Validação Acc: %f -- Melhor Avaliação Acc: %f -- epoch %d.' % (total_acc, best_val_acc, best_epoch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotar os gráficos de perda e precisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar os gráficos de perda e precisão\n",
    "\"\"\"\n",
    "    Inicialização das listas: train_losses, train_accuracies e val_accuracies são listas para armazenar a perda e a precisão de treinamento e validação em cada época.\n",
    "    \n",
    "    Armazenamento dos valores: Durante o loop de treinamento, a perda e a precisão são calculadas e armazenadas nas listas correspondentes.\n",
    "    Plotagem dos gráficos: Após o loop de treinamento, os gráficos de perda e precisão são plotados usando matplotlib.\n",
    "\n",
    "    Este código deve ser adicionado ao final do seu loop de treinamento no notebook para visualizar os resultados do treinamento ao longo das épocas.\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Perda de Treinamento')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Perda')\n",
    "plt.title('Perda de Treinamento ao longo das Épocas')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Precisão de Treinamento')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Precisão de Validação')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Precisão')\n",
    "plt.title('Precisão de Treinamento e Validação ao longo das Épocas')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "## Salve o gráfico em um diretorio de resultados \"save_dir\"\n",
    "plt.savefig(save_dir + 'result_model_segmentadas_unet_loss_accuracy.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferência de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configurações do treinamento\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "model = UNetVgg(nClasses)\n",
    "#model.load_state_dict(torch.load(model_file_name))\n",
    "model.load_state_dict(torch.load(model_file_name, weights_only=True))\n",
    "model.eval()\n",
    "print(\"Modelo carregado e pronto para uso.\")\n",
    "model.to(device)\n",
    "\n",
    "img_list = glob.glob(osp.join(img_folder_val, '*.JPG'))\n",
    "print(f\"Imagens de teste: {len(img_list)}\")\n",
    "\n",
    "for img_path in img_list:\n",
    "    img_np = cv2.imread(img_path, cv2.IMREAD_IGNORE_ORIENTATION + cv2.IMREAD_COLOR)\n",
    "    img_np = cv2.resize(img_np, (resolution_input[0], resolution_input[1]))[..., ::-1]\n",
    "    img_np = np.ascontiguousarray(img_np)\n",
    "    \n",
    "    img_pt = np.copy(img_np).astype(np.float32) / 255.0\n",
    "    for i in range(3):\n",
    "        img_pt[..., i] -= mean[i]\n",
    "        img_pt[..., i] /= std[i]\n",
    "\n",
    "    img_pt = img_pt.transpose(2, 0, 1)\n",
    "    img_pt = torch.from_numpy(img_pt[None, ...]).to(device)\n",
    "\n",
    "    label_out = model(img_pt)\n",
    "    label_out = torch.nn.functional.softmax(label_out, dim=1)\n",
    "    label_out = label_out.cpu().detach().numpy()\n",
    "    label_out = np.squeeze(label_out)\n",
    "\n",
    "    labels = np.argmax(label_out, axis=0)\n",
    "\n",
    "    color_label = np.zeros((resolution_input[1], resolution_input[0], 3))\n",
    "\n",
    "    for key, val in id_to_class.items():\n",
    "        color_label[labels == key] = class_to_color[val]\n",
    "        \n",
    "    final_image = osp.basename(img_path)\n",
    "    final_image = osp.splitext(final_image)[0]\n",
    "    final_image = osp.join(img_folder_test_segmentadas, final_image)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow((img_np / 255) * 0.5 + (color_label / 255) * 0.5)\n",
    "    if plt_savefig: plt.savefig(final_image + \"IMG_\" + \".png\")\n",
    "    plt.show()\n",
    "    plt.close('all')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(color_label.astype(np.uint8))\n",
    "    if plt_savefig: plt.savefig(final_image + \"GT_\" + \".png\")\n",
    "    plt.show()\n",
    "    plt.close('all')        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
