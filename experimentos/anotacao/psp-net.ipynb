{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import os.path as osp\n",
    "import glob\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição da Classe PSPDec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSPDec(torch.nn.Module):\n",
    "\tdef __init__(self, in_dim, reduction_dim, setting):\n",
    "\t\tsuper(PSPDec, self).__init__()\n",
    "\t\tself.features = []\n",
    "\t\tfor s in setting:\n",
    "\t\t\tself.features.append(torch.nn.Sequential(\n",
    "\t\t\t\ttorch.nn.AdaptiveAvgPool2d(s),\n",
    "\t\t\t\ttorch.nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),\n",
    "\t\t\t\t#torch.nn.BatchNorm2d(reduction_dim, momentum=.95),\n",
    "\t\t\t\t#torch.nn.InstanceNorm2d(reduction_dim, momentum=.95),\n",
    "\t\t\t\ttorch.nn.ReLU(inplace=True)\n",
    "\t\t\t))\n",
    "\t\tself.features = torch.nn.ModuleList(self.features)\n",
    "\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx_size = x.size()\n",
    "\t\tout = [x]\n",
    "\t\tfor f in self.features:\n",
    "\t\t\tout.append(torch.nn.functional.upsample(f(x), x_size[2:], mode='bilinear'))\n",
    "\t\tout = torch.cat(out, 1)\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição da classe PSPNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSPNet(torch.nn.Module):\n",
    "\n",
    "\tdef __init__(self, num_classes):\n",
    "\t\tsuper(PSPNet, self).__init__()\n",
    "\n",
    "\t\tresnet = torchvision.models.resnet101(pretrained=True)\n",
    "\t\t#print('resnet', resnet)\n",
    "\n",
    "\t\tself.layer0 = torch.nn.Sequential(\n",
    "\t\t\tresnet.conv1,\n",
    "\t\t\tresnet.bn1,\n",
    "\t\t\tresnet.relu,\n",
    "\t\t\tresnet.maxpool\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tself.layer1 = resnet.layer1\n",
    "\t\tself.layer2 = resnet.layer2\n",
    "\t\tself.layer3 = resnet.layer3\n",
    "\t\tself.layer4 = resnet.layer4\n",
    "\t\t\n",
    "\n",
    "\t\tfor n, m in self.layer3.named_modules():\n",
    "\t\t\tif 'conv2' in n:\n",
    "\t\t\t\tm.dilation, m.padding, m.stride = (2, 2), (2, 2), (1, 1)\n",
    "\t\t\telif 'downsample.0' in n:\n",
    "\t\t\t\tm.stride = (1, 1)\n",
    "\t\tfor n, m in self.layer4.named_modules():\n",
    "\t\t\tif 'conv2' in n:\n",
    "\t\t\t\tm.dilation, m.padding, m.stride = (4, 4), (4, 4), (1, 1)\n",
    "\t\t\telif 'downsample.0' in n:\n",
    "\t\t\t\tm.stride = (1, 1)\n",
    "\n",
    "\n",
    "\t\tself.ppm = PSPDec(2048, 512, (1, 2, 3, 6))\n",
    "\n",
    "\t\tself.final = torch.nn.Sequential(\n",
    "\t\t\ttorch.nn.Conv2d(4096, 512, 3, padding=1, bias=False),\n",
    "\t\t\ttorch.nn.BatchNorm2d(512, momentum=.95),\n",
    "\t\t\ttorch.nn.ReLU(inplace=True),\n",
    "\t\t\ttorch.nn.Dropout(.1),\n",
    "\t\t\ttorch.nn.Conv2d(512, num_classes, 1),\n",
    "\t\t)\n",
    "\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.layer0(x)\n",
    "\t\tx = self.layer1(x)\n",
    "\t\tx = self.layer2(x)\n",
    "\t\tx = self.layer3(x)\n",
    "\t\tx = self.layer4(x)\n",
    "\t\t\n",
    "\t\tx = self.ppm(x)\n",
    "\t\tx = self.final(x)\n",
    "\t\treturn torch.nn.functional.upsample(x, (480, 640), mode='bilinear')\n",
    "\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef eval_net_with_loss(model, inp, gt, class_weights, device):\n",
    "\n",
    "\t\tweights = torch.from_numpy(np.array(class_weights, dtype=np.float32)).to(device)\n",
    "\t\tout = model(inp)\n",
    "\n",
    "\t\tsoftmax = torch.nn.functional.log_softmax(out, dim = 1)\n",
    "\t\tloss = torch.nn.functional.nll_loss(softmax, gt, ignore_index=-1, weight=weights)\n",
    "\n",
    "\t\treturn (out, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declarações Variavies e outras configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponível: True\n",
      "Nome da GPU: NVIDIA GeForce RTX 4070\n",
      "VRAM Total: 11.99 GB\n",
      "Diretório do Projeto c:\\git\\image-segmentation\\dataset.\n"
     ]
    }
   ],
   "source": [
    "# Variável que define se as figuras são exibidas no console ou salvas em um arquivo\n",
    "plt_show = False\n",
    "plt_savefig = False\n",
    "\n",
    "# Configuração do dispositivo CUDA\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f'CUDA disponível: {cuda_available}')\n",
    "\n",
    "if cuda_available:\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_total = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # Convertendo para GB\n",
    "    vram_available = torch.cuda.memory_reserved(0) / (1024 ** 3)  # Convertendo para GB\n",
    "    print(f'Nome da GPU: {gpu_name}')\n",
    "    print(f'VRAM Total: {vram_total:.2f} GB')\n",
    "\n",
    "# Caminho do diretório Dataset\n",
    "directory = os.path.abspath(os.path.join(os.getcwd(), '..\\\\..')) + r'\\dataset'\n",
    "print(f'Diretório do Projeto {directory}.')\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "img_folder_val = directory + r'\\\\base\\\\Val'\n",
    "img_folder_train = directory + r'\\\\base\\\\Train'\n",
    "img_folder_test = directory + r'\\\\base\\\\Test'\n",
    "save_dir = directory + r'\\\\result_PSPNET\\\\'\n",
    "if not os.path.exists(img_folder_val):\n",
    "    os.makedirs(img_folder_val)\n",
    "if not os.path.exists(img_folder_train):\n",
    "    os.makedirs(img_folder_train)\n",
    "if not os.path.exists(img_folder_test):\n",
    "    os.makedirs(img_folder_test)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "## Imagens Segmentadas\n",
    "img_folder_train_segmentadas = directory + r'\\\\segmentadas\\\\train\\\\'\n",
    "img_folder_val_segmentadas = directory + r'\\\\segmentadas\\\\val\\\\'\n",
    "img_folder_test_segmentadas = directory + r'\\\\segmentadas\\\\test\\\\'\n",
    "if not os.path.exists(img_folder_train_segmentadas):\n",
    "    os.makedirs(img_folder_train_segmentadas)\n",
    "if not os.path.exists(img_folder_val_segmentadas):\n",
    "    os.makedirs(img_folder_val_segmentadas)\n",
    "if not os.path.exists(img_folder_test_segmentadas):\n",
    "    os.makedirs(img_folder_test_segmentadas)\n",
    "    \n",
    "# Local onde o Modelo será salvo\n",
    "model_file_name = save_dir + 'model_PSPNET.pth'\n",
    "\n",
    "# Configurações do treinamento\n",
    "resolution_input = (640, 480)  # Tamanho de entrada\n",
    "patience = 30\n",
    "plot_val = True\n",
    "plot_train = True\n",
    "max_epochs = 300\n",
    "class_weights = [1, 1, 1]\n",
    "nClasses = 3\n",
    "\n",
    "# Mapeamento de classes e cores\n",
    "class_to_color = {'Doenca': (255, 0, 0), 'Solo': (0, 0, 255), 'Saudavel': (0, 255, 255)}\n",
    "class_to_id = {'Doenca': 0, 'Solo': 1, 'Saudavel': 2}\n",
    "id_to_class = {v: k for k, v in class_to_id.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição do Dataset\n",
    "class SegmentationDataset(Dataset):\n",
    "    \"\"\"Segmentation dataset loader.\"\"\"\n",
    "\n",
    "    def __init__(self, json_folder, img_folder, is_train, class_to_id, resolution_input=(640, 480), augmentation=False, transform=None):\n",
    "        self.gt_file_list = glob.glob(osp.join(json_folder, '*.json'))\n",
    "        self.total_samples = len(self.gt_file_list)\n",
    "        self.img_folder = img_folder\n",
    "        self.is_train = is_train\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "        self.resolution = resolution_input\n",
    "        self.class_to_id = class_to_id\n",
    "        self.mean = [0.485, 0.456, 0.406]\n",
    "        self.std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gt_file = self.gt_file_list[idx]\n",
    "        img_number_str = osp.splitext(osp.basename(gt_file))[0]\n",
    "        \n",
    "        # Verificação de existência de arquivos\n",
    "        if not osp.exists(gt_file):\n",
    "            raise FileNotFoundError(f\"Arquivo JSON não encontrado: {gt_file}\")\n",
    "        \n",
    "        # Extrair o nome da imagem considerando que o nome da classe está no meio do nome do arquivo\n",
    "        img_path = osp.join(self.img_folder, img_number_str + '.JPG')\n",
    "        \n",
    "        if not osp.exists(img_path):\n",
    "            raise FileNotFoundError(f\"Imagem não encontrada: {img_path}\")\n",
    "        \n",
    "        gt_json = json.load(open(gt_file, 'r'))\n",
    "        img_np = cv2.imread(img_path, cv2.IMREAD_IGNORE_ORIENTATION + cv2.IMREAD_COLOR)\n",
    "        \n",
    "        if img_np is None:\n",
    "            raise FileNotFoundError(f\"Imagem não encontrada: {img_path}\")\n",
    "        \n",
    "        original_shape = img_np.shape\n",
    "        img_np = cv2.resize(img_np, (self.resolution[0], self.resolution[1]))[..., ::-1]\n",
    "        img_np = np.ascontiguousarray(img_np)\n",
    "        label_np = np.zeros((img_np.shape[0], img_np.shape[1]))\n",
    "        label_np[...] = -1\n",
    "\n",
    "        for shape in gt_json['shapes']:\n",
    "            points_np = np.array(shape['points'], dtype=np.float64)\n",
    "            points_np[:, 0] *= self.resolution[0] / original_shape[1]\n",
    "            points_np[:, 1] *= self.resolution[1] / original_shape[0]\n",
    "            points_np = np.round(points_np).astype(np.int64)\n",
    "            points_np = points_np.reshape((-1, 1, 2))\n",
    "            label = shape['label']\n",
    "            if label not in self.class_to_id:\n",
    "                raise KeyError(f\"Label '{label}' não encontrado em class_to_id\")\n",
    "            label_np = cv2.fillPoly(label_np, [points_np], self.class_to_id[label])\n",
    "\n",
    "        label_np = label_np.astype(np.int32)\n",
    "\n",
    "        if self.is_train and self.augmentation:\n",
    "            if np.random.rand() > 0.5:\n",
    "                img_np = np.fliplr(img_np)\n",
    "                label_np = np.fliplr(label_np)\n",
    "                img_np = np.ascontiguousarray(img_np)\n",
    "                label_np = np.ascontiguousarray(label_np)\n",
    "\n",
    "        img_pt = img_np.astype(np.float32) / 255.0\n",
    "        for i in range(3):\n",
    "            img_pt[..., i] -= self.mean[i]\n",
    "            img_pt[..., i] /= self.std[i]\n",
    "\n",
    "        img_pt = img_pt.transpose(2, 0, 1)\n",
    "        img_pt = torch.from_numpy(img_pt)\n",
    "        label_pt = torch.from_numpy(label_np).long()\n",
    "\n",
    "        sample = {'image': img_pt, 'gt': label_pt, 'image_original': img_np}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de amostras no dataset de treinamento: 20\n",
      "Arquivos no dataset de treinamento: ['DJI_0026.JPG', 'DJI_0026.json', 'DJI_0064.JPG', 'DJI_0064.json', 'DJI_0089.JPG', 'DJI_0089.json', 'DJI_0105.JPG', 'DJI_0105.json', 'DJI_0122.JPG', 'DJI_0122.json', 'DJI_0160.JPG', 'DJI_0160.json', 'DJI_0161.JPG', 'DJI_0161.json', 'DJI_0165.JPG', 'DJI_0165.json', 'DJI_0211.JPG', 'DJI_0211.json', 'DJI_0214.JPG', 'DJI_0214.json', 'DJI_0217.JPG', 'DJI_0217.json', 'DJI_0233.JPG', 'DJI_0233.json', 'DJI_0234.JPG', 'DJI_0234.json', 'DJI_0237.JPG', 'DJI_0237.json', 'DJI_0252.JPG', 'DJI_0252.json', 'DJI_0402.JPG', 'DJI_0402.json', 'DJI_0432.JPG', 'DJI_0432.json', 'DJI_0433.JPG', 'DJI_0433.json', 'DJI_0435.JPG', 'DJI_0435.json', 'DJI_0444.JPG', 'DJI_0444.json']\n",
      "Número de amostras no dataset de validação: 10\n",
      "Arquivos no dataset de validação: ['DJI_0052.JPG', 'DJI_0052.json', 'DJI_0085.JPG', 'DJI_0085.json', 'DJI_0107.JPG', 'DJI_0107.json', 'DJI_0124.JPG', 'DJI_0124.json', 'DJI_0163.JPG', 'DJI_0163.json', 'DJI_0212.JPG', 'DJI_0212.json', 'DJI_0215.JPG', 'DJI_0215.json', 'DJI_0236.JPG', 'DJI_0236.json', 'DJI_0251.JPG', 'DJI_0251.json', 'DJI_0434.JPG', 'DJI_0434.json']\n",
      "Adding vgg0.0.weight to base vgg weight.\n",
      "Adding vgg0.0.bias to base vgg bias.\n",
      "Adding vgg0.2.weight to base vgg weight.\n",
      "Adding vgg0.2.bias to base vgg bias.\n",
      "Adding vgg1.1.weight to base vgg weight.\n",
      "Adding vgg1.1.bias to base vgg bias.\n",
      "Adding vgg1.3.weight to base vgg weight.\n",
      "Adding vgg1.3.bias to base vgg bias.\n",
      "Adding vgg2.1.weight to base vgg weight.\n",
      "Adding vgg2.1.bias to base vgg bias.\n",
      "Adding vgg2.3.weight to base vgg weight.\n",
      "Adding vgg2.3.bias to base vgg bias.\n",
      "Adding vgg2.5.weight to base vgg weight.\n",
      "Adding vgg2.5.bias to base vgg bias.\n",
      "Adding vgg3.1.weight to base vgg weight.\n",
      "Adding vgg3.1.bias to base vgg bias.\n",
      "Adding vgg3.3.weight to base vgg weight.\n",
      "Adding vgg3.3.bias to base vgg bias.\n",
      "Adding vgg3.5.weight to base vgg weight.\n",
      "Adding vgg3.5.bias to base vgg bias.\n",
      "Adding vgg4.1.weight to base vgg weight.\n",
      "Adding vgg4.1.bias to base vgg bias.\n",
      "Adding vgg4.3.weight to base vgg weight.\n",
      "Adding vgg4.3.bias to base vgg bias.\n",
      "Adding vgg4.5.weight to base vgg weight.\n",
      "Adding vgg4.5.bias to base vgg bias.\n",
      "Adding smooth0.0.weight to core weight.\n",
      "Adding smooth0.0.bias to core bias.\n",
      "Adding smooth0.2.weight to core weight.\n",
      "Adding smooth0.2.bias to core bias.\n",
      "Adding smooth1.0.weight to core weight.\n",
      "Adding smooth1.0.bias to core bias.\n",
      "Adding smooth1.2.weight to core weight.\n",
      "Adding smooth1.2.bias to core bias.\n",
      "Adding smooth2.0.weight to core weight.\n",
      "Adding smooth2.0.bias to core bias.\n",
      "Adding smooth2.2.weight to core weight.\n",
      "Adding smooth2.2.bias to core bias.\n",
      "Adding smooth3.0.weight to core weight.\n",
      "Adding smooth3.0.bias to core bias.\n",
      "Adding smooth3.2.weight to core weight.\n",
      "Adding smooth3.2.bias to core bias.\n",
      "Adding final.weight to core weight.\n",
      "Adding final.bias to core bias.\n",
      "Epoch 1 starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\image-segmentation\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.935652, train acc: 0.599737\n",
      "Validação Acc: 0.763460 -- Melhor Avaliação Acc: 0.763460 -- epoch 0.\n",
      "Epoch 2 starting...\n",
      "Train loss: 0.622584, train acc: 0.738125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcjea\\AppData\\Local\\Temp\\ipykernel_18100\\3053091084.py:134: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validação Acc: 0.750849 -- Melhor Avaliação Acc: 0.763460 -- epoch 0.\n",
      "Epoch 3 starting...\n",
      "Train loss: 0.557653, train acc: 0.757237\n",
      "Validação Acc: 0.782876 -- Melhor Avaliação Acc: 0.782876 -- epoch 2.\n",
      "Epoch 4 starting...\n",
      "Train loss: 0.530104, train acc: 0.770727\n",
      "Validação Acc: 0.783808 -- Melhor Avaliação Acc: 0.783808 -- epoch 3.\n",
      "Epoch 5 starting...\n",
      "Train loss: 0.534554, train acc: 0.759090\n",
      "Validação Acc: 0.687441 -- Melhor Avaliação Acc: 0.783808 -- epoch 3.\n",
      "Epoch 6 starting...\n",
      "Train loss: 0.574654, train acc: 0.748291\n",
      "Validação Acc: 0.790627 -- Melhor Avaliação Acc: 0.790627 -- epoch 5.\n",
      "Epoch 7 starting...\n",
      "Train loss: 0.475852, train acc: 0.762264\n",
      "Validação Acc: 0.787272 -- Melhor Avaliação Acc: 0.790627 -- epoch 5.\n",
      "Epoch 8 starting...\n",
      "Train loss: 0.483752, train acc: 0.779782\n",
      "Validação Acc: 0.797093 -- Melhor Avaliação Acc: 0.797093 -- epoch 7.\n",
      "Epoch 9 starting...\n",
      "Train loss: 0.474288, train acc: 0.777572\n",
      "Validação Acc: 0.753025 -- Melhor Avaliação Acc: 0.797093 -- epoch 7.\n",
      "Epoch 10 starting...\n",
      "Train loss: 0.497390, train acc: 0.759721\n",
      "Validação Acc: 0.789421 -- Melhor Avaliação Acc: 0.797093 -- epoch 7.\n",
      "Epoch 11 starting...\n",
      "Train loss: 0.475819, train acc: 0.783548\n",
      "Nova melhor conta de validação. Salvo... %f 10\n",
      "Validação Acc: 0.797749 -- Melhor Avaliação Acc: 0.797749 -- epoch 10.\n",
      "Epoch 12 starting...\n",
      "Train loss: 0.437625, train acc: 0.786376\n",
      "Nova melhor conta de validação. Salvo... %f 11\n",
      "Validação Acc: 0.799304 -- Melhor Avaliação Acc: 0.799304 -- epoch 11.\n",
      "Epoch 13 starting...\n",
      "Train loss: 0.443944, train acc: 0.797152\n",
      "Validação Acc: 0.775233 -- Melhor Avaliação Acc: 0.799304 -- epoch 11.\n",
      "Epoch 14 starting...\n",
      "Train loss: 0.457844, train acc: 0.779023\n",
      "Nova melhor conta de validação. Salvo... %f 13\n",
      "Validação Acc: 0.804692 -- Melhor Avaliação Acc: 0.804692 -- epoch 13.\n",
      "Epoch 15 starting...\n",
      "Train loss: 0.540114, train acc: 0.757607\n",
      "Validação Acc: 0.675887 -- Melhor Avaliação Acc: 0.804692 -- epoch 13.\n",
      "Epoch 16 starting...\n",
      "Train loss: 0.501626, train acc: 0.765452\n",
      "Validação Acc: 0.784510 -- Melhor Avaliação Acc: 0.804692 -- epoch 13.\n",
      "Epoch 17 starting...\n",
      "Train loss: 0.530171, train acc: 0.770694\n",
      "Validação Acc: 0.804148 -- Melhor Avaliação Acc: 0.804692 -- epoch 13.\n",
      "Epoch 18 starting...\n",
      "Train loss: 0.489614, train acc: 0.757249\n",
      "Validação Acc: 0.797725 -- Melhor Avaliação Acc: 0.804692 -- epoch 13.\n",
      "Epoch 19 starting...\n",
      "Train loss: 0.507465, train acc: 0.752920\n",
      "Validação Acc: 0.728397 -- Melhor Avaliação Acc: 0.804692 -- epoch 13.\n",
      "Epoch 20 starting...\n",
      "Train loss: 0.495085, train acc: 0.749637\n",
      "Validação Acc: 0.792505 -- Melhor Avaliação Acc: 0.804692 -- epoch 13.\n",
      "Epoch 21 starting...\n",
      "Train loss: 0.420229, train acc: 0.791648\n",
      "Nova melhor conta de validação. Salvo... %f 20\n",
      "Validação Acc: 0.809225 -- Melhor Avaliação Acc: 0.809225 -- epoch 20.\n",
      "Epoch 22 starting...\n",
      "Train loss: 0.425142, train acc: 0.781372\n",
      "Validação Acc: 0.807109 -- Melhor Avaliação Acc: 0.809225 -- epoch 20.\n",
      "Epoch 23 starting...\n",
      "Train loss: 0.414318, train acc: 0.769482\n",
      "Validação Acc: 0.796579 -- Melhor Avaliação Acc: 0.809225 -- epoch 20.\n",
      "Epoch 24 starting...\n",
      "Train loss: 0.411906, train acc: 0.798904\n",
      "Validação Acc: 0.800516 -- Melhor Avaliação Acc: 0.809225 -- epoch 20.\n",
      "Epoch 25 starting...\n",
      "Train loss: 0.441249, train acc: 0.789986\n",
      "Validação Acc: 0.808652 -- Melhor Avaliação Acc: 0.809225 -- epoch 20.\n",
      "Epoch 26 starting...\n",
      "Train loss: 0.424200, train acc: 0.799213\n",
      "Nova melhor conta de validação. Salvo... %f 25\n",
      "Validação Acc: 0.814236 -- Melhor Avaliação Acc: 0.814236 -- epoch 25.\n",
      "Epoch 27 starting...\n",
      "Train loss: 0.397369, train acc: 0.799181\n",
      "Nova melhor conta de validação. Salvo... %f 26\n",
      "Validação Acc: 0.814473 -- Melhor Avaliação Acc: 0.814473 -- epoch 26.\n",
      "Epoch 28 starting...\n",
      "Train loss: 0.393192, train acc: 0.795530\n",
      "Nova melhor conta de validação. Salvo... %f 27\n",
      "Validação Acc: 0.814763 -- Melhor Avaliação Acc: 0.814763 -- epoch 27.\n",
      "Epoch 29 starting...\n",
      "Train loss: 0.407000, train acc: 0.799256\n",
      "Validação Acc: 0.813471 -- Melhor Avaliação Acc: 0.814763 -- epoch 27.\n",
      "Epoch 30 starting...\n",
      "Train loss: 0.381487, train acc: 0.802167\n",
      "Validação Acc: 0.814548 -- Melhor Avaliação Acc: 0.814763 -- epoch 27.\n",
      "Epoch 31 starting...\n",
      "Train loss: 0.377594, train acc: 0.801990\n",
      "Nova melhor conta de validação. Salvo... %f 30\n",
      "Validação Acc: 0.815194 -- Melhor Avaliação Acc: 0.815194 -- epoch 30.\n",
      "Epoch 32 starting...\n",
      "Train loss: 0.370233, train acc: 0.802673\n",
      "Nova melhor conta de validação. Salvo... %f 31\n",
      "Validação Acc: 0.815688 -- Melhor Avaliação Acc: 0.815688 -- epoch 31.\n",
      "Epoch 33 starting...\n",
      "Train loss: 0.369708, train acc: 0.802919\n",
      "Nova melhor conta de validação. Salvo... %f 32\n",
      "Validação Acc: 0.815973 -- Melhor Avaliação Acc: 0.815973 -- epoch 32.\n",
      "Epoch 34 starting...\n",
      "Train loss: 0.366787, train acc: 0.803727\n",
      "Nova melhor conta de validação. Salvo... %f 33\n",
      "Validação Acc: 0.816393 -- Melhor Avaliação Acc: 0.816393 -- epoch 33.\n",
      "Epoch 35 starting...\n",
      "Train loss: 0.369703, train acc: 0.803423\n",
      "Validação Acc: 0.815565 -- Melhor Avaliação Acc: 0.816393 -- epoch 33.\n",
      "Epoch 36 starting...\n",
      "Train loss: 0.366058, train acc: 0.804189\n",
      "Nova melhor conta de validação. Salvo... %f 35\n",
      "Validação Acc: 0.816597 -- Melhor Avaliação Acc: 0.816597 -- epoch 35.\n",
      "Epoch 37 starting...\n",
      "Train loss: 0.362909, train acc: 0.804255\n",
      "Nova melhor conta de validação. Salvo... %f 36\n",
      "Validação Acc: 0.816673 -- Melhor Avaliação Acc: 0.816673 -- epoch 36.\n",
      "Epoch 38 starting...\n",
      "Train loss: 0.363269, train acc: 0.804156\n",
      "Nova melhor conta de validação. Salvo... %f 37\n",
      "Validação Acc: 0.816902 -- Melhor Avaliação Acc: 0.816902 -- epoch 37.\n",
      "Epoch 39 starting...\n",
      "Train loss: 0.363065, train acc: 0.804994\n",
      "Validação Acc: 0.816717 -- Melhor Avaliação Acc: 0.816902 -- epoch 37.\n",
      "Epoch 40 starting...\n",
      "Train loss: 0.364790, train acc: 0.804860\n",
      "Nova melhor conta de validação. Salvo... %f 39\n",
      "Validação Acc: 0.816943 -- Melhor Avaliação Acc: 0.816943 -- epoch 39.\n",
      "Epoch 41 starting...\n",
      "Train loss: 0.360412, train acc: 0.806501\n",
      "Nova melhor conta de validação. Salvo... %f 40\n",
      "Validação Acc: 0.817106 -- Melhor Avaliação Acc: 0.817106 -- epoch 40.\n",
      "Epoch 42 starting...\n",
      "Train loss: 0.361823, train acc: 0.805066\n",
      "Nova melhor conta de validação. Salvo... %f 41\n",
      "Validação Acc: 0.817279 -- Melhor Avaliação Acc: 0.817279 -- epoch 41.\n",
      "Epoch 43 starting...\n",
      "Train loss: 0.359734, train acc: 0.805048\n",
      "Nova melhor conta de validação. Salvo... %f 42\n",
      "Validação Acc: 0.817398 -- Melhor Avaliação Acc: 0.817398 -- epoch 42.\n",
      "Epoch 44 starting...\n",
      "Train loss: 0.362371, train acc: 0.804189\n",
      "Validação Acc: 0.817300 -- Melhor Avaliação Acc: 0.817398 -- epoch 42.\n",
      "Epoch 45 starting...\n",
      "Train loss: 0.360152, train acc: 0.806599\n",
      "Nova melhor conta de validação. Salvo... %f 44\n",
      "Validação Acc: 0.817443 -- Melhor Avaliação Acc: 0.817443 -- epoch 44.\n",
      "Epoch 46 starting...\n",
      "Train loss: 0.356718, train acc: 0.806160\n",
      "Validação Acc: 0.817171 -- Melhor Avaliação Acc: 0.817443 -- epoch 44.\n",
      "Epoch 47 starting...\n",
      "Train loss: 0.355213, train acc: 0.806600\n",
      "Validação Acc: 0.817359 -- Melhor Avaliação Acc: 0.817443 -- epoch 44.\n",
      "Epoch 48 starting...\n",
      "Train loss: 0.361117, train acc: 0.807197\n",
      "Nova melhor conta de validação. Salvo... %f 47\n",
      "Validação Acc: 0.818221 -- Melhor Avaliação Acc: 0.818221 -- epoch 47.\n",
      "Epoch 49 starting...\n",
      "Train loss: 0.361060, train acc: 0.807498\n",
      "Nova melhor conta de validação. Salvo... %f 48\n",
      "Validação Acc: 0.818449 -- Melhor Avaliação Acc: 0.818449 -- epoch 48.\n",
      "Epoch 50 starting...\n",
      "Train loss: 0.355555, train acc: 0.806958\n",
      "Validação Acc: 0.818336 -- Melhor Avaliação Acc: 0.818449 -- epoch 48.\n",
      "Epoch 51 starting...\n",
      "Train loss: 0.353842, train acc: 0.808420\n",
      "Validação Acc: 0.817871 -- Melhor Avaliação Acc: 0.818449 -- epoch 48.\n",
      "Epoch 52 starting...\n",
      "Train loss: 0.355934, train acc: 0.807401\n",
      "Nova melhor conta de validação. Salvo... %f 51\n",
      "Validação Acc: 0.818643 -- Melhor Avaliação Acc: 0.818643 -- epoch 51.\n",
      "Epoch 53 starting...\n",
      "Train loss: 0.360758, train acc: 0.809607\n",
      "Validação Acc: 0.818643 -- Melhor Avaliação Acc: 0.818643 -- epoch 51.\n",
      "Epoch 54 starting...\n",
      "Train loss: 0.354926, train acc: 0.808573\n",
      "Validação Acc: 0.816793 -- Melhor Avaliação Acc: 0.818643 -- epoch 51.\n",
      "Epoch 55 starting...\n",
      "Train loss: 0.352487, train acc: 0.809262\n",
      "Validação Acc: 0.818430 -- Melhor Avaliação Acc: 0.818643 -- epoch 51.\n",
      "Epoch 56 starting...\n",
      "Train loss: 0.349225, train acc: 0.808935\n",
      "Nova melhor conta de validação. Salvo... %f 55\n",
      "Validação Acc: 0.819053 -- Melhor Avaliação Acc: 0.819053 -- epoch 55.\n",
      "Epoch 57 starting...\n",
      "Train loss: 0.351818, train acc: 0.809881\n",
      "Validação Acc: 0.818757 -- Melhor Avaliação Acc: 0.819053 -- epoch 55.\n",
      "Epoch 58 starting...\n",
      "Train loss: 0.348996, train acc: 0.808915\n",
      "Validação Acc: 0.818229 -- Melhor Avaliação Acc: 0.819053 -- epoch 55.\n",
      "Epoch 59 starting...\n",
      "Train loss: 0.347949, train acc: 0.809135\n",
      "Validação Acc: 0.818992 -- Melhor Avaliação Acc: 0.819053 -- epoch 55.\n",
      "Epoch 60 starting...\n",
      "Train loss: 0.346974, train acc: 0.809631\n",
      "Nova melhor conta de validação. Salvo... %f 59\n",
      "Validação Acc: 0.819553 -- Melhor Avaliação Acc: 0.819553 -- epoch 59.\n",
      "Epoch 61 starting...\n",
      "Train loss: 0.346374, train acc: 0.810698\n",
      "Nova melhor conta de validação. Salvo... %f 60\n",
      "Validação Acc: 0.819624 -- Melhor Avaliação Acc: 0.819624 -- epoch 60.\n",
      "Epoch 62 starting...\n",
      "Train loss: 0.343855, train acc: 0.810051\n",
      "Validação Acc: 0.819475 -- Melhor Avaliação Acc: 0.819624 -- epoch 60.\n",
      "Epoch 63 starting...\n",
      "Train loss: 0.344806, train acc: 0.809582\n",
      "Validação Acc: 0.819544 -- Melhor Avaliação Acc: 0.819624 -- epoch 60.\n",
      "Epoch 64 starting...\n",
      "Train loss: 0.344365, train acc: 0.810510\n",
      "Validação Acc: 0.819537 -- Melhor Avaliação Acc: 0.819624 -- epoch 60.\n",
      "Epoch 65 starting...\n",
      "Train loss: 0.345409, train acc: 0.810209\n",
      "Validação Acc: 0.819555 -- Melhor Avaliação Acc: 0.819624 -- epoch 60.\n",
      "Epoch 66 starting...\n",
      "Train loss: 0.344710, train acc: 0.810671\n",
      "Nova melhor conta de validação. Salvo... %f 65\n",
      "Validação Acc: 0.819760 -- Melhor Avaliação Acc: 0.819760 -- epoch 65.\n",
      "Epoch 67 starting...\n",
      "Train loss: 0.344734, train acc: 0.810187\n",
      "Nova melhor conta de validação. Salvo... %f 66\n",
      "Validação Acc: 0.819768 -- Melhor Avaliação Acc: 0.819768 -- epoch 66.\n",
      "Epoch 68 starting...\n",
      "Train loss: 0.344069, train acc: 0.810724\n",
      "Nova melhor conta de validação. Salvo... %f 67\n",
      "Validação Acc: 0.819849 -- Melhor Avaliação Acc: 0.819849 -- epoch 67.\n",
      "Epoch 69 starting...\n",
      "Train loss: 0.342513, train acc: 0.811030\n",
      "Validação Acc: 0.819781 -- Melhor Avaliação Acc: 0.819849 -- epoch 67.\n",
      "Epoch 70 starting...\n",
      "Train loss: 0.344203, train acc: 0.810331\n",
      "Nova melhor conta de validação. Salvo... %f 69\n",
      "Validação Acc: 0.819969 -- Melhor Avaliação Acc: 0.819969 -- epoch 69.\n",
      "Epoch 71 starting...\n",
      "Train loss: 0.343449, train acc: 0.811750\n",
      "Nova melhor conta de validação. Salvo... %f 70\n",
      "Validação Acc: 0.820041 -- Melhor Avaliação Acc: 0.820041 -- epoch 70.\n",
      "Epoch 72 starting...\n",
      "Train loss: 0.343554, train acc: 0.810982\n",
      "Validação Acc: 0.819992 -- Melhor Avaliação Acc: 0.820041 -- epoch 70.\n",
      "Epoch 73 starting...\n",
      "Train loss: 0.342906, train acc: 0.811799\n",
      "Validação Acc: 0.819850 -- Melhor Avaliação Acc: 0.820041 -- epoch 70.\n",
      "Epoch 74 starting...\n",
      "Train loss: 0.342762, train acc: 0.811311\n",
      "Nova melhor conta de validação. Salvo... %f 73\n",
      "Validação Acc: 0.820109 -- Melhor Avaliação Acc: 0.820109 -- epoch 73.\n",
      "Epoch 75 starting...\n",
      "Train loss: 0.342615, train acc: 0.811233\n",
      "Nova melhor conta de validação. Salvo... %f 74\n",
      "Validação Acc: 0.820173 -- Melhor Avaliação Acc: 0.820173 -- epoch 74.\n",
      "Epoch 76 starting...\n",
      "Train loss: 0.342339, train acc: 0.811152\n",
      "Validação Acc: 0.820151 -- Melhor Avaliação Acc: 0.820173 -- epoch 74.\n",
      "Epoch 77 starting...\n",
      "Train loss: 0.342287, train acc: 0.811617\n",
      "Nova melhor conta de validação. Salvo... %f 76\n",
      "Validação Acc: 0.820226 -- Melhor Avaliação Acc: 0.820226 -- epoch 76.\n",
      "Epoch 78 starting...\n",
      "Train loss: 0.342351, train acc: 0.810966\n",
      "Nova melhor conta de validação. Salvo... %f 77\n",
      "Validação Acc: 0.820289 -- Melhor Avaliação Acc: 0.820289 -- epoch 77.\n",
      "Epoch 79 starting...\n",
      "Train loss: 0.342793, train acc: 0.811880\n",
      "Nova melhor conta de validação. Salvo... %f 78\n",
      "Validação Acc: 0.820838 -- Melhor Avaliação Acc: 0.820838 -- epoch 78.\n",
      "Epoch 80 starting...\n",
      "Train loss: 0.342572, train acc: 0.812354\n",
      "Validação Acc: 0.820720 -- Melhor Avaliação Acc: 0.820838 -- epoch 78.\n",
      "Epoch 81 starting...\n",
      "Train loss: 0.342211, train acc: 0.812077\n",
      "Validação Acc: 0.820570 -- Melhor Avaliação Acc: 0.820838 -- epoch 78.\n",
      "Epoch 82 starting...\n",
      "Train loss: 0.344838, train acc: 0.811912\n",
      "Nova melhor conta de validação. Salvo... %f 81\n",
      "Validação Acc: 0.821537 -- Melhor Avaliação Acc: 0.821537 -- epoch 81.\n",
      "Epoch 83 starting...\n",
      "Train loss: 0.340995, train acc: 0.812365\n",
      "Validação Acc: 0.820538 -- Melhor Avaliação Acc: 0.821537 -- epoch 81.\n",
      "Epoch 84 starting...\n",
      "Train loss: 0.340666, train acc: 0.811996\n",
      "Validação Acc: 0.820880 -- Melhor Avaliação Acc: 0.821537 -- epoch 81.\n",
      "Epoch 85 starting...\n",
      "Train loss: 0.341943, train acc: 0.812687\n",
      "Validação Acc: 0.821129 -- Melhor Avaliação Acc: 0.821537 -- epoch 81.\n",
      "Epoch 86 starting...\n",
      "Train loss: 0.340876, train acc: 0.813008\n",
      "Validação Acc: 0.821181 -- Melhor Avaliação Acc: 0.821537 -- epoch 81.\n",
      "Epoch 87 starting...\n",
      "Train loss: 0.340402, train acc: 0.813589\n",
      "Validação Acc: 0.821152 -- Melhor Avaliação Acc: 0.821537 -- epoch 81.\n",
      "Epoch 88 starting...\n",
      "Train loss: 0.340336, train acc: 0.814057\n",
      "Validação Acc: 0.821423 -- Melhor Avaliação Acc: 0.821537 -- epoch 81.\n",
      "Epoch 89 starting...\n",
      "Train loss: 0.341406, train acc: 0.811913\n",
      "Validação Acc: 0.821111 -- Melhor Avaliação Acc: 0.821537 -- epoch 81.\n",
      "Epoch 90 starting...\n",
      "Train loss: 0.339144, train acc: 0.812805\n",
      "Validação Acc: 0.821257 -- Melhor Avaliação Acc: 0.821537 -- epoch 81.\n",
      "Epoch 91 starting...\n",
      "Train loss: 0.339635, train acc: 0.813059\n",
      "Validação Acc: 0.821509 -- Melhor Avaliação Acc: 0.821537 -- epoch 81.\n",
      "Epoch 92 starting...\n",
      "Train loss: 0.339282, train acc: 0.813426\n",
      "Validação Acc: 0.821499 -- Melhor Avaliação Acc: 0.821537 -- epoch 81.\n",
      "Epoch 93 starting...\n",
      "Train loss: 0.339616, train acc: 0.813798\n",
      "Nova melhor conta de validação. Salvo... %f 92\n",
      "Validação Acc: 0.821642 -- Melhor Avaliação Acc: 0.821642 -- epoch 92.\n",
      "Epoch 94 starting...\n",
      "Train loss: 0.338590, train acc: 0.813971\n",
      "Validação Acc: 0.821408 -- Melhor Avaliação Acc: 0.821642 -- epoch 92.\n",
      "Epoch 95 starting...\n",
      "Train loss: 0.338002, train acc: 0.813471\n",
      "Validação Acc: 0.821592 -- Melhor Avaliação Acc: 0.821642 -- epoch 92.\n",
      "Epoch 96 starting...\n",
      "Train loss: 0.337582, train acc: 0.814256\n",
      "Nova melhor conta de validação. Salvo... %f 95\n",
      "Validação Acc: 0.821696 -- Melhor Avaliação Acc: 0.821696 -- epoch 95.\n",
      "Epoch 97 starting...\n",
      "Train loss: 0.338979, train acc: 0.813886\n",
      "Validação Acc: 0.821517 -- Melhor Avaliação Acc: 0.821696 -- epoch 95.\n",
      "Epoch 98 starting...\n",
      "Train loss: 0.338573, train acc: 0.814087\n",
      "Validação Acc: 0.821678 -- Melhor Avaliação Acc: 0.821696 -- epoch 95.\n",
      "Epoch 99 starting...\n",
      "Train loss: 0.338294, train acc: 0.813547\n",
      "Validação Acc: 0.821614 -- Melhor Avaliação Acc: 0.821696 -- epoch 95.\n",
      "Epoch 100 starting...\n",
      "Train loss: 0.338453, train acc: 0.813625\n",
      "Nova melhor conta de validação. Salvo... %f 99\n",
      "Validação Acc: 0.821722 -- Melhor Avaliação Acc: 0.821722 -- epoch 99.\n",
      "Epoch 101 starting...\n",
      "Train loss: 0.338311, train acc: 0.814327\n",
      "Nova melhor conta de validação. Salvo... %f 100\n",
      "Validação Acc: 0.821913 -- Melhor Avaliação Acc: 0.821913 -- epoch 100.\n",
      "Epoch 102 starting...\n",
      "Train loss: 0.338986, train acc: 0.813698\n",
      "Validação Acc: 0.821894 -- Melhor Avaliação Acc: 0.821913 -- epoch 100.\n",
      "Epoch 103 starting...\n",
      "Train loss: 0.338853, train acc: 0.814343\n",
      "Validação Acc: 0.821711 -- Melhor Avaliação Acc: 0.821913 -- epoch 100.\n",
      "Epoch 104 starting...\n",
      "Train loss: 0.337529, train acc: 0.814472\n",
      "Validação Acc: 0.821830 -- Melhor Avaliação Acc: 0.821913 -- epoch 100.\n",
      "Epoch 105 starting...\n",
      "Train loss: 0.338505, train acc: 0.814324\n",
      "Nova melhor conta de validação. Salvo... %f 104\n",
      "Validação Acc: 0.822037 -- Melhor Avaliação Acc: 0.822037 -- epoch 104.\n",
      "Epoch 106 starting...\n",
      "Train loss: 0.338287, train acc: 0.814561\n",
      "Validação Acc: 0.822023 -- Melhor Avaliação Acc: 0.822037 -- epoch 104.\n",
      "Epoch 107 starting...\n",
      "Train loss: 0.338338, train acc: 0.814799\n",
      "Validação Acc: 0.821979 -- Melhor Avaliação Acc: 0.822037 -- epoch 104.\n",
      "Epoch 108 starting...\n",
      "Train loss: 0.338828, train acc: 0.814455\n",
      "Nova melhor conta de validação. Salvo... %f 107\n",
      "Validação Acc: 0.822050 -- Melhor Avaliação Acc: 0.822050 -- epoch 107.\n",
      "Epoch 109 starting...\n",
      "Train loss: 0.339036, train acc: 0.814393\n",
      "Nova melhor conta de validação. Salvo... %f 108\n",
      "Validação Acc: 0.822117 -- Melhor Avaliação Acc: 0.822117 -- epoch 108.\n",
      "Epoch 110 starting...\n",
      "Train loss: 0.339067, train acc: 0.814249\n",
      "Validação Acc: 0.821948 -- Melhor Avaliação Acc: 0.822117 -- epoch 108.\n",
      "Epoch 111 starting...\n",
      "Train loss: 0.338554, train acc: 0.814578\n",
      "Validação Acc: 0.822110 -- Melhor Avaliação Acc: 0.822117 -- epoch 108.\n",
      "Epoch 112 starting...\n",
      "Train loss: 0.337344, train acc: 0.814741\n",
      "Nova melhor conta de validação. Salvo... %f 111\n",
      "Validação Acc: 0.822214 -- Melhor Avaliação Acc: 0.822214 -- epoch 111.\n",
      "Epoch 113 starting...\n",
      "Train loss: 0.338210, train acc: 0.814528\n",
      "Validação Acc: 0.822128 -- Melhor Avaliação Acc: 0.822214 -- epoch 111.\n",
      "Epoch 114 starting...\n",
      "Train loss: 0.338517, train acc: 0.814601\n",
      "Validação Acc: 0.822184 -- Melhor Avaliação Acc: 0.822214 -- epoch 111.\n",
      "Epoch 115 starting...\n",
      "Train loss: 0.338511, train acc: 0.815094\n",
      "Validação Acc: 0.822195 -- Melhor Avaliação Acc: 0.822214 -- epoch 111.\n",
      "Epoch 116 starting...\n",
      "Train loss: 0.338180, train acc: 0.814995\n",
      "Nova melhor conta de validação. Salvo... %f 115\n",
      "Validação Acc: 0.822396 -- Melhor Avaliação Acc: 0.822396 -- epoch 115.\n",
      "Epoch 117 starting...\n",
      "Train loss: 0.336787, train acc: 0.815606\n",
      "Validação Acc: 0.822335 -- Melhor Avaliação Acc: 0.822396 -- epoch 115.\n",
      "Epoch 118 starting...\n"
     ]
    }
   ],
   "source": [
    "# Inicializar listas para armazenar a perda e a precisão\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Inicia o treinamento\n",
    "train_dataset = SegmentationDataset(img_folder_train, img_folder_train, True, class_to_id, resolution_input, True, None)\n",
    "print(f\"Número de amostras no dataset de treinamento: {len(train_dataset)}\")\n",
    "print(f\"Arquivos no dataset de treinamento: {os.listdir(img_folder_train)}\")\n",
    "\n",
    "val_dataset = SegmentationDataset(img_folder_val, img_folder_val, False, class_to_id, resolution_input, False, None)\n",
    "print(f\"Número de amostras no dataset de validação: {len(val_dataset)}\")\n",
    "print(f\"Arquivos no dataset de validação: {os.listdir(img_folder_val)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=0, drop_last=False)\n",
    "\n",
    "if plot_train:\n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "        image_np = np.squeeze(sample_batched['image_original'].cpu().numpy())\n",
    "        gt = np.squeeze(sample_batched['gt'].cpu().numpy())\n",
    "        \n",
    "        color_label = np.zeros((resolution_input[1], resolution_input[0], 3))\n",
    "        \n",
    "        for key, val in id_to_class.items():\n",
    "            color_label[gt == key] = class_to_color.get(val, [0, 0, 0])  # Provide a default color if key is missing\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.imshow((image_np / 255) * 0.5 + (color_label / 255) * 0.5)\n",
    "        if plt_savefig: \n",
    "            plt.savefig(img_folder_train_segmentadas + \"TRAIN_IMG_\" + str(i_batch) + \".png\")\n",
    "        if plt_show: \n",
    "            plt.show()\n",
    "        plt.close('all')        \n",
    "        plt.figure()\n",
    "        plt.imshow(color_label.astype(np.uint8))\n",
    "        if plt_savefig: \n",
    "            plt.savefig(img_folder_train_segmentadas + \"TRAIN_GT_\" + str(i_batch) + \".png\")\n",
    "        if plt_show: \n",
    "            plt.show()\n",
    "        plt.close('all')\n",
    "\n",
    "model = PSPNet(nClasses).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), 1e-2, .9, 1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 30, gamma=0.2)\n",
    "\n",
    "best_val_acc = -1\n",
    "best_epoch = 0\n",
    "\n",
    "# Start training...\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    print('Epoch %d starting...' % (epoch+1))\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    mean_loss = 0.0\n",
    "    \n",
    "    n_correct = 0\n",
    "    n_false = 0\n",
    "    \n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "    \n",
    "    \n",
    "        image = sample_batched['image'].to(device)\n",
    "        gt = sample_batched['gt'].to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        output, total_loss = model.eval_net_with_loss(model, image, gt, class_weights, device)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        mean_loss += total_loss.cpu().detach().numpy()\n",
    "        \n",
    "        # Measure accuracy\n",
    "        \n",
    "        gt = np.squeeze(sample_batched['gt'].cpu().numpy())\n",
    "        \n",
    "        label_out = torch.nn.functional.softmax(output, dim = 1)\n",
    "        label_out = label_out.cpu().detach().numpy()\n",
    "        label_out = np.squeeze(label_out)\n",
    "        \n",
    "        labels = np.argmax(label_out, axis=0)\n",
    "        valid_mask = gt != -1\n",
    "        curr_correct = np.sum(gt[valid_mask] == labels[valid_mask])\n",
    "        curr_false = np.sum(valid_mask) - curr_correct\n",
    "        n_correct += curr_correct\n",
    "        n_false += curr_false\n",
    "        \n",
    "    mean_loss /= len(train_loader)\n",
    "    train_acc = n_correct / (n_correct + n_false)\n",
    "        \n",
    "    print('Train loss: %f, train acc: %f' % (mean_loss, train_acc))\n",
    "    # Armazenar a perda e a precisão de treinamento\n",
    "    train_losses.append(mean_loss)\n",
    "    train_accuracies.append(train_acc)    \n",
    "    \n",
    "    n_correct = 0\n",
    "    n_false = 0\n",
    "    \n",
    "    \n",
    "    for i_batch, sample_batched in enumerate(val_loader):\n",
    "    \n",
    "    \n",
    "        image = sample_batched['image'].to(device)\n",
    "        image_np = np.squeeze(sample_batched['image_original'].cpu().numpy())\n",
    "        gt = np.squeeze(sample_batched['gt'].cpu().numpy())\n",
    "        \n",
    "    \n",
    "        label_out = model(image)\n",
    "        label_out = torch.nn.functional.softmax(label_out, dim = 1)\n",
    "        label_out = label_out.cpu().detach().numpy()\n",
    "        label_out = np.squeeze(label_out)\n",
    "        \n",
    "        labels = np.argmax(label_out, axis=0)\n",
    "        \n",
    "        if plot_val:\n",
    "            \n",
    "            color_label = np.zeros((resolution_input[1], resolution_input[0], 3))\n",
    "            \n",
    "            for key, val in id_to_class.items():\n",
    "                color_label[labels == key] = class_to_color[val]\n",
    "                \n",
    "            plt.figure()\n",
    "            plt.imshow((image_np/255) * 0.5 + (color_label/255) * 0.5)\n",
    "            if plt_savefig: \n",
    "                plt.savefig(img_folder_val_segmentadas + \"IMG_\" + str(i_batch) + \"_epoch_\" + str(epoch) + \".png\")\n",
    "            if plt_show: \n",
    "                plt.show()\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.imshow(color_label.astype(np.uint8))\n",
    "            if plt_savefig: \n",
    "                plt.savefig(img_folder_val_segmentadas + \"GT_\" + str(i_batch) + \"_epoch_\" + str(epoch) +  \".png\")\n",
    "            if plt_show: \n",
    "                plt.show()\n",
    "        \n",
    "        valid_mask = gt != -1\n",
    "        curr_correct = np.sum(gt[valid_mask] == labels[valid_mask])\n",
    "        curr_false = np.sum(valid_mask) - curr_correct\n",
    "        n_correct += curr_correct\n",
    "        n_false += curr_false    \n",
    "        \n",
    "    total_acc = n_correct / (n_correct + n_false)\n",
    "    val_accuracies.append(total_acc)\n",
    "    \n",
    "    if best_val_acc < total_acc:\n",
    "        best_val_acc = total_acc\n",
    "        if epoch > 7:\n",
    "            torch.save(model.state_dict(), model_file_name)\n",
    "            print('Nova melhor conta de validação. Salvo... %f', epoch)\n",
    "        best_epoch = epoch\n",
    "\n",
    "    if (epoch - best_epoch) > patience:\n",
    "        print(f\"Terminando o treinamento, melhor conta de validação {best_val_acc:.6f}\")\n",
    "        break\n",
    "    \n",
    "    print('Validação Acc: %f -- Melhor Avaliação Acc: %f -- epoch %d.' % (total_acc, best_val_acc, best_epoch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotar os gráficos de perda e precisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar os gráficos de perda e precisão\n",
    "\"\"\"\n",
    "    Inicialização das listas: train_losses, train_accuracies e val_accuracies são listas para armazenar a perda e a precisão de treinamento e validação em cada época.\n",
    "    \n",
    "    Armazenamento dos valores: Durante o loop de treinamento, a perda e a precisão são calculadas e armazenadas nas listas correspondentes.\n",
    "    Plotagem dos gráficos: Após o loop de treinamento, os gráficos de perda e precisão são plotados usando matplotlib.\n",
    "\n",
    "    Este código deve ser adicionado ao final do seu loop de treinamento no notebook para visualizar os resultados do treinamento ao longo das épocas.\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Perda de Treinamento')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Perda')\n",
    "plt.title('Perda de Treinamento ao longo das Épocas')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Precisão de Treinamento')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Precisão de Validação')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Precisão')\n",
    "plt.title('Precisão de Treinamento e Validação ao longo das Épocas')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir + 'result_model_segmentadas_unet_loss_accuracy.png')\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferência de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações do treinamento\n",
    "resolution_input = (640, 480)  # Tamanho de entrada\n",
    "patience = 30\n",
    "plot_val = True\n",
    "plot_train = True\n",
    "max_epochs = 300\n",
    "class_weights = [1, 1, 1]\n",
    "nClasses = 3\n",
    "\n",
    "# Mapeamento de classes e cores\n",
    "class_to_color = {'Doenca': (255, 0, 0), 'Solo': (0, 0, 255), 'Saudavel': (0, 255, 255)}\n",
    "class_to_id = {'Doenca': 0, 'Solo': 1, 'Saudavel': 2}\n",
    "id_to_class = {v: k for k, v in class_to_id.items()}\n",
    "\n",
    "## Configurações do treinamento\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "model = PSPNet(nClasses)\n",
    "#model.load_state_dict(torch.load(model_file_name))\n",
    "model.load_state_dict(torch.load(model_file_name, weights_only=True))\n",
    "model.eval()\n",
    "print(\"Modelo carregado e pronto para uso.\")\n",
    "model.to(device)\n",
    "\n",
    "img_list = glob.glob(osp.join(img_folder_val, '*.JPG'))\n",
    "print(f\"Imagens de teste: {len(img_list)}\")\n",
    "\n",
    "for img_path in img_list:\n",
    "    img_np = cv2.imread(img_path, cv2.IMREAD_IGNORE_ORIENTATION + cv2.IMREAD_COLOR)\n",
    "    img_np = cv2.resize(img_np, (resolution_input[0], resolution_input[1]))[..., ::-1]\n",
    "    img_np = np.ascontiguousarray(img_np)\n",
    "    \n",
    "    img_pt = np.copy(img_np).astype(np.float32) / 255.0\n",
    "    for i in range(3):\n",
    "        img_pt[..., i] -= mean[i]\n",
    "        img_pt[..., i] /= std[i]\n",
    "\n",
    "    img_pt = img_pt.transpose(2, 0, 1)\n",
    "    img_pt = torch.from_numpy(img_pt[None, ...]).to(device)\n",
    "\n",
    "    label_out = model(img_pt)\n",
    "    label_out = torch.nn.functional.softmax(label_out, dim=1)\n",
    "    label_out = label_out.cpu().detach().numpy()\n",
    "    label_out = np.squeeze(label_out)\n",
    "\n",
    "    labels = np.argmax(label_out, axis=0)\n",
    "\n",
    "    color_label = np.zeros((resolution_input[1], resolution_input[0], 3))\n",
    "\n",
    "    for key, val in id_to_class.items():\n",
    "        color_label[labels == key] = class_to_color[val]\n",
    "        \n",
    "    final_image = osp.basename(img_path)\n",
    "    final_image = osp.splitext(final_image)[0]\n",
    "    final_image = osp.join(img_folder_test_segmentadas, final_image)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow((img_np / 255) * 0.5 + (color_label / 255) * 0.5)\n",
    "    plt.savefig(final_image + \"RESULT_INFERENCIA_IMG_\" + \".png\")\n",
    "    if plt_show:\n",
    "        plt.show()\n",
    "    plt.close('all')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(color_label.astype(np.uint8))\n",
    "    plt.savefig(final_image + \"RESULT_INFERENCIA_GT_\" + \".png\")\n",
    "    if plt_show:\n",
    "        plt.show()\n",
    "    plt.close('all')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
